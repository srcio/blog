<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>博客 · 丁鹏</title>
    <link>https://srcio.cn/</link>
    <description>Recent content on 博客 · 丁鹏</description>
    <image>
      <url>https://srcio.cn/cover.png</url>
      <link>https://srcio.cn/cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Tue, 18 Oct 2022 18:00:26 +0800</lastBuildDate><atom:link href="https://srcio.cn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>数组递归构造具有父子层级关系的对象</title>
      <link>https://srcio.cn/series/programming-go/menu-recursive/</link>
      <pubDate>Tue, 18 Oct 2022 18:00:26 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-go/menu-recursive/</guid>
      <description>场景介绍 从数据库获取到了菜单列表数据，这些菜单数据通过字段 ParentID 表示父子层级关系，现在需要将菜单列表数据转成树状的实例对象。
数据库取出的初始数据：
raw := []Menu{ {Name: &amp;#34;一级菜单 1&amp;#34;, ID: 1, PID: 0}, {Name: &amp;#34;一级菜单 2&amp;#34;, ID: 2, PID: 0}, {Name: &amp;#34;一级菜单 3&amp;#34;, ID: 3, PID: 0}, {Name: &amp;#34;二级菜单 1-1&amp;#34;, ID: 11, PID: 1}, {Name: &amp;#34;二级菜单 1-2&amp;#34;, ID: 12, PID: 1}, {Name: &amp;#34;二级菜单 1-3&amp;#34;, ID: 13, PID: 1}, {Name: &amp;#34;二级菜单 2-1&amp;#34;, ID: 21, PID: 2}, {Name: &amp;#34;二级菜单 2-2&amp;#34;, ID: 22, PID: 2}, {Name: &amp;#34;二级菜单 2-3&amp;#34;, ID: 23, PID: 2}, } 需要得到的目标数据：</description>
    </item>
    
    <item>
      <title>K8s 集群规划之节点资源配置</title>
      <link>https://srcio.cn/posts/k8s-node-resource-config/</link>
      <pubDate>Mon, 17 Oct 2022 20:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/k8s-node-resource-config/</guid>
      <description>文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size
 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。
集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。
例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。
 例如，因为要在集群上运行的应用程序需要此数量的资源。
 以下是实现集群的两种可能方法：
  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。
究竟哪种配置方式更好呢？
为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。
 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。
 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。
在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。
优势 让我们来看看这种方法可能具有的优势。
更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。
但请注意，这主要适用于裸机服务器而不适用于云实例。
如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。
更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。</description>
    </item>
    
    <item>
      <title>Kubelet 垃圾回收原理剖析</title>
      <link>https://srcio.cn/posts/kubelet-recycle-policy/</link>
      <pubDate>Mon, 17 Oct 2022 19:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/kubelet-recycle-policy/</guid>
      <description>文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html
 Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。
镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。
主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。
需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。
用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：
 image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。
实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。
# vim /etc/systemd/system/kubelet.</description>
    </item>
    
    <item>
      <title>CRD 简介</title>
      <link>https://srcio.cn/series/programming-kubernetes/crd/</link>
      <pubDate>Tue, 11 Oct 2022 23:50:34 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/crd/</guid>
      <description>CRD 字段校验配置
apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:scalings.control.srcio.iospec:group:control.srcio.ioversions:- name:v1served:truestorage:truescope:Namespacednames:plural:scalingssingular:scalingkind:Scalingvalidation:openAPIV3Schema:properties:spec:required:- targetDeployment- minReplicas- maxReplicas- metricType- step- scaleUp- scaleDownproperties:targetDeployment:type:stringminReplicas:type:integerminimum:0maxReplicas:type:integerminimum:0metricType:type:stringenum:- CPU- MEMORY- REQUESTSstep:type:integerminimum:1scaleUp:type:integerscaleDown:type:integerminimum:0  是否必须 参数类型 枚举范围 数值最大最小   </description>
    </item>
    
    <item>
      <title>Golang 切片扩容</title>
      <link>https://srcio.cn/series/programming-go/slice-append/</link>
      <pubDate>Tue, 11 Oct 2022 14:19:02 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-go/slice-append/</guid>
      <description>怎么理解切片 s = append(s, item) 需要使用 s 重新接收呢？
 在 golang 语言中所有的参数传递的方式都是值传递的，即便是指针，也是复制了一份指针传递； 切片发生扩容后，底层的数组发生了变化，不再是原来的数组结构。    </description>
    </item>
    
    <item>
      <title>Docker Compose 实践</title>
      <link>https://srcio.cn/posts/docker-compose/</link>
      <pubDate>Mon, 10 Oct 2022 19:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/docker-compose/</guid>
      <description>安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。
使用一下命令安装或升级 Docker Compose（linux）：
 Ubuntu，Debian：  sudo apt update sudo apt install docker-compose-plugin  基于 RPM 发行版:  sudo yum update sudo yum install docker-compose-plugin 验证安装版本：
docker-compose version 常用命令 运行
docker-compose up 查看运行
docker-compose ps 停止
docker-compose stop 启动&amp;amp;重启
docker-compose start docker-compose restart 退出
docker-compose down 使用 docker-compose -h 查看更多命令及参数。
实践 使用 Docker Compose 运行一个简单的 golang web 程序。
 程序初始化  mkdir docker-compose-go-demo cd docker-compose-go-demo go mod init docker-compose-go-demo 创建 main.</description>
    </item>
    
    <item>
      <title>Golang 生成证书</title>
      <link>https://srcio.cn/series/programming-go/gen-cert/</link>
      <pubDate>Sat, 08 Oct 2022 17:13:58 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-go/gen-cert/</guid>
      <description>代码实现 package certutil import ( &amp;#34;bytes&amp;#34; &amp;#34;crypto/rand&amp;#34; &amp;#34;crypto/rsa&amp;#34; &amp;#34;crypto/x509&amp;#34; &amp;#34;crypto/x509/pkix&amp;#34; &amp;#34;encoding/pem&amp;#34; &amp;#34;math/big&amp;#34; &amp;#34;net&amp;#34; &amp;#34;time&amp;#34; ) // CA ca type CA struct { caInfo *x509.Certificate caPrivKey *rsa.PrivateKey caPem, caKeyPem []byte } // GetCAPem get ca pem bytes func (c *CA) GetCAPem() ([]byte, error) { if c.caPem == nil { // create the CA 	caBytes, err := x509.CreateCertificate(rand.Reader, c.caInfo, c.caInfo, &amp;amp;c.caPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, err } // pem encode 	caPEM := new(bytes.</description>
    </item>
    
    <item>
      <title>Golang 实现双向认证</title>
      <link>https://srcio.cn/series/programming-go/mtls/</link>
      <pubDate>Sun, 02 Oct 2022 01:49:13 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-go/mtls/</guid>
      <description>TLS 传输层安全协议（TLS），在互联网上，通常是由服务器单向的向客户端提供证书，以证明其身份。
mTLS 双向 TLS 认证，是指在客户端和服务器之间使用双行加密通道，mTLS 是云原生应用中常用的通信安全协议。
使用双向TLS连接的主要目的是当服务器应该只接受来自有限的允许的客户端的 TLS 连接时。例如，一个组织希望将服务器的 TLS 连接限制为只来自该组织的合法合作伙伴或客户。显然，为客户端添加IP白名单不是一个好的安全实践，因为IP可能被欺骗。
为了简化 mTLS 握手的过程，我们这样简单梳理：
  客户端发送访问服务器上受保护信息的请求；
  服务器向客户端提供公钥证书；
  客户端通过使用 CA 的公钥来验证服务器公钥证书的数字签名，以验证服务器的证书；
  如果步骤 3 成功，客户机将其客户端公钥证书发送到服务器；
  服务器使用步骤 3 中相同的方法验证客户机的证书；
  如果成功，服务器将对受保护信息的访问权授予客户机。
  代码实现 需要实现客户端验证服务端的公钥证书，服务端验证客户端的公钥证书。
生成证书 echo &amp;#39;清理并生成目录&amp;#39; OUT=./certs DAYS=365 RSALEN=2048 CN=srcio rm -rf ${OUT}/* mkdir ${OUT} &amp;gt;&amp;gt; /dev/null 2&amp;gt;&amp;amp;1 cd ${OUT} echo &amp;#39;生成CA的私钥&amp;#39; openssl genrsa -out ca.key ${RSALEN} &amp;gt;&amp;gt; /dev/null 2&amp;gt;&amp;amp;1 echo &amp;#39;生成CA的签名证书&amp;#39; openssl req -new \ -x509 \ -key ca.</description>
    </item>
    
    <item>
      <title>构造 rest.Config 实例</title>
      <link>https://srcio.cn/series/programming-kubernetes/rest-config/</link>
      <pubDate>Sat, 01 Oct 2022 23:28:02 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/rest-config/</guid>
      <description>本节介绍几种构造 rest.Config 实例的场景或者方法。
rest.Config 可以帮助我们构建各种类型的 Kubernetes 客户端实例，从而访问 Kubernetes APIServer。
通过 kubeconfig 文件构造 程序通过读取 kubeconfig 文件来构造一个 rest.Config 对象。
package main import ( &amp;#34;k8s.io/client-go/rest&amp;#34; &amp;#34;k8s.io/client-go/tools/clientcmd&amp;#34; ) func KubeConfig() *rest.Config { config, err := clientcmd.BuildConfigFromFlags(&amp;#34;&amp;#34;, clientcmd.RecommendedHomeFile) if err != nil { panic(err) } return config } 通过 Secret 资源构造 通过将程序部署在 Kubernetes 集群中，使用 Pod 所配置的 ServiceAccount（默认：default）账号构造 rest.Config 对象。
 运行的 Pod 内都会存储一个
每个 ServiceAccount 都有一个对应的 Secret，这个 Secret 包含了对集群的操作权限。
 package main import ( &amp;#34;k8s.io/client-go/rest&amp;#34; ) func KubeConfig() *rest.</description>
    </item>
    
    <item>
      <title>Go1.18 - 工作区模式</title>
      <link>https://srcio.cn/series/programming-go/go-work/</link>
      <pubDate>Wed, 28 Sep 2022 18:25:05 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-go/go-work/</guid>
      <description>安装的 Go1.18 或更新版本，它为你提供了工作区模式（Workspace mode），帮助你更好做 go 模块之间依赖的管理。
例如，你开发一个新项目，分了两个 go module，分别为 service-a 和 service-b，service-a 依赖了service-b ，现在项目还处于开发阶段，我们都是这么处理的。
创建项目目录：
mkdir service cd service 创建 service-b 模块:
mkdir service-b cd service-b go mod init github.com/srcio/service-b 编写 service-b 模块代码：
mkdir greeting vim greeting/hello.go package greeting import &amp;#34;fmt&amp;#34; func Hello(name string){ fmt.Println(&amp;#34;Hello, &amp;#34; + name) } 继续创建 service-a 模块：
cd .. mkdir service-a cd service-a go mod init github.com/srcio/service-a 因为 service-a 需要依赖本地开发的 service-b 类库，所以我们需要在 go.mod 中引入 service-a ：</description>
    </item>
    
    <item>
      <title>使用 Giscus 作为博客评论系统</title>
      <link>https://srcio.cn/posts/use-giscus/</link>
      <pubDate>Wed, 28 Sep 2022 09:02:43 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/use-giscus/</guid>
      <description>Giscus  开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载  必要条件  你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus.app，安装的时候，分配你的博客所用的 GitHub 仓库即可。   当然，如果你的博客没有托管在 Github 上，你也可以单独创建一个 Github 仓库作为开通 giscus 评论。
 使用姿势  在 giscus.app 做自定义配置，填入你的仓库名称，选择主题等，Giscus 会自动帮你生成 javascript 脚本； Hugo 博客目录下，创建 layouts/partials/comments.html 文件，写入获取的脚本：  &amp;lt;script src=&amp;#34;https://giscus.app/client.js&amp;#34; data-repo=&amp;#34;[在此输入仓库]&amp;#34; data-repo-id=&amp;#34;[在此输入仓库 ID]&amp;#34; data-category=&amp;#34;[在此输入分类名]&amp;#34; data-category-id=&amp;#34;[在此输入分类 ID]&amp;#34; data-mapping=&amp;#34;pathname&amp;#34; data-strict=&amp;#34;0&amp;#34; data-reactions-enabled=&amp;#34;1&amp;#34; data-emit-metadata=&amp;#34;0&amp;#34; data-input-position=&amp;#34;bottom&amp;#34; data-theme=&amp;#34;light&amp;#34; data-lang=&amp;#34;zh-CN&amp;#34; crossorigin=&amp;#34;anonymous&amp;#34; async&amp;gt; &amp;lt;/script&amp;gt;  ⚠️注意：为了使下面的 javascript 脚本生效，data-theme 选择 light；或者你可以根据你选择的主题修改下面的 javascript 脚本。
 自动主题  使用一个 div 作为评论区域的容器  &amp;lt;div class=&amp;#34;giscus_comments&amp;#34;&amp;gt; {{- partial &amp;#34;comments.</description>
    </item>
    
    <item>
      <title>构造 Kubernetes 客户端实例</title>
      <link>https://srcio.cn/series/programming-kubernetes/kube-client/</link>
      <pubDate>Sat, 01 Oct 2022 23:39:15 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/kube-client/</guid>
      <description>本节介绍 Golang 程序如何通过 rest.Config 实例获取各种类型的 Kubernetes 客户端实例。 通过客户端访问 Kubernetes 中的 API 资源实例。
Clientset  获取 *kubernetes.Clientset
推荐使用该客户端实例去操作 K8s API 资源。  package main import ( &amp;#34;k8s.io/client-go/kubernetes&amp;#34; &amp;#34;k8s.io/client-go/rest&amp;#34; ) func Clientset(config *rest.Config) *kubernetes.Clientset { client, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } return client } 获取 *rest.RESTClient
可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。  package main import &amp;#34;k8s.io/client-go/rest&amp;#34; func RESTClient(config *rest.Config) *rest.RESTClient { client, err := rest.RESTClientFor(config) if err != nil { panic(err) } return client } DiscoveryClient DiscoveryClient 动态客户端，通过动态指定 GVR 来操作任意的 Kubernetes 资源（内置资源 + CR）</description>
    </item>
    
    <item>
      <title>Kubernetes API 设计</title>
      <link>https://srcio.cn/series/programming-kubernetes/api-design/</link>
      <pubDate>Sun, 02 Oct 2022 01:28:03 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/api-design/</guid>
      <description>术语 Group
API 资源置于某个分组下，组作为相关功能的集合。一个组包含一个或多个版本。
Version
API 资源的版本，API 资源版本是会不断迭代的。
Kind
API 资源的的类型，用于存储 API 资源的描述信息或状态等。同一个 Kind 的 API 资源可以有多个版本，随着版本的不断迭代，Kind 代表的资源的会有字段内容的更改。
GVK
Group/Version/Kind，例如 Deployment：
apiVersion:apps/v1kind:Deploymentmetadata:- name:deploy-1... 上面的代码示例描述了一个 API 资源对象，这个资源对象：
 Group 是 apps Version 是 v1 Kind 是 Deployment。   Resource
代表 API 资源，与 GVK 一对一的关系。
GVR
可以将 GVK 比作是一个类，GVR 就是这个 GVK 类的实例。
当我们以 REST 的方式向发起 API 资源的请求是，请求 URL 格式一般类似这样：/api/apps/v1/deployments，里面就包含了三个上面提到的术语概念：
 /apps：请求资源所在的组（Group） /v1：请求资源的版本（Version） /deployments：请求的资源的名称（Resource）  </description>
    </item>
    
    <item>
      <title>Kuberentes Operator</title>
      <link>https://srcio.cn/series/programming-kubernetes/operator/</link>
      <pubDate>Sun, 02 Oct 2022 01:12:34 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/operator/</guid>
      <description></description>
    </item>
    
    <item>
      <title>使用 client-gen 生成 clientset 代码</title>
      <link>https://srcio.cn/series/programming-kubernetes/client-gen-usage/</link>
      <pubDate>Sat, 08 Oct 2022 17:37:07 +0800</pubDate>
      
      <guid>https://srcio.cn/series/programming-kubernetes/client-gen-usage/</guid>
      <description>本页是这篇Kubernetes 文档中一些内容摘要。
大致分为 3 个步骤：
 为 API 类型结构做 tag 标签 在 API 类型例如 pkg/apis/${Group}/${Version}/types.go 中的 Pod 结构体上打标签，支持的标签：   // +genclient：生成客户端函数（包括 Create, Update, Delete, DeleteCollection, Get, List, Update, Patch, Watch，如果 API 类型结构中包括 .Status 字段，还会额外生成 UpdateStatus 函数）； // +genclient:nonNamespaced：指定 API 类型是集群级别而不是命名空间级别的，生成的客户端函数都没有命名空间； // +genclient:onlyVerbs=create,get：只生成 Create, Get 客户端函数； // +genclient:skipVerbs=watch：生成除了 Watch 之外的所有其他客户端函数； // +genclient:noStatus：即使 API 类型结构包含 .Status 字段，也不生成 UpdateStatus 客户端函数； 有些情况下，可能你想要额外生成非标准的客户端函数，例如子资源函数，那么你需要使用下列这些 tag 标签： // +genclient:method=Scale,verb=update,subresource=scale,input=k8s.io/api/extensions/v1beta1.Scale,result=k8s.io/api/extensions/v1beta1.Scale：该例中使用标签，将会自动生成 Scale(string, *v1beta.Scale) *v1beta.Scale 客户端函数，里面配置了子资源函数的输入和输出参数。 另外，以下的 tag 标签也影响着客户端代码的生成： // +groupName=policy.</description>
    </item>
    
    
    
  </channel>
</rss>

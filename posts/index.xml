<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>文章 on 博客 · 丁鹏</title>
    <link>https://srcio.cn/posts/</link>
    <description>Recent content in 文章 on 博客 · 丁鹏</description>
    <image>
      <url>https://srcio.cn/cover.png</url>
      <link>https://srcio.cn/cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 10 Oct 2022 19:21:58 +0800</lastBuildDate><atom:link href="https://srcio.cn/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Docker Compose 实践</title>
      <link>https://srcio.cn/posts/docker-compose/</link>
      <pubDate>Mon, 10 Oct 2022 19:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/docker-compose/</guid>
      <description>安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。
使用一下命令安装或升级 Docker Compose（linux）：
 Ubuntu，Debian：  sudo apt update sudo apt install docker-compose-plugin  基于 RPM 发行版:  sudo yum update sudo yum install docker-compose-plugin 验证安装版本：
docker-compose version 常用命令 运行
docker-compose up 查看运行
docker-compose ps 停止
docker-compose stop 启动&amp;amp;重启
docker-compose start docker-compose restart 退出
docker-compose down 使用 docker-compose -h 查看更多命令及参数。
实践 使用 Docker Compose 运行一个简单的 golang web 程序。
 程序初始化  mkdir docker-compose-go-demo cd docker-compose-go-demo go mod init docker-compose-go-demo 创建 main.</description>
    </item>
    
    <item>
      <title>K8s 集群规划之节点资源配置</title>
      <link>https://srcio.cn/posts/k8s-node-resource-config/</link>
      <pubDate>Mon, 10 Oct 2022 19:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/k8s-node-resource-config/</guid>
      <description>文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size
 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。
集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。
例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。
 例如，因为要在集群上运行的应用程序需要此数量的资源。
 以下是实现集群的两种可能方法：
  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。
究竟哪种配置方式更好呢？
为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。
 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。
 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。
在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。
优势 让我们来看看这种方法可能具有的优势。
更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。
但请注意，这主要适用于裸机服务器而不适用于云实例。
如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。
更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。</description>
    </item>
    
    <item>
      <title>Kubelet 垃圾回收原理剖析</title>
      <link>https://srcio.cn/posts/kubelet-recycle-policy/</link>
      <pubDate>Mon, 10 Oct 2022 19:21:58 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/kubelet-recycle-policy/</guid>
      <description>文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html
 Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。
镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。
主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。
需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。
用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：
image-gc-high-threshold`：磁盘使用率上限，有效范围 [0-100]，默认 `85 image-gc-low-threshold`：磁盘使用率下限，有效范围 [0-100]，默认 `80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟
实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。
实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。
# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf .</description>
    </item>
    
    <item>
      <title>使用 Giscus 作为博客评论系统</title>
      <link>https://srcio.cn/posts/use-giscus/</link>
      <pubDate>Wed, 28 Sep 2022 09:02:43 +0800</pubDate>
      
      <guid>https://srcio.cn/posts/use-giscus/</guid>
      <description>Giscus  开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载  必要条件  你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus.app，安装的时候，分配你的博客所用的 GitHub 仓库即可。   当然，如果你的博客没有托管在 Github 上，你也可以单独创建一个 Github 仓库作为开通 giscus 评论。
 使用姿势  在 giscus.app 做自定义配置，填入你的仓库名称，选择主题等，Giscus 会自动帮你生成 javascript 脚本； Hugo 博客目录下，创建 layouts/partials/comments.html 文件，写入获取的脚本：  &amp;lt;script src=&amp;#34;https://giscus.app/client.js&amp;#34; data-repo=&amp;#34;[在此输入仓库]&amp;#34; data-repo-id=&amp;#34;[在此输入仓库 ID]&amp;#34; data-category=&amp;#34;[在此输入分类名]&amp;#34; data-category-id=&amp;#34;[在此输入分类 ID]&amp;#34; data-mapping=&amp;#34;pathname&amp;#34; data-strict=&amp;#34;0&amp;#34; data-reactions-enabled=&amp;#34;1&amp;#34; data-emit-metadata=&amp;#34;0&amp;#34; data-input-position=&amp;#34;bottom&amp;#34; data-theme=&amp;#34;light&amp;#34; data-lang=&amp;#34;zh-CN&amp;#34; crossorigin=&amp;#34;anonymous&amp;#34; async&amp;gt; &amp;lt;/script&amp;gt;  ⚠️注意：为了使下面的 javascript 脚本生效，data-theme 选择 light；或者你可以根据你选择的主题修改下面的 javascript 脚本。
 自动主题  使用一个 div 作为评论区域的容器  &amp;lt;div class=&amp;#34;giscus_comments&amp;#34;&amp;gt; {{- partial &amp;#34;comments.</description>
    </item>
    
  </channel>
</rss>

<!doctype html><html lang=zh dir=auto>
<head>
<link rel=stylesheet href=/fontawesome/css/all.min.css crossorigin=anonymous><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>K8s 集群规划之节点资源配置 | 博客 · 丁鹏</title>
<meta name=keywords content="kubelet,docker image,镜像回收">
<meta name=description content="文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size
 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。
集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。
例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。
 例如，因为要在集群上运行的应用程序需要此数量的资源。
 以下是实现集群的两种可能方法：
  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。
究竟哪种配置方式更好呢？
为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。
 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。
 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。
在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。
优势 让我们来看看这种方法可能具有的优势。
更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。
但请注意，这主要适用于裸机服务器而不适用于云实例。
如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。
更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。">
<meta name=author content="丁鹏">
<link rel=canonical href=https://srcio.cn/posts/k8s-node-resource-config/>
<meta name=baidu-site-verification content="code-yW1u8Rg4sz">
<meta name=google-site-verification content="qnXhMaPNdhaZ4A36ptMVUn-0o111dAsJqugH_0ZCYvM">
<link crossorigin=anonymous href=/assets/css/stylesheet.8d66a0f10892977540fe89e072a17e78812f4f0e1c942295458d63f8984d5e21.css integrity="sha256-jWag8QiSl3VA/ongcqF+eIEvTw4clCKVRY1j+JhNXiE=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://srcio.cn/images/blog.png>
<link rel=icon type=image/png sizes=16x16 href=https://srcio.cn/images/blog.png>
<link rel=icon type=image/png sizes=32x32 href=https://srcio.cn/images/blog.png>
<link rel=apple-touch-icon href=https://srcio.cn/images/blog.png>
<link rel=mask-icon href=https://srcio.cn/images/blog.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="K8s 集群规划之节点资源配置">
<meta property="og:description" content="文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size
 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。
集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。
例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。
 例如，因为要在集群上运行的应用程序需要此数量的资源。
 以下是实现集群的两种可能方法：
  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。
究竟哪种配置方式更好呢？
为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。
 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。
 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。
在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。
优势 让我们来看看这种方法可能具有的优势。
更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。
但请注意，这主要适用于裸机服务器而不适用于云实例。
如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。
更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://srcio.cn/posts/k8s-node-resource-config/"><meta property="og:image" content="https://srcio.cn/cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-10-17T20:21:58+08:00">
<meta property="article:modified_time" content="2022-10-17T20:21:58+08:00"><meta property="og:site_name" content="博客">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://srcio.cn/cover.png">
<meta name=twitter:title content="K8s 集群规划之节点资源配置">
<meta name=twitter:description content="文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size
 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。
集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。
例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。
 例如，因为要在集群上运行的应用程序需要此数量的资源。
 以下是实现集群的两种可能方法：
  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。
究竟哪种配置方式更好呢？
为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。
 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。
 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。
在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。
优势 让我们来看看这种方法可能具有的优势。
更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。
但请注意，这主要适用于裸机服务器而不适用于云实例。
如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。
更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://srcio.cn/posts/"},{"@type":"ListItem","position":2,"name":"K8s 集群规划之节点资源配置","item":"https://srcio.cn/posts/k8s-node-resource-config/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"K8s 集群规划之节点资源配置","name":"K8s 集群规划之节点资源配置","description":"文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size\n 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。\n集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。\n例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。\n 例如，因为要在集群上运行的应用程序需要此数量的资源。\n 以下是实现集群的两种可能方法：\n  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。\n究竟哪种配置方式更好呢？\n为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。\n 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。\n 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。\n在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。\n优势 让我们来看看这种方法可能具有的优势。\n更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。\n但请注意，这主要适用于裸机服务器而不适用于云实例。\n如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。\n更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。","keywords":["kubelet","docker image","镜像回收"],"articleBody":" 文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size\n 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。\n集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。\n例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。\n 例如，因为要在集群上运行的应用程序需要此数量的资源。\n 以下是实现集群的两种可能方法：\n  通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。\n究竟哪种配置方式更好呢？\n为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。\n 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。\n 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。\n在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。\n优势 让我们来看看这种方法可能具有的优势。\n更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。\n但请注意，这主要适用于裸机服务器而不适用于云实例。\n如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。\n更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。\n但请注意，如果您使用云实例，这个原则可能并不适用。\n在主要的云提供商 Amazon Web Services、Google Cloud Platform 和 Microsoft Azure 的当前定价方案中，实例价格会随容量线性增加。例如，在 Google Cloud Platform 上，64 个 n1-standard-1 实例的成本与单个 n1-standard-64 实例完全相同——这两种方式都为您提供 64 个 CPU 和 240GB 内存。因此，在云上，您通常无法通过使用更大的机器来节省资金投入。\n可运行饥饿型应用 具备大型节点可能只是您要在集群中运行的应用程序类型的需求。\n例如，如果您有一个需要 8GB 内存的机器学习应用程序，则无法在仅具有 1GB 内存的节点的集群上运行它。但是，您可以在具有 10GB 内存节点的集群上运行它。\n劣势 看完了优势，让我们再来看看劣势。\n单节点运行大量 Pod 在较少的节点上运行相同的工作负载自然意味着在每个节点上运行更多的 Pod。\n这可能会成为一个问题。\n原因是每个 Pod 都会为在节点上运行的 Kubernetes 代理程序引入一些开销——例如容器运行时（如 Docker）、kubelet 和 cAdvisor。\n例如，kubelet 对节点上的每个容器执行周期性的 liveness 和 readiness 探测——更多容器意味着在每轮迭代中 kubelet 将执行更多工作。cAdvisor 收集节点上所有容器的资源使用统计信息，并且 kubelet 定期查询此信息并在其 API 上公开它——再次，这意味着每轮迭代中 cAdvisor 和 kubelet 的工作量都会增加。\n随着 Pod 数量的增长，这些问题的聚积可能会开始减慢系统速度，甚至使集群系统变得不可靠。\n  有报告称，节点被报告为未就绪，是因为周期性的的 kubelet 运行状况检查花费了太长时间来迭代节点上的所有容器。\n出于这些原因，Kubernetes 官方建议每个节点最多 110 个 Pod。对于这个数字，Kubernetes 已经进行过相关测试，可以在一般类型的节点上可靠地工作。\n当然，如果节点的性能足够好，您可能也能够成功地让每个节点运行更多的 Pod ——但很难预测这是否能够顺利进行，也许会遇到一些问题。\n大多数托管 Kubernetes 服务甚至对每个节点的 Pod 数量施加了严格的限制：\n 在 Amazon Elastic Kubernetes Service（EKS）上，每个节点的最大 Pod 数取决于节点类型，范围从 4 到 737。 在 Google Kubernetes Engine（GKE）上，无论节点类型如何，每个节点的限制为 100 个 Pod。 在 Azure Kubernetes Service（AKS）上，默认限制是每个节点 30 个 Pod，但最多可以增加到 250 个。  因此，如果您计划为每个节点运行大量 Pod，则应该事先进行测试，看能否按预期那样工作。\n有限的副本数量 较少的节点可能会限制应用程序的副本数量。\n例如，如果您有一个由 5 个副本组成的高可用应用程序，但您只有 2 个节点，那么应用程序的有效副本数量将减少到 2。这是因为 5 个副本只能分布在 2 个节点上，如果其中一个节点失败，它可能会同时挂掉该节点上的多个副本。相反，如果您有至少 5 个节点，则每个副本可以在单独的节点上运行，并且单个节点的故障最多只会挂掉其中一个副本。\n因此，如果您有高可用要求，则可能需要集群节点数大于某个下限值。\n更高的爆炸半径 如果您只有几个工作节点，那么节点失败造成的影响比使用大量节点时的影响要大。\n例如，如果您只有两个节点，那其中一个节点出现故障，就意味着一半的节点会消失。Kubernetes 可以将失败节点的工作负载重新安排到其他节点。但是，如果您只有几个节点，那风险也会增加，因为剩余节点上可能没有足够的备用资源容量来容纳故障节点的所有工作负载。结果是，部分应用程序将永久停机，直到再次启动故障节点。\n因此，如果您想减少硬件故障的影响，则应该选择更多的节点。\n更大的资源伸缩增量 Kubernetes 为云基础架构提供了 Cluster Autoscaler，允许根据当前需求自动添加或删除节点。如果使用大型节点，则会有较大的资源伸缩增量，这会使资源扩缩容更加笨重。\n例如，如果您只有 2 个节点，则添加节点意味着将群集容量增加 50%。这可能比您实际需要的资源多得多，就意味着您需要为未使用的资源付费。\n因此，如果您计划使用集群的自动弹性伸缩功能，则较小的节点允许您进行更轻量且经济高效的资源扩缩容。\n更多的低配节点 在讨论了更少高配节点的优缺点之后，让我们转向更多低配节点的场景。\n这种方法通过许多低配节点构建集群，而不使用更少的高配节点。\n*那它的优缺点又是什么呢？*\n优势 使用更多低配节点的优点正对应于使用更少高配节点的缺点。\n减少爆炸半径 如果您有更多节点，则每个节点上的 Pod 自然会更少。\n例如，如果您有 100 个 Pod 和 10 个节点，则每个节点平均只包含 10 个 Pod。这样，即便其中一个节点发生故障，它的影响也仅限于总工作负载的较小的一部分。有可能只有部分应用程序受到影响，并且可能只有少量副本挂掉，因此整个应用程序会仍然保持运行状态。\n此外，剩余节点上的备用资源很可能足以容纳故障节点的工作负载，因此Kubernetes 可以重新安排所有 Pod，并且您的应用程序可以相对快速地恢复到完全正常的运行状态。\n允许更多副本 如果您有一个多副本高可用应用程序以及足够的可用节点，Kubernetes 调度程序可以将每个副本分配给不同的节点。\n 您可以通过节点亲和、Pod 亲和/反亲和以及污点和容忍来影响调度程序对 Pod 的调度。\n 这意味着如果某个节点出现故障，则最多只有一个副本受影响，且您的应用程序仍然可用。\n劣势 看了使用更多低配节点的优点，那它有什么缺点呢？\n较大的节点数量 如果使用较小的节点，则自然需要更多节点来实现给定的集群容量。\n但是大量节点对 Kubernetes 控制平面来说可能是一个挑战。\n例如，每个节点都需要能够与其他节点通信，这使得可能的通信路径数量会按照节点数量的平方增长——所有节点都必须由控制平面管理。\nKubernetes controller manager 中的节点控制器定期遍历集群中的所有节点以运行状况检查——更多节点意味着节点控制器的负载更多。\n更多节点同时也意味着 etcd 数据库上的负载也更多——每个 kubelet 和 kube-proxy 都会成为一个 etcd 的 watcher 客户端（通过 APIServer），etcd 必须广播对象变化到这些客户端。\n通常，每个工作节点都会对主节点上的系统组件施加一些开销。\n  据官方统计，Kubernetes 声称支持最多 5000 个节点的集群。然而，在实践中，500 个节点可能已经形成了巨大的挑战。\n通过使用性能更高的主节点，往往可以减轻大量工作节点带来的影响。这也正是目前在实践中所应用的——这里是 kube-up 在云基础架构上使用的主节点大小：\n Google Cloud Platform  5 个工作节点 → n1-standard-1 主节点 500 个工作节点 → n1-standard-32 主节点   Amazon Web Services  5 个工作节点 → m3.medium 主节点 500 个工作节点 → c4.8xlarge 主节点    如您所见，对于 500 个工作节点，使用的主节点分别具有 32 和 36 个 CPU 以及 120GB 和 60GB 内存。\n这些都是相当大的机器！\n因此，如果您打算使用大量低配节点，则需要记住两件事：\n 您拥有的工作节点越多，主节点需要的性能就越高 如果您计划使用超过 500 个节点，则可能会遇到一些需要付出一些努力才能解决的性能瓶颈   像 Virtual Kubelet 这样的新开发产品允许您绕过这些限制，以构建具有大量工作节点的集群。\n 更多的系统开销 Kubernetes 在每个工作节点上运行一组系统守护进程——包括容器运行时（如 Docker）、kube-proxy 和包含 cAdvisor 的 kubelet。\n cAdvisor 包含在 kubelet 二进制文件中。\n 所有这些守护进程一起消耗固定数量的资源。\n如果使用许多低配节点，则这些系统组件消耗的资源占比会增大。\n例如，假设单个节点的所有系统守护程序一起使用 0.1 个 CPU 和 0.1GB 内存。如果您拥有 10 个 CPU 和 10GB 内存的单个节点，那么守护程序将占用集群容量的 1%。而如果您有 1 个 CPU 和 1GB 内存的 10 个节点，则后台程序将占用集群容量的 10%。在第二种情况下，10% 的资源消耗用于运行系统，而在第一种情况下，它只占 1%。\n因此，如果您希望最大化基础架构支出的回报，那么您可能会喜欢更少的节点。\n更低的资源利用率 如果您使用较小的节点，那么可能会产生大量资源碎片因资源太少而无法分配给任何工作负载，最终保持未使用状态。\n例如，假设您的所有 Pod 都需要 0.75GB 的内存。如果您有 10 个 1GB 内存的节点，那么最多可以运行 10 个这样的 Pod——您最终会在每个节点上有一块 0.25GB 的内存不能使用。这意味着，集群总内存的 25% 被浪费了。相反，如果您使用具有 10GB 内存的单个节点，那么您可以运行 13 个 Pod ——您最终会在这个节点上有一块 0.25GB 的内存不能使用。在这种情况下，您只会浪费 2.5% 的内存。\n因此，如果您想最大限度地减少资源浪费，使用更大的节点可能会带来更好的结果。\nPod 运行数量限制 在某些云基础架构上，低配节点上允许的最大 Pod 数量比您预期的要限制得更多。\nAmazon Elastic Kubernetes Service（EKS）就是这种情况，其中每个节点的最大 Pod 数取决于实例类型。\n例如，对于 t2.medium 实例，最大 Pod 数为 17，t2.small 为 11，而 t2.micro为 4。\n这些都是非常小的数字！\n任何超出这些限制的 Pod 都无法由 Kubernetes 调度，并被无限期地保持在 Pending 状态。\n如果您不了解这些限制，则可能导致难以发现的错误。\n因此，如果您计划在 Amazon EKS 上使用低配节点，请检查相应的每节点 Pod 数量限制，并计算节点是否可以容纳所有 Pod。\n结论 那么，您应该在集群中使用更少的高配节点还是更多的低配节点呢？\n这没有明确的答案。\n您要部署到集群的应用程序类型可能会影响您的决策。\n例如，如果您的应用程序需要 10GB 内存，则可能不应使用低配节点——集群中的节点应至少具有 10GB 内存。或者，如果您的应用程序需要 10 副本以实现高可用，那么您可能不应该只使用 2 个节点——您的集群应该至少有 10 个节点。\n对于中间的所有场景，它取决于您的具体需求。\n以上哪项优缺点与您相关？哪项与您不相关？\n话虽如此，但没有规则说所有节点必须具有相同的大小。没有什么能阻止您使用不同大小的节点来混合构建集群。Kubernetes 集群的工作节点可以是完全异构的。这可能会让您权衡两种方法的优缺点。\n最后，实践是检验真理的唯一标准——最好的方法是反复试验并找到最适合您的资源配置组合！\n","wordCount":"448","inLanguage":"zh","datePublished":"2022-10-17T20:21:58+08:00","dateModified":"2022-10-17T20:21:58+08:00","author":[{"@type":"Person","name":"丁鹏"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://srcio.cn/posts/k8s-node-resource-config/"},"publisher":{"@type":"Organization","name":"博客 · 丁鹏","logo":{"@type":"ImageObject","url":"https://srcio.cn/images/blog.png"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://srcio.cn/ accesskey=h title="博客 (Alt + H)">
<img src=https://srcio.cn/images/blog.png alt aria-label=logo height=25>博客</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://srcio.cn/ title=主页>
<span>
<i class="fa-solid fa-home"></i>&nbsp;主页</span>
</a>
</li>
<li>
<a href=https://srcio.cn/posts/ title=文章>
<span>
<i class="fa-solid fa-feather"></i>&nbsp;文章</span>
</a>
</li>
<li>
<a href=https://srcio.cn/series/ title=系列>
<span>
<i class="fa-solid fa-swatchbook"></i>&nbsp;系列</span>
</a>
</li>
<li>
<a href=https://srcio.cn/tags/ title=标签>
<span>
<i class="fa-solid fa-tags"></i>&nbsp;标签</span>
</a>
</li>
<li>
<a href=https://srcio.cn/archives/ title=归档>
<span>
<i class="fa-solid fa-folder-minus"></i>&nbsp;归档</span>
</a>
</li>
<li>
<a href=https://srcio.cn/search/ title>
<span>
<i class="fa-solid fa-magnifying-glass"></i>&nbsp;</span>
</a>
</li>
</ul>
</nav>
</header><main class=main>
<article class=post-single>
<div class=post-left-article>
<header class=post-header>
<div class=breadcrumbs><p><a href=/>主页</a>
<a href=https://srcio.cn/posts/><i class="fa-solid fa-angle-right"></i>&nbsp;文章</a>&nbsp;<span><i class="fa-solid fa-angle-right"></i>&nbsp;K8s 集群规划之节点资源配置</span></p>
</div>
<h1 id=post-title class=post-title>
K8s 集群规划之节点资源配置
</h1>
<div class=post-meta><span class=meta-item>
<span>
<i class="fa-solid fa-calendar-day"></i>
2022-10-17
</span>
</span><span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-clock"></i>&nbsp;3 分钟</span>
</span><span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-chart-simple"></i></i>&nbsp;448 字</span>
</span>
<span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-user"></i></i>&nbsp;丁鹏
</span>
</span><span class=meta-item>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-book-open-reader"></i>&nbsp;<span id=busuanzi_value_page_pv></span>
</span>&nbsp;&nbsp;|&nbsp;&nbsp;<a href=https://github.com/srcio/blog/edit/master/content/posts/k8s-node-resource-config.md rel="noopener noreferrer" target=_blank>
<i class="fa-regular fa-pen-to-square"></i>&nbsp;编辑</a>
</div>
</header>
<div class=post-content><blockquote>
<p>文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size</p>
</blockquote>
<p>在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。</p>
<h2 id=集群容量>集群容量<a hidden class=anchor aria-hidden=true href=#集群容量>#</a></h2>
<p>首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。</p>
<p>例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。</p>
<blockquote>
<p>例如，因为要在集群上运行的应用程序需要此数量的资源。</p>
</blockquote>
<p>以下是实现集群的两种可能方法：</p>
<p>
<input type=checkbox id=zoomCheck-1a698 hidden>
<label for=zoomCheck-1a698>
<img class=zoomCheck loading=lazy decoding=async src=https://srcio.oss-cn-hangzhou.aliyuncs.com/images/cluster-design.svg alt=img>
</label>
</p>
<p>通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。</p>
<p><strong>究竟哪种配置方式更好呢？</strong></p>
<p>为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。</p>
<blockquote>
<p>请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。</p>
</blockquote>
<h2 id=更少的高配节点>更少的高配节点<a hidden class=anchor aria-hidden=true href=#更少的高配节点>#</a></h2>
<p>这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。</p>
<p>在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。</p>
<h3 id=优势>优势<a hidden class=anchor aria-hidden=true href=#优势>#</a></h3>
<p><em>让我们来看看这种方法可能具有的优势。</em></p>
<h4 id=更少的管理开销>更少的管理开销<a hidden class=anchor aria-hidden=true href=#更少的管理开销>#</a></h4>
<p>简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。</p>
<p><em>但请注意，这主要适用于裸机服务器而不适用于云实例。</em></p>
<p>如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。</p>
<h4 id=更低的单节点成本>更低的单节点成本<a hidden class=anchor aria-hidden=true href=#更低的单节点成本>#</a></h4>
<p>虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。</p>
<p><em>但请注意，如果您使用云实例，这个原则可能并不适用。</em></p>
<p>在主要的云提供商 <a href=https://aws.amazon.com/ec2/pricing/on-demand/>Amazon Web Services</a>、<a href=https://cloud.google.com/compute/vm-instance-pricing>Google Cloud Platform</a> 和 <a href=https://azure.microsoft.com/en-us/pricing/calculator/#virtual-machines>Microsoft Azure</a> 的当前定价方案中，实例价格会随容量线性增加。例如，在 Google Cloud Platform 上，64 个 <code>n1-standard-1</code> 实例的成本与单个 <code>n1-standard-64</code> 实例完全相同——这两种方式都为您提供 64 个 CPU 和 240GB 内存。因此，在云上，您通常无法通过使用更大的机器来节省资金投入。</p>
<h4 id=可运行饥饿型应用>可运行饥饿型应用<a hidden class=anchor aria-hidden=true href=#可运行饥饿型应用>#</a></h4>
<p>具备大型节点可能只是您要在集群中运行的应用程序类型的需求。</p>
<p>例如，如果您有一个需要 8GB 内存的机器学习应用程序，则无法在仅具有 1GB 内存的节点的集群上运行它。但是，您可以在具有 10GB 内存节点的集群上运行它。</p>
<h3 id=劣势>劣势<a hidden class=anchor aria-hidden=true href=#劣势>#</a></h3>
<p><em>看完了优势，让我们再来看看劣势。</em></p>
<h4 id=单节点运行大量-pod>单节点运行大量 Pod<a hidden class=anchor aria-hidden=true href=#单节点运行大量-pod>#</a></h4>
<p>在较少的节点上运行相同的工作负载自然意味着在每个节点上运行更多的 Pod。</p>
<p><em>这可能会成为一个问题。</em></p>
<p>原因是每个 Pod 都会为在节点上运行的 Kubernetes 代理程序引入一些开销——例如容器运行时（如 Docker）、kubelet 和 cAdvisor。</p>
<p>例如，kubelet 对节点上的每个容器执行周期性的 liveness 和 readiness 探测——更多容器意味着在每轮迭代中 kubelet 将执行更多工作。cAdvisor 收集节点上所有容器的资源使用统计信息，并且 kubelet 定期查询此信息并在其 API 上公开它——再次，这意味着每轮迭代中 cAdvisor 和 kubelet 的工作量都会增加。</p>
<p>随着 Pod 数量的增长，这些问题的聚积可能会开始减慢系统速度，甚至使集群系统变得不可靠。</p>
<p>
<input type=checkbox id=zoomCheck-903b2 hidden>
<label for=zoomCheck-903b2>
<img class=zoomCheck loading=lazy decoding=async src=https://srcio.oss-cn-hangzhou.aliyuncs.com/images/node-overload.svg alt=img>
</label>
</p>
<p>有报告称，<a href=https://github.com/kubernetes/kubernetes/issues/45419>节点被报告为未就绪</a>，是因为周期性的的 kubelet 运行状况检查花费了太长时间来迭代节点上的所有容器。</p>
<p>出于这些原因，Kubernetes 官方<a href=https://kubernetes.io/docs/setup/best-practices/cluster-large/>建议每个节点最多 110 个 Pod</a>。对于这个数字，Kubernetes 已经进行过相关测试，可以在一般类型的节点上可靠地工作。</p>
<p>当然，如果节点的性能足够好，您可能也能够成功地让每个节点运行更多的 Pod ——但很难预测这是否能够顺利进行，也许会遇到一些问题。</p>
<p><em>大多数托管 Kubernetes 服务甚至对每个节点的 Pod 数量施加了严格的限制：</em></p>
<ul>
<li>在 <a href=https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt>Amazon Elastic Kubernetes Service（EKS）</a>上，每个节点的最大 Pod 数取决于节点类型，范围从 4 到 737。</li>
<li>在 <a href=https://cloud.google.com/kubernetes-engine/quotas>Google Kubernetes Engine（GKE）</a>上，无论节点类型如何，每个节点的限制为 100 个 Pod。</li>
<li>在 <a href=https://docs.microsoft.com/bs-latn-ba/azure/aks/configure-azure-cni#maximum-pods-per-node>Azure Kubernetes Service（AKS）</a>上，默认限制是每个节点 30 个 Pod，但最多可以增加到 250 个。</li>
</ul>
<p>因此，如果您计划为每个节点运行大量 Pod，则应该事先进行测试，看能否按预期那样工作。</p>
<h4 id=有限的副本数量>有限的副本数量<a hidden class=anchor aria-hidden=true href=#有限的副本数量>#</a></h4>
<p>较少的节点可能会限制应用程序的副本数量。</p>
<p>例如，如果您有一个由 5 个副本组成的高可用应用程序，但您只有 2 个节点，那么应用程序的有效副本数量将减少到 2。这是因为 5 个副本只能分布在 2 个节点上，如果其中一个节点失败，它可能会同时挂掉该节点上的多个副本。相反，如果您有至少 5 个节点，则每个副本可以在单独的节点上运行，并且单个节点的故障最多只会挂掉其中一个副本。</p>
<p>因此，如果您有高可用要求，则可能需要集群节点数大于某个下限值。</p>
<h4 id=更高的爆炸半径>更高的爆炸半径<a hidden class=anchor aria-hidden=true href=#更高的爆炸半径>#</a></h4>
<p>如果您只有几个工作节点，那么节点失败造成的影响比使用大量节点时的影响要大。</p>
<p>例如，如果您只有两个节点，那其中一个节点出现故障，就意味着一半的节点会消失。Kubernetes 可以将失败节点的工作负载重新安排到其他节点。但是，如果您只有几个节点，那风险也会增加，因为剩余节点上可能没有足够的备用资源容量来容纳故障节点的所有工作负载。结果是，部分应用程序将永久停机，直到再次启动故障节点。</p>
<p>因此，如果您想减少硬件故障的影响，则应该选择更多的节点。</p>
<h4 id=更大的资源伸缩增量>更大的资源伸缩增量<a hidden class=anchor aria-hidden=true href=#更大的资源伸缩增量>#</a></h4>
<p>Kubernetes 为云基础架构提供了 Cluster Autoscaler，允许根据当前需求自动添加或删除节点。如果使用大型节点，则会有较大的资源伸缩增量，这会使资源扩缩容更加笨重。</p>
<p>例如，如果您只有 2 个节点，则添加节点意味着将群集容量增加 50%。这可能比您实际需要的资源多得多，就意味着您需要为未使用的资源付费。</p>
<p>因此，如果您计划使用集群的自动弹性伸缩功能，则较小的节点允许您进行更轻量且经济高效的资源扩缩容。</p>
<h2 id=更多的低配节点>更多的低配节点<a hidden class=anchor aria-hidden=true href=#更多的低配节点>#</a></h2>
<p><em>在讨论了更少高配节点的优缺点之后，让我们转向更多低配节点的场景。</em></p>
<p>这种方法通过许多低配节点构建集群，而不使用更少的高配节点。</p>
<p>*<strong>那它的优缺点又是什么呢？*</strong></p>
<h3 id=优势-1>优势<a hidden class=anchor aria-hidden=true href=#优势-1>#</a></h3>
<p>使用更多低配节点的优点正对应于使用更少高配节点的缺点。</p>
<h4 id=减少爆炸半径>减少爆炸半径<a hidden class=anchor aria-hidden=true href=#减少爆炸半径>#</a></h4>
<p>如果您有更多节点，则每个节点上的 Pod 自然会更少。</p>
<p>例如，如果您有 100 个 Pod 和 10 个节点，则每个节点平均只包含 10 个 Pod。这样，即便其中一个节点发生故障，它的影响也仅限于总工作负载的较小的一部分。有可能只有部分应用程序受到影响，并且可能只有少量副本挂掉，因此整个应用程序会仍然保持运行状态。</p>
<p>此外，剩余节点上的备用资源很可能足以容纳故障节点的工作负载，因此Kubernetes 可以重新安排所有 Pod，并且您的应用程序可以相对快速地恢复到完全正常的运行状态。</p>
<h4 id=允许更多副本>允许更多副本<a hidden class=anchor aria-hidden=true href=#允许更多副本>#</a></h4>
<p>如果您有一个多副本高可用应用程序以及足够的可用节点，Kubernetes 调度程序可以将每个副本分配给不同的节点。</p>
<blockquote>
<p>您可以通过<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity>节点亲和</a>、<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity>Pod 亲和/反亲和</a>以及<a href=https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/>污点和容忍</a>来影响调度程序对 Pod 的调度。</p>
</blockquote>
<p>这意味着如果某个节点出现故障，则最多只有一个副本受影响，且您的应用程序仍然可用。</p>
<h3 id=劣势-1>劣势<a hidden class=anchor aria-hidden=true href=#劣势-1>#</a></h3>
<p><em>看了使用更多低配节点的优点，那它有什么缺点呢？</em></p>
<h4 id=较大的节点数量>较大的节点数量<a hidden class=anchor aria-hidden=true href=#较大的节点数量>#</a></h4>
<p>如果使用较小的节点，则自然需要更多节点来实现给定的集群容量。</p>
<p><em>但是大量节点对 Kubernetes 控制平面来说可能是一个挑战。</em></p>
<p>例如，每个节点都需要能够与其他节点通信，这使得可能的通信路径数量会按照节点数量的平方增长——所有节点都必须由控制平面管理。</p>
<p>Kubernetes controller manager 中的节点控制器定期遍历集群中的所有节点以运行状况检查——更多节点意味着节点控制器的负载更多。</p>
<p>更多节点同时也意味着 etcd 数据库上的负载也更多——每个 kubelet 和 kube-proxy 都会成为一个 etcd 的 <a href=https://etcd.io/docs/v3.3.12/dev-guide/interacting_v3/#watch-key-changes>watcher</a> 客户端（通过 APIServer），etcd 必须广播对象变化到这些客户端。</p>
<p>通常，每个工作节点都会对主节点上的系统组件施加一些开销。</p>
<p>
<input type=checkbox id=zoomCheck-4160d hidden>
<label for=zoomCheck-4160d>
<img class=zoomCheck loading=lazy decoding=async src=https://srcio.oss-cn-hangzhou.aliyuncs.com/images/cluster-overload.svg alt=img>
</label>
</p>
<p>据官方统计，Kubernetes 声称<a href=https://kubernetes.io/docs/setup/best-practices/cluster-large/>支持最多 5000 个节点的集群</a>。然而，在实践中，500 个节点可能已经形成了<a href=https://www.lfasiallc.com/wp-content/uploads/2017/11/BoF_-Not-One-Size-Fits-All-How-to-Size-Kubernetes-Clusters_Guang-Ya-Liu-_-Sahdev-Zala.pdf>巨大的挑战</a>。</p>
<p>通过使用性能更高的主节点，往往可以减轻大量工作节点带来的影响。这也正是目前在实践中所应用的——这里是 <code>kube-up</code> 在云基础架构上使用的<a href=https://kubernetes.io/docs/setup/best-practices/cluster-large/#size-of-master-and-master-components>主节点大小</a>：</p>
<ul>
<li>Google Cloud Platform
<ul>
<li>5 个工作节点 → <code>n1-standard-1</code> 主节点</li>
<li>500 个工作节点 → <code>n1-standard-32</code> 主节点</li>
</ul>
</li>
<li>Amazon Web Services
<ul>
<li>5 个工作节点 → <code>m3.medium</code> 主节点</li>
<li>500 个工作节点 → <code>c4.8xlarge</code> 主节点</li>
</ul>
</li>
</ul>
<p>如您所见，对于 500 个工作节点，使用的主节点分别具有 32 和 36 个 CPU 以及 120GB 和 60GB 内存。</p>
<p><em>这些都是相当大的机器！</em></p>
<p>因此，如果您打算使用大量低配节点，则需要记住两件事：</p>
<ul>
<li>您拥有的工作节点越多，主节点需要的性能就越高</li>
<li>如果您计划使用超过 500 个节点，则可能会遇到一些需要付出一些努力才能解决的性能瓶颈</li>
</ul>
<blockquote>
<p>像 <a href="https://www.youtube.com/watch?v=v9cwYvuzROs">Virtual Kubelet</a> 这样的新开发产品允许您绕过这些限制，以构建具有大量工作节点的集群。</p>
</blockquote>
<h4 id=更多的系统开销>更多的系统开销<a hidden class=anchor aria-hidden=true href=#更多的系统开销>#</a></h4>
<p>Kubernetes 在每个工作节点上运行一组系统守护进程——包括容器运行时（如 Docker）、kube-proxy 和包含 cAdvisor 的 kubelet。</p>
<blockquote>
<p>cAdvisor 包含在 kubelet 二进制文件中。</p>
</blockquote>
<p>所有这些守护进程一起消耗固定数量的资源。</p>
<p>如果使用许多低配节点，则这些系统组件消耗的资源占比会增大。</p>
<p>例如，假设单个节点的所有系统守护程序一起使用 0.1 个 CPU 和 0.1GB 内存。如果您拥有 10 个 CPU 和 10GB 内存的单个节点，那么守护程序将占用集群容量的 1%。而如果您有 1 个 CPU 和 1GB 内存的 10 个节点，则后台程序将占用集群容量的 10%。在第二种情况下，10% 的资源消耗用于运行系统，而在第一种情况下，它只占 1%。</p>
<p>因此，如果您希望最大化基础架构支出的回报，那么您可能会喜欢更少的节点。</p>
<h4 id=更低的资源利用率>更低的资源利用率<a hidden class=anchor aria-hidden=true href=#更低的资源利用率>#</a></h4>
<p>如果您使用较小的节点，那么可能会产生大量资源碎片因资源太少而无法分配给任何工作负载，最终保持未使用状态。</p>
<p>例如，假设您的所有 Pod 都需要 0.75GB 的内存。如果您有 10 个 1GB 内存的节点，那么最多可以运行 10 个这样的 Pod——您最终会在每个节点上有一块 0.25GB 的内存不能使用。这意味着，集群总内存的 25% 被浪费了。相反，如果您使用具有 10GB 内存的单个节点，那么您可以运行 13 个 Pod ——您最终会在这个节点上有一块 0.25GB 的内存不能使用。在这种情况下，您只会浪费 2.5% 的内存。</p>
<p>因此，如果您想最大限度地减少资源浪费，使用更大的节点可能会带来更好的结果。</p>
<h4 id=pod-运行数量限制>Pod 运行数量限制<a hidden class=anchor aria-hidden=true href=#pod-运行数量限制>#</a></h4>
<p>在某些云基础架构上，低配节点上允许的最大 Pod 数量比您预期的要限制得更多。</p>
<p><a href=https://aws.amazon.com/eks/>Amazon Elastic Kubernetes Service（EKS）</a>就是这种情况，其中<a href=https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt>每个节点的最大 Pod 数</a>取决于实例类型。</p>
<p>例如，对于 <code>t2.medium</code> 实例，最大 Pod 数为 17，<code>t2.small</code> 为 11，而 <code>t2.micro</code>为 4。</p>
<p><em>这些都是非常小的数字！</em></p>
<p>任何超出这些限制的 Pod 都无法由 Kubernetes 调度，并被无限期地保持在 Pending 状态。</p>
<p>如果您不了解这些限制，则可能导致难以发现的错误。</p>
<p>因此，如果您计划在 Amazon EKS 上使用低配节点，请<a href=https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt>检查相应的每节点 Pod 数量限制</a>，并计算节点是否可以容纳所有 Pod。</p>
<h2 id=结论>结论<a hidden class=anchor aria-hidden=true href=#结论>#</a></h2>
<p><em>那么，您应该在集群中使用更少的高配节点还是更多的低配节点呢？</em></p>
<p>这没有明确的答案。</p>
<p>您要部署到集群的应用程序类型可能会影响您的决策。</p>
<p>例如，如果您的应用程序需要 10GB 内存，则可能不应使用低配节点——集群中的节点应至少具有 10GB 内存。或者，如果您的应用程序需要 10 副本以实现高可用，那么您可能不应该只使用 2 个节点——您的集群应该至少有 10 个节点。</p>
<p>对于中间的所有场景，它取决于您的具体需求。</p>
<p><em>以上哪项优缺点与您相关？哪项与您不相关？</em></p>
<p>话虽如此，但没有规则说所有节点必须具有相同的大小。没有什么能阻止您使用不同大小的节点来混合构建集群。Kubernetes 集群的工作节点可以是完全异构的。这可能会让您权衡两种方法的优缺点。</p>
<p>最后，实践是检验真理的唯一标准——最好的方法是反复试验并找到最适合您的资源配置组合！</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://srcio.cn/tags/kubernetes/><i class="fa-solid fa-tag"></i>&nbsp;Kubernetes</a></li>
</ul>
<nav class=paginav>
<a class=next href=https://srcio.cn/posts/kubelet-recycle-policy/>
<strong class=title>下一页&nbsp;<i class="fa-solid fa-forward"></i></strong>
<br>Kubelet 垃圾回收原理剖析</a>
</nav>
</footer>
<div class=giscus_comments><script src=https://giscus.app/client.js data-repo=srcio/blog data-repo-id=R_kgDOIFWX7A data-category=General data-category-id=DIC_kwDOIFWX7M4CRrSg data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN crossorigin=anonymous async></script>
</div>
<script>document.querySelector("div.giscus_comments > script").setAttribute("data-theme",localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme"):window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.querySelector("#theme-toggle").addEventListener("click",()=>{let a=document.querySelector("iframe.giscus-frame");a&&a.contentWindow.postMessage({giscus:{setConfig:{theme:localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme")==="dark"?"light":"dark":document.body.className.includes("dark")?"light":"dark"}}},"https://giscus.app")})</script>
</div>
<div class=post-right-toc><div class=toc>
<div open>
<p class=toc-title><a href=#post-title><i class="fa-regular fa-bookmark"></i>&nbsp; 标题</a></p>
<div class=inner><ul>
<li>
<a href=#%e9%9b%86%e7%be%a4%e5%ae%b9%e9%87%8f aria-label=集群容量>集群容量</a></li>
<li>
<a href=#%e6%9b%b4%e5%b0%91%e7%9a%84%e9%ab%98%e9%85%8d%e8%8a%82%e7%82%b9 aria-label=更少的高配节点>更少的高配节点</a><ul>
<li>
<a href=#%e4%bc%98%e5%8a%bf aria-label=优势>优势</a><ul>
<li>
<a href=#%e6%9b%b4%e5%b0%91%e7%9a%84%e7%ae%a1%e7%90%86%e5%bc%80%e9%94%80 aria-label=更少的管理开销>更少的管理开销</a></li>
<li>
<a href=#%e6%9b%b4%e4%bd%8e%e7%9a%84%e5%8d%95%e8%8a%82%e7%82%b9%e6%88%90%e6%9c%ac aria-label=更低的单节点成本>更低的单节点成本</a></li>
<li>
<a href=#%e5%8f%af%e8%bf%90%e8%a1%8c%e9%a5%a5%e9%a5%bf%e5%9e%8b%e5%ba%94%e7%94%a8 aria-label=可运行饥饿型应用>可运行饥饿型应用</a></li></ul>
</li>
<li>
<a href=#%e5%8a%a3%e5%8a%bf aria-label=劣势>劣势</a><ul>
<li>
<a href=#%e5%8d%95%e8%8a%82%e7%82%b9%e8%bf%90%e8%a1%8c%e5%a4%a7%e9%87%8f-pod aria-label="单节点运行大量 Pod">单节点运行大量 Pod</a></li>
<li>
<a href=#%e6%9c%89%e9%99%90%e7%9a%84%e5%89%af%e6%9c%ac%e6%95%b0%e9%87%8f aria-label=有限的副本数量>有限的副本数量</a></li>
<li>
<a href=#%e6%9b%b4%e9%ab%98%e7%9a%84%e7%88%86%e7%82%b8%e5%8d%8a%e5%be%84 aria-label=更高的爆炸半径>更高的爆炸半径</a></li>
<li>
<a href=#%e6%9b%b4%e5%a4%a7%e7%9a%84%e8%b5%84%e6%ba%90%e4%bc%b8%e7%bc%a9%e5%a2%9e%e9%87%8f aria-label=更大的资源伸缩增量>更大的资源伸缩增量</a></li></ul>
</li></ul>
</li>
<li>
<a href=#%e6%9b%b4%e5%a4%9a%e7%9a%84%e4%bd%8e%e9%85%8d%e8%8a%82%e7%82%b9 aria-label=更多的低配节点>更多的低配节点</a><ul>
<li>
<a href=#%e4%bc%98%e5%8a%bf-1 aria-label=优势>优势</a><ul>
<li>
<a href=#%e5%87%8f%e5%b0%91%e7%88%86%e7%82%b8%e5%8d%8a%e5%be%84 aria-label=减少爆炸半径>减少爆炸半径</a></li>
<li>
<a href=#%e5%85%81%e8%ae%b8%e6%9b%b4%e5%a4%9a%e5%89%af%e6%9c%ac aria-label=允许更多副本>允许更多副本</a></li></ul>
</li>
<li>
<a href=#%e5%8a%a3%e5%8a%bf-1 aria-label=劣势>劣势</a><ul>
<li>
<a href=#%e8%be%83%e5%a4%a7%e7%9a%84%e8%8a%82%e7%82%b9%e6%95%b0%e9%87%8f aria-label=较大的节点数量>较大的节点数量</a></li>
<li>
<a href=#%e6%9b%b4%e5%a4%9a%e7%9a%84%e7%b3%bb%e7%bb%9f%e5%bc%80%e9%94%80 aria-label=更多的系统开销>更多的系统开销</a></li>
<li>
<a href=#%e6%9b%b4%e4%bd%8e%e7%9a%84%e8%b5%84%e6%ba%90%e5%88%a9%e7%94%a8%e7%8e%87 aria-label=更低的资源利用率>更低的资源利用率</a></li>
<li>
<a href=#pod-%e8%bf%90%e8%a1%8c%e6%95%b0%e9%87%8f%e9%99%90%e5%88%b6 aria-label="Pod 运行数量限制">Pod 运行数量限制</a></li></ul>
</li></ul>
</li>
<li>
<a href=#%e7%bb%93%e8%ae%ba aria-label=结论>结论</a>
</li>
</ul>
</div>
</div>
</div>
<script>let activeElement,elements;window.addEventListener('DOMContentLoaded',function(b){elements=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]'),activeElement=elements[0];const a=encodeURI(activeElement.getAttribute('id')).toLowerCase();document.querySelector(`.inner ul li a[href="#${a}"]`).classList.add('active')},!1),window.addEventListener('scroll',()=>{activeElement=Array.from(elements).find(a=>{if(getOffsetTop(a)-window.pageYOffset>0&&getOffsetTop(a)-window.pageYOffset<window.innerHeight/2)return a})||activeElement,elements.forEach(a=>{const b=encodeURI(a.getAttribute('id')).toLowerCase();a===activeElement?document.querySelector(`.inner ul li a[href="#${b}"]`)?.classList.add('active'):document.querySelector(`.inner ul li a[href="#${b}"]`)?.classList.remove('active')})},!1);function getOffsetTop(a){if(!a.getClientRects().length)return 0;let b=a.getBoundingClientRect(),c=a.ownerDocument.defaultView;return b.top+c.pageYOffset}</script>
</div>
<div class=clear_float></div>
</article>
</main>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<footer class=footer>
<p>
🔥
<span>2016 - 2022
<a href=https://srcio.cn/>博客 · 丁鹏</a>
</span>
&nbsp;|&nbsp;
<span>
<a href=https://beian.miit.gov.cn/>湘ICP备2022019990号</a>
</span>
</p>
<p>
📈
<span>本站共计
<a id=busuanzi_value_site_pv>0</a> 次访问 & <a id=busuanzi_value_site_uv>0</a> 位访客
</span>
</p>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='<i class="fa-regular fa-copy"></i>';function d(){a.innerHTML='<i class="fa-solid fa-check"></i>',setTimeout(()=>{a.innerHTML='<i class="fa-regular fa-copy"></i>'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body>
</html>
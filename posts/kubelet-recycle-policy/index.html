<!doctype html><html lang=zh dir=auto>
<head>
<link rel=stylesheet href=/fontawesome/css/all.min.css crossorigin=anonymous><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Kubelet 垃圾回收原理剖析 | 博客 · 丁鹏</title>
<meta name=keywords content="kubelet,docker image,镜像回收">
<meta name=description content="文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html
 Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。
镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。
主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。
需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。
用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：
 image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。
实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。
# vim /etc/systemd/system/kubelet.">
<meta name=author content="丁鹏">
<link rel=canonical href=https://srcio.cn/posts/kubelet-recycle-policy/>
<meta name=baidu-site-verification content="code-yW1u8Rg4sz">
<meta name=google-site-verification content="qnXhMaPNdhaZ4A36ptMVUn-0o111dAsJqugH_0ZCYvM">
<link crossorigin=anonymous href=/assets/css/stylesheet.da53450bd98e0263c39b02bc198d8b16f6c311038c0e7d51eb5975031919c346.css integrity="sha256-2lNFC9mOAmPDmwK8GY2LFvbDEQOMDn1R61l1AxkZw0Y=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://srcio.cn/images/blog.png>
<link rel=icon type=image/png sizes=16x16 href=https://srcio.cn/images/blog.png>
<link rel=icon type=image/png sizes=32x32 href=https://srcio.cn/images/blog.png>
<link rel=apple-touch-icon href=https://srcio.cn/images/blog.png>
<link rel=mask-icon href=https://srcio.cn/images/blog.png>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript><style>@media screen and (min-width:769px){.post-content input[type=checkbox]:checked~label>img{transform:scale(1.6);cursor:zoom-out;position:relative;z-index:999}.post-content img.zoomCheck{transition:transform .15s ease;z-index:999;cursor:zoom-in}}</style>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="Kubelet 垃圾回收原理剖析">
<meta property="og:description" content="文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html
 Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。
镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。
主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。
需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。
用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：
 image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。
实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。
# vim /etc/systemd/system/kubelet.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://srcio.cn/posts/kubelet-recycle-policy/"><meta property="og:image" content="https://srcio.cn/cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2022-10-17T19:21:58+08:00">
<meta property="article:modified_time" content="2022-10-17T19:21:58+08:00"><meta property="og:site_name" content="博客">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://srcio.cn/cover.png">
<meta name=twitter:title content="Kubelet 垃圾回收原理剖析">
<meta name=twitter:description content="文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html
 Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。
镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。
主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。
需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。
用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：
 image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。
实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。
# vim /etc/systemd/system/kubelet.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"文章","item":"https://srcio.cn/posts/"},{"@type":"ListItem","position":2,"name":"Kubelet 垃圾回收原理剖析","item":"https://srcio.cn/posts/kubelet-recycle-policy/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubelet 垃圾回收原理剖析","name":"Kubelet 垃圾回收原理剖析","description":"文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html\n Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。\n镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。\n主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。\n需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。\n用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：\n image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。\n实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。\n# vim /etc/systemd/system/kubelet.","keywords":["kubelet","docker image","镜像回收"],"articleBody":" 文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html\n Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。\n镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。\n主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。\n需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。\n用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：\n image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。\n实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。\n# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ... ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --image-gc-high-threshold=2 --image-gc-low-threshold=1 ... 我们在 Kubelet 启动参数的最后追加了 --image-gc-high-threshold=2 --image-gc-low-threshold=1，这么低的配置，Kubelet 应该会一直忙于进行镜像回收了，生产环境可不能这么配置！\n执行以下命令使得配置生效：\n# systemctl daemon-reload # systemctl restart kubelet 首先，看下本地都有哪些镜像：\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 接下来，我们运行一个 nginx 程序，让 Kubelet 自动拉取镜像。\nroot@shida-machine:~# kubectl run nginx --image=nginx deployment.apps/nginx created root@shida-machine:~# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 62s root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，nginx 镜像已经被自动 pull 到本地了，ID 为 f68d6e55e065。\n然后，删除 nginx Deployment：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \"nginx\" deleted 过大概 5 分钟后，再次检查本地镜像列表，发现 nginx 镜像已被清理！\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 通过以下命令查看镜像垃圾回收日志：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:03:20.883489 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72470076620 bytes down to the low threshold (1%). I0714 18:03:20.899370 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \"sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\" to free 109357355 bytes 可以看到，日志中记录的删除镜像 ID 与 nginx 镜像的 ID 是一致的（均为 f68d6e55e065）。\n继续验证用户手动拉取的镜像是否会被清理，手动运行 nginx 程序：\nroot@shida-machine:~# docker run --name nginx -d nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx fc7181108d40: Pull complete d2e987ca2267: Pull complete 0b760b431b11: Pull complete Digest: sha256:48cbeee0cb0a3b5e885e36222f969e0a2f41819a68e07aeb6631ca7cb356fed1 Status: Downloaded newer image for nginx:latest 2fc8a836ba3c7cbd488c7fd4f2ffa7287b709abf1b7701685291c3b1e5df3472 通过查看镜像 GC 日志，会发现 GC 会尝试清理用户自己手动拉取的 nginx 镜像，但因为该镜像被使用中，所以这次删除操作不会成功：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:28:23.015586 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72501525708 bytes down to the low threshold (1%). I0714 18:28:23.306696 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \"sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\" to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 将该容器停止，继续观察回收动作：\nroot@shida-machine:~# docker stop nginx nginx root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:53:23.579629 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72549280972 bytes down to the low threshold (1%). I0714 18:53:23.629492 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \"sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\" to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，对于已经停止的容器，Kubelet 也是会尝试删除，但删除操作依然不会成功（存在死亡容器对该镜像的引用）。\n彻底删除 nginx 容器，此时就没有任何容器继续使用该镜像，经过 1 次 GC 后，nginx 镜像就会被清理。\nroot@shida-machine:~# docker rm nginx nginx root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 容器回收 了解了镜像回收的基本原理，我们再来看看容器回收。容器在停止运行（比如出错退出或者正常结束）后会残留一系列的垃圾文件，一方面会占据磁盘空间，另一方面也会影响系统运行速度。此时，就需要 Kubelet 容器回收了。要特别注意的是，Kubelet 回收的容器是指那些由其管理的的容器（也就是 Pod 容器），用户手动运行的容器不会被 Kubelet 进行垃圾回收。\n与容器垃圾回收相关的控制参数主要有 3 个：\n  MinAge：容器可以被执行垃圾回收的最小年龄\n  MaxPerPodContainer：每个 pod 内允许存在的死亡容器的最大数量\n  MaxContainers：节点上全部死亡容器的最大数量\n   注意：当 MaxPerPodContainer 与 MaxContainers 发生冲突时，Kubelet 会自动调整 MaxPerPodContainer 的取值以满足 MaxContainers 要求。\n 主体流程   容器回收主要针对三个目标资源：普通容器、sandbox 容器以及容器日志目录。\n对于普通容器，主要根据 MaxPerPodContainer 与 MaxContainers 的设置，按照 LRU 策略，从 Pod 的死亡容器列表删除一定数量的容器，直到满足配置需求；对于 sandbox 容器，按照每个 Pod 保留一个的原则清理多余的死亡 sandbox；对于日志目录，只要没有 Pod 与之关联了就将其删除。\nKubelet 的容器垃圾回收只针对 Pod 容器，非 Kubelet Pod 容器（比如通过 docker run 启动的容器）不会被主动清理。\n用户配置 影响容器垃圾回收的关键参数有：\nminimum-container-ttl-duration：容器可被回收的最小生存年龄，默认是 0 分钟，这意味着每个死亡容器都会被立即执行垃圾回收\nmaximum-dead-containers-per-container`：每个 Pod 要保留的死亡容器的最大数量，默认值为 `1 maximum-dead-containers：节点可保留的死亡容器的最大数量，默认值是 -1，这意味着节点没有限制死亡容器数量\n实验环节 还是以 nginx 为例，创建一个 nginx 服务：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 7bef0308d9ea nginx \"nginx -g 'daemon of…\" 16 seconds ago Up 14 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \"/pause\" 2 minutes ago Up 2 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到，Kubelet 启动了一个 sandbox 以及一个 nginx 实例。\n手动杀死 nginx 实例，模拟容器异常退出：\nroot@shida-machine:~# docker kill 7bef0308d9ea 7bef0308d9ea root@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \"nginx -g 'daemon of…\" 3 seconds ago Up 2 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \"nginx -g 'daemon of…\" 2 minutes ago Exited (137) 15 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \"/pause\" 5 minutes ago Up 5 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到 Kubelet 重新拉起了一个新的 nginx 实例。\n等待几分钟，发现 Kubelet 并未清理异常退出的 nginx 容器（因为此时仅有一个 dead container）。\nroot@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \"nginx -g 'daemon of…\" 3 minutes ago Up 3 minutes k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \"nginx -g 'daemon of…\" 5 minutes ago Exited (137) 3 minutes ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \"/pause\" 8 minutes ago Up 8 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 继续杀死当前 nginx 实例：\nroot@shida-machine:~# docker kill 408b23b2b72a 408b23b2b72a root@shida-machine:~# docker ps -a | grep nginx e064e376819f nginx \"nginx -g 'daemon of…\" 9 seconds ago Up 7 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_2 408b23b2b72a nginx \"nginx -g 'daemon of…\" 5 minutes ago Exited (137) 40 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7e65e0db52c2 k8s.gcr.io/pause:3.1 \"/pause\" 10 minutes ago Up 10 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 这下看到效果了，仍然只有一个退出的容器被保留，而且被清理掉的是最老的死亡容器，这与之前的分析是一致的！\n删除这个 nginx Deployment，会发现所有的 nginx 容器都会被清理：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \"nginx\" deleted root@shida-machine:~# docker ps -a | grep nginx root@shida-machine:~# 进一步，我们修改 Kubelet 参数，设置 maximum-dead-containers 为 0，这就告诉 Kubelet 清理所有死亡容器。\n重复前边的实验步骤：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 8de9ae8e2c9b nginx \"nginx -g 'daemon of…\" 33 seconds ago Up 32 seconds k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \"/pause\" 41 seconds ago Up 38 seconds k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 root@shida-machine:~# docker kill 8de9ae8e2c9b 8de9ae8e2c9b root@shida-machine:~# docker ps -a | grep nginx 95ee5bd2cab2 nginx \"nginx -g 'daemon of…\" About a minute ago Up About a minute k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_1 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \"/pause\" 2 minutes ago Up About a minute k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 结果显示，nginx Pod 的所有死亡容器都会被清理，因为我们已经强制要求节点不保留任何死亡容器，与预期一致！\n那对于手动运行的容器呢？我们通过 docker run 运行 nginx：\nroot@shida-machine:~# docker run --name nginx -d nginx 46ebb365f6be060a6950f44728e4f11e4666bf2fb007cad557ffc65ecf8aded8 root@shida-machine:~# docker ps | grep nginx 46ebb365f6be nginx \"nginx -g 'daemon of…\" 9 seconds ago Up 6 seconds 80/tcp nginx 杀死该容器：\nroot@shida-machine:~# docker kill 46ebb365f6be 46ebb365f6be root@shida-machine:~# docker ps -a | grep nginx 46ebb365f6be nginx \"nginx -g 'daemon of…\" About a minute ago Exited (137) 18 seconds ago nginx 经过几分钟，我们发现该死亡容器还是会存在的，Kubelet 不会清理这类容器！\n小结 Kubelet 每 5 分钟进行一次镜像清理。当磁盘使用率超过上限阈值，Kubelet 会按照 LRU 策略逐一清理没有被任何容器所使用的镜像，直到磁盘使用率降到下限阈值或没有空闲镜像可以清理。Kubelet 认为镜像可被清理的标准是未被任何 Pod 容器（包括那些死亡了的容器）所引用，那些非 Pod 容器（如用户通过 docker run 启动的容器）是不会被用来计算镜像引用关系的。也就是说，即便用户运行的容器使用了 A 镜像，只要没有任何 Pod 容器使用到 A，那 A 镜像对于 Kubelet 而言就是可被回收的。但是我们无需担心手动运行容器使用的镜像会被意外回收，因为 Kubelet 的镜像删除是非 force 类型的，底层容器运行时会使存在容器关联的镜像删除操作失败（因为 Docker 会认为仍有容器使用着 A 镜像）。\nKubelet 每 1 分钟执行一次容器清理。根据启动配置参数，Kubelet 会按照 LRU 策略依次清理每个 Pod 内的死亡容器，直到达到死亡容器限制数要求，对于 sandbox 容器，Kubelet 仅会保留最新的（这不受 GC 策略的控制）。对于日志目录，只要已经没有 Pod 继续占用，就将其清理。对于非 Pod 容器（如用户通过 docker run 启动的容器）不会被 Kubelet 垃圾回收。\n","wordCount":"1377","inLanguage":"zh","datePublished":"2022-10-17T19:21:58+08:00","dateModified":"2022-10-17T19:21:58+08:00","author":[{"@type":"Person","name":"丁鹏"}],"mainEntityOfPage":{"@type":"WebPage","@id":"https://srcio.cn/posts/kubelet-recycle-policy/"},"publisher":{"@type":"Organization","name":"博客 · 丁鹏","logo":{"@type":"ImageObject","url":"https://srcio.cn/images/blog.png"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://srcio.cn/ accesskey=h title="博客 (Alt + H)">
<img src=https://srcio.cn/images/blog.png alt aria-label=logo height=25>博客</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://srcio.cn/ title=主页>
<span>
<i class="fa-solid fa-home"></i>&nbsp;主页</span>
</a>
</li>
<li>
<a href=https://srcio.cn/posts/ title=文章>
<span>
<i class="fa-solid fa-feather"></i>&nbsp;文章</span>
</a>
</li>
<li>
<a href=https://srcio.cn/series/ title=系列>
<span>
<i class="fa-solid fa-swatchbook"></i>&nbsp;系列</span>
</a>
</li>
<li>
<a href=https://srcio.cn/tags/ title=标签>
<span>
<i class="fa-solid fa-tags"></i>&nbsp;标签</span>
</a>
</li>
<li>
<a href=https://srcio.cn/archives/ title=归档>
<span>
<i class="fa-solid fa-folder-minus"></i>&nbsp;归档</span>
</a>
</li>
<li>
<a href=https://srcio.cn/search/ title>
<span>
<i class="fa-solid fa-magnifying-glass"></i>&nbsp;</span>
</a>
</li>
</ul>
</nav>
</header><main class=main>
<article class=post-single>
<div class=post-left-article>
<header class=post-header>
<div class=breadcrumbs><p><a href=/>主页</a>
<a href=https://srcio.cn/posts/><i class="fa-solid fa-angle-right"></i>&nbsp;文章</a>&nbsp;<span><i class="fa-solid fa-angle-right"></i>&nbsp;Kubelet 垃圾回收原理剖析</span></p>
</div>
<h1 id=post-title class=post-title>
Kubelet 垃圾回收原理剖析
</h1>
<div class=post-meta><span class=meta-item>
<span>
<i class="fa-solid fa-calendar-day"></i>
2022-10-17
</span>
</span><span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-clock"></i>&nbsp;7 分钟</span>
</span><span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-chart-simple"></i></i>&nbsp;1377 字</span>
</span>
<span class=meta-item>
<span>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-user"></i></i>&nbsp;丁鹏
</span>
</span><span class=meta-item>
&nbsp;·&nbsp;&nbsp;<i class="fa-solid fa-book-open-reader"></i>&nbsp;<span id=busuanzi_value_page_pv></span>
</span>&nbsp;&nbsp;|&nbsp;&nbsp;<a href=https://github.com/srcio/blog/edit/master/content/posts/kubelet-recycle-policy.md rel="noopener noreferrer" target=_blank>
<i class="fa-regular fa-pen-to-square"></i>&nbsp;编辑</a>
</div>
</header>
<div class=post-content><blockquote>
<p>文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html</p>
</blockquote>
<p>Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。</p>
<h2 id=镜像回收>镜像回收<a hidden class=anchor aria-hidden=true href=#镜像回收>#</a></h2>
<p>Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（<code>HighThresholdPercent</code>）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（<code>LowThresholdPercent</code>）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（<code>MinAge</code>）要求的镜像，暂不予以清理。</p>
<h3 id=主体流程>主体流程<a hidden class=anchor aria-hidden=true href=#主体流程>#</a></h3>
<p>
<input type=checkbox id=zoomCheck-7bd32 hidden>
<label for=zoomCheck-7bd32>
<img class=zoomCheck loading=lazy decoding=async src=https://srcio.oss-cn-hangzhou.aliyuncs.com/images/image-gc-workflow.svg alt=img>
</label>
</p>
<p>如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。</p>
<p>需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 <code>run</code> 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。</p>
<h3 id=用户配置>用户配置<a hidden class=anchor aria-hidden=true href=#用户配置>#</a></h3>
<p>通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：</p>
<ol>
<li><code>image-gc-high-threshold</code>：磁盘使用率上限，有效范围 [0-100]，默认 <code>85</code></li>
<li><code>image-gc-low-threshold</code>：磁盘使用率下限，有效范围 [0-100]，默认 <code>80</code></li>
<li><code>minimum-image-ttl-duration</code>：镜像最短应该生存的年龄，默认 <code>2</code> 分钟</li>
</ol>
<h3 id=实验环节>实验环节<a hidden class=anchor aria-hidden=true href=#实验环节>#</a></h3>
<p>本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。</p>
<p>实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。</p>
<pre tabindex=0><code># vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
...
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --image-gc-high-threshold=2 --image-gc-low-threshold=1
...
</code></pre><p>我们在 Kubelet 启动参数的最后追加了 <code>--image-gc-high-threshold=2 --image-gc-low-threshold=1</code>，这么低的配置，Kubelet 应该会一直忙于进行镜像回收了，生产环境可不能这么配置！</p>
<p>执行以下命令使得配置生效：</p>
<pre tabindex=0><code># systemctl daemon-reload
# systemctl restart kubelet
</code></pre><p>首先，看下本地都有哪些镜像：</p>
<pre tabindex=0><code>root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><p>接下来，我们运行一个 <code>nginx</code> 程序，让 Kubelet 自动拉取镜像。</p>
<pre tabindex=0><code>root@shida-machine:~# kubectl run nginx --image=nginx
deployment.apps/nginx created
root@shida-machine:~# kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   1/1     1            1           62s
root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
nginx                                latest              f68d6e55e065        12 days ago         109MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><p>可以看到，<code>nginx</code> 镜像已经被自动 <code>pull</code> 到本地了，ID 为 <code>f68d6e55e065</code>。</p>
<p>然后，删除 <code>nginx</code> Deployment：</p>
<pre tabindex=0><code>root@shida-machine:~# kubectl delete deployment nginx
deployment.extensions &quot;nginx&quot; deleted
</code></pre><p>过大概 5 分钟后，再次检查本地镜像列表，发现 <code>nginx</code> 镜像已被清理！</p>
<pre tabindex=0><code>root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><p>通过以下命令查看镜像垃圾回收日志：</p>
<pre tabindex=0><code>root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager
...
I0714 18:03:20.883489   51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72470076620 bytes down to the low threshold (1%).
I0714 18:03:20.899370   51179 image_gc_manager.go:371] [imageGCManager]: Removing image &quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc&quot; to free 109357355 bytes
</code></pre><p>可以看到，日志中记录的删除镜像 ID 与 <code>nginx</code> 镜像的 ID 是一致的（均为 <code>f68d6e55e065</code>）。</p>
<p>继续验证用户手动拉取的镜像是否会被清理，手动运行 <code>nginx</code> 程序：</p>
<pre tabindex=0><code>root@shida-machine:~# docker run --name nginx -d nginx 
Unable to find image 'nginx:latest' locally
latest: Pulling from library/nginx
fc7181108d40: Pull complete 
d2e987ca2267: Pull complete 
0b760b431b11: Pull complete 
Digest: sha256:48cbeee0cb0a3b5e885e36222f969e0a2f41819a68e07aeb6631ca7cb356fed1
Status: Downloaded newer image for nginx:latest
2fc8a836ba3c7cbd488c7fd4f2ffa7287b709abf1b7701685291c3b1e5df3472
</code></pre><p>通过查看镜像 GC 日志，会发现 GC 会尝试清理用户自己手动拉取的 <code>nginx</code> 镜像，但因为该镜像被使用中，所以这次删除操作不会成功：</p>
<pre tabindex=0><code>root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager
...
I0714 18:28:23.015586   51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72501525708 bytes down to the low threshold (1%).
I0714 18:28:23.306696   51179 image_gc_manager.go:371] [imageGCManager]: Removing image &quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc&quot; to free 109357355 bytes
root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
nginx                                latest              f68d6e55e065        12 days ago         109MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><p>将该容器停止，继续观察回收动作：</p>
<pre tabindex=0><code>root@shida-machine:~# docker stop nginx
nginx
root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager
...
I0714 18:53:23.579629   51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72549280972 bytes down to the low threshold (1%).
I0714 18:53:23.629492   51179 image_gc_manager.go:371] [imageGCManager]: Removing image &quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc&quot; to free 109357355 bytes
root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
nginx                                latest              f68d6e55e065        12 days ago         109MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><p>可以看到，对于已经停止的容器，Kubelet 也是会尝试删除，但删除操作依然不会成功（存在死亡容器对该镜像的引用）。</p>
<p>彻底删除 <code>nginx</code> 容器，此时就没有任何容器继续使用该镜像，经过 1 次 GC 后，<code>nginx</code> 镜像就会被清理。</p>
<pre tabindex=0><code>root@shida-machine:~# docker rm nginx
nginx
root@shida-machine:~# docker images
REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE
k8s.gcr.io/kube-proxy                v1.14.4             5f2081c22306        6 days ago          82.1MB
k8s.gcr.io/kube-apiserver            v1.14.4             f3171d49fa9b        6 days ago          210MB
k8s.gcr.io/kube-controller-manager   v1.14.4             35f0904dc8fa        6 days ago          158MB
k8s.gcr.io/kube-scheduler            v1.14.4             ee080c083e45        6 days ago          81.6MB
calico/node                          v3.7.3              bf4ff15c9db0        4 weeks ago         156MB
calico/cni                           v3.7.3              1a6ade52d471        4 weeks ago         135MB
calico/kube-controllers              v3.7.3              283860d96794        4 weeks ago         46.8MB
k8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB
k8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB
k8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB
</code></pre><h2 id=容器回收>容器回收<a hidden class=anchor aria-hidden=true href=#容器回收>#</a></h2>
<p>了解了镜像回收的基本原理，我们再来看看容器回收。容器在停止运行（比如出错退出或者正常结束）后会残留一系列的垃圾文件，一方面会占据磁盘空间，另一方面也会影响系统运行速度。此时，就需要 Kubelet 容器回收了。要特别注意的是，Kubelet 回收的容器是指那些由其管理的的容器（也就是 Pod 容器），用户手动运行的容器不会被 Kubelet 进行垃圾回收。</p>
<p>与容器垃圾回收相关的控制参数主要有 3 个：</p>
<ol>
<li>
<p><code>MinAge</code>：容器可以被执行垃圾回收的最小年龄</p>
</li>
<li>
<p><code>MaxPerPodContainer</code>：每个 pod 内允许存在的死亡容器的最大数量</p>
</li>
<li>
<p><code>MaxContainers</code>：节点上全部死亡容器的最大数量</p>
</li>
</ol>
<blockquote>
<p>注意：当 <code>MaxPerPodContainer</code> 与 <code>MaxContainers</code> 发生冲突时，Kubelet 会自动调整 <code>MaxPerPodContainer</code> 的取值以满足 <code>MaxContainers</code> 要求。</p>
</blockquote>
<h3 id=主体流程-1>主体流程<a hidden class=anchor aria-hidden=true href=#主体流程-1>#</a></h3>
<p>
<input type=checkbox id=zoomCheck-8995d hidden>
<label for=zoomCheck-8995d>
<img class=zoomCheck loading=lazy decoding=async src=https://srcio.oss-cn-hangzhou.aliyuncs.com/images/container-gc-workflow.svg alt=img>
</label>
</p>
<p>容器回收主要针对三个目标资源：普通容器、sandbox 容器以及容器日志目录。</p>
<p>对于普通容器，主要根据 <code>MaxPerPodContainer</code> 与 <code>MaxContainers</code> 的设置，按照 LRU 策略，从 Pod 的死亡容器列表删除一定数量的容器，直到满足配置需求；对于 <code>sandbox</code> 容器，按照每个 Pod 保留一个的原则清理多余的死亡 <code>sandbox</code>；对于日志目录，只要没有 Pod 与之关联了就将其删除。</p>
<p>Kubelet 的容器垃圾回收只针对 Pod 容器，非 Kubelet Pod 容器（比如通过 <code>docker run</code> 启动的容器）不会被主动清理。</p>
<h3 id=用户配置-1>用户配置<a hidden class=anchor aria-hidden=true href=#用户配置-1>#</a></h3>
<p>影响容器垃圾回收的关键参数有：</p>
<p><code>minimum-container-ttl-duration</code>：容器可被回收的最小生存年龄，默认是 <code>0</code> 分钟，这意味着每个死亡容器都会被立即执行垃圾回收</p>
<pre tabindex=0><code>maximum-dead-containers-per-container`：每个 Pod 要保留的死亡容器的最大数量，默认值为 `1
</code></pre><p><code>maximum-dead-containers</code>：节点可保留的死亡容器的最大数量，默认值是 <code>-1</code>，这意味着节点没有限制死亡容器数量</p>
<h3 id=实验环节-1>实验环节<a hidden class=anchor aria-hidden=true href=#实验环节-1>#</a></h3>
<p>还是以 <code>nginx</code> 为例，创建一个 <code>nginx</code> 服务：</p>
<pre tabindex=0><code>root@shida-machine:~# kubectl run nginx --image nginx
deployment.apps/nginx created
root@shida-machine:~# docker ps -a | grep nginx
7bef0308d9ea        nginx                     &quot;nginx -g 'daemon of…&quot;   16 seconds ago      Up 14 seconds                                 k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
7e65e0db52c2        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 2 minutes ago       Up 2 minutes                                  k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
</code></pre><p>可以看到，Kubelet 启动了一个 <code>sandbox</code> 以及一个 <code>nginx</code> 实例。</p>
<p>手动杀死 <code>nginx</code> 实例，模拟容器异常退出：</p>
<pre tabindex=0><code>root@shida-machine:~# docker kill 7bef0308d9ea
7bef0308d9ea
root@shida-machine:~# docker ps -a | grep nginx
408b23b2b72a        nginx                     &quot;nginx -g 'daemon of…&quot;   3 seconds ago       Up 2 seconds                                      k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1
7bef0308d9ea        nginx                     &quot;nginx -g 'daemon of…&quot;   2 minutes ago       Exited (137) 15 seconds ago                       k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
7e65e0db52c2        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 5 minutes ago       Up 5 minutes                                      k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
</code></pre><p>可以看到 Kubelet 重新拉起了一个新的 <code>nginx</code> 实例。</p>
<p>等待几分钟，发现 Kubelet 并未清理异常退出的 <code>nginx</code> 容器（因为此时仅有一个 dead container）。</p>
<pre tabindex=0><code>root@shida-machine:~# docker ps -a | grep nginx
408b23b2b72a        nginx                     &quot;nginx -g 'daemon of…&quot;   3 minutes ago       Up 3 minutes                                     k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1
7bef0308d9ea        nginx                     &quot;nginx -g 'daemon of…&quot;   5 minutes ago       Exited (137) 3 minutes ago                       k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
7e65e0db52c2        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 8 minutes ago       Up 8 minutes                                     k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
</code></pre><p>继续杀死当前 <code>nginx</code> 实例：</p>
<pre tabindex=0><code>root@shida-machine:~# docker kill 408b23b2b72a
408b23b2b72a
root@shida-machine:~# docker ps -a | grep nginx
e064e376819f        nginx                     &quot;nginx -g 'daemon of…&quot;   9 seconds ago       Up 7 seconds                                      k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_2
408b23b2b72a        nginx                     &quot;nginx -g 'daemon of…&quot;   5 minutes ago       Exited (137) 40 seconds ago                       k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1
7e65e0db52c2        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 10 minutes ago      Up 10 minutes                                     k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0
</code></pre><p>这下看到效果了，仍然只有一个退出的容器被保留，而且被清理掉的是最老的死亡容器，这与之前的分析是一致的！</p>
<p>删除这个 <code>nginx</code> Deployment，会发现所有的 <code>nginx</code> 容器都会被清理：</p>
<pre tabindex=0><code>root@shida-machine:~# kubectl delete deployment nginx
deployment.extensions &quot;nginx&quot; deleted
root@shida-machine:~# docker ps -a | grep nginx
root@shida-machine:~#
</code></pre><p>进一步，我们修改 Kubelet 参数，设置 <code>maximum-dead-containers</code> 为 <code>0</code>，这就告诉 Kubelet 清理所有死亡容器。</p>
<p>重复前边的实验步骤：</p>
<pre tabindex=0><code>root@shida-machine:~# kubectl run nginx --image nginx
deployment.apps/nginx created
root@shida-machine:~# docker ps -a | grep nginx
8de9ae8e2c9b        nginx                     &quot;nginx -g 'daemon of…&quot;   33 seconds ago      Up 32 seconds                                   k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0
d2cdfafdbe50        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 41 seconds ago      Up 38 seconds                                   k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0
root@shida-machine:~# docker kill 8de9ae8e2c9b
8de9ae8e2c9b
root@shida-machine:~# docker ps -a | grep nginx
95ee5bd2cab2        nginx                     &quot;nginx -g 'daemon of…&quot;   About a minute ago   Up About a minute                             k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_1
d2cdfafdbe50        k8s.gcr.io/pause:3.1      &quot;/pause&quot;                 2 minutes ago        Up About a minute                             k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0
</code></pre><p>结果显示，<code>nginx</code> Pod 的所有死亡容器都会被清理，因为我们已经强制要求节点不保留任何死亡容器，与预期一致！</p>
<p>那对于手动运行的容器呢？我们通过 <code>docker run</code> 运行 <code>nginx</code>：</p>
<pre tabindex=0><code>root@shida-machine:~# docker run --name nginx -d nginx
46ebb365f6be060a6950f44728e4f11e4666bf2fb007cad557ffc65ecf8aded8
root@shida-machine:~# docker ps | grep nginx
46ebb365f6be        nginx                     &quot;nginx -g 'daemon of…&quot;   9 seconds ago       Up 6 seconds        80/tcp              nginx
</code></pre><p>杀死该容器：</p>
<pre tabindex=0><code>root@shida-machine:~# docker kill 46ebb365f6be
46ebb365f6be
root@shida-machine:~# docker ps -a | grep nginx
46ebb365f6be        nginx                     &quot;nginx -g 'daemon of…&quot;   About a minute ago   Exited (137) 18 seconds ago                       nginx
</code></pre><p>经过几分钟，我们发现该死亡容器还是会存在的，Kubelet 不会清理这类容器！</p>
<h2 id=小结>小结<a hidden class=anchor aria-hidden=true href=#小结>#</a></h2>
<p>Kubelet 每 5 分钟进行一次镜像清理。当磁盘使用率超过上限阈值，Kubelet 会按照 LRU 策略逐一清理没有被任何容器所使用的镜像，直到磁盘使用率降到下限阈值或没有空闲镜像可以清理。Kubelet 认为镜像可被清理的标准是未被任何 Pod 容器（包括那些死亡了的容器）所引用，那些非 Pod 容器（如用户通过 <code>docker run</code> 启动的容器）是不会被用来计算镜像引用关系的。也就是说，即便用户运行的容器使用了 A 镜像，只要没有任何 Pod 容器使用到 A，那 A 镜像对于 Kubelet 而言就是可被回收的。但是我们无需担心手动运行容器使用的镜像会被意外回收，因为 Kubelet 的镜像删除是非 force 类型的，底层容器运行时会使存在容器关联的镜像删除操作失败（因为 Docker 会认为仍有容器使用着 A 镜像）。</p>
<p>Kubelet 每 1 分钟执行一次容器清理。根据启动配置参数，Kubelet 会按照 LRU 策略依次清理每个 Pod 内的死亡容器，直到达到死亡容器限制数要求，对于 <code>sandbox</code> 容器，Kubelet 仅会保留最新的（这不受 GC 策略的控制）。对于日志目录，只要已经没有 Pod 继续占用，就将其清理。对于非 Pod 容器（如用户通过 <code>docker run</code> 启动的容器）不会被 Kubelet 垃圾回收。</p>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://srcio.cn/tags/kubernetes/><i class="fa-solid fa-tag"></i>&nbsp;Kubernetes</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://srcio.cn/posts/k8s-node-resource-config/>
<strong class=title><i class="fa-solid fa-backward"></i>&nbsp;上一页</strong>
<br>K8s 集群规划之节点资源配置</a>
<a class=next href=https://srcio.cn/series/programming-kubernetes/crd/>
<strong class=title>下一页&nbsp;<i class="fa-solid fa-forward"></i></strong>
<br>CRD 简介</a>
</nav>
</footer>
<div class=giscus_comments><script src=https://giscus.app/client.js data-repo=srcio/blog data-repo-id=R_kgDOIFWX7A data-category=General data-category-id=DIC_kwDOIFWX7M4CRrSg data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=top data-theme=light data-lang=zh-CN crossorigin=anonymous async></script>
</div>
<script>document.querySelector("div.giscus_comments > script").setAttribute("data-theme",localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme"):window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"),document.querySelector("#theme-toggle").addEventListener("click",()=>{let a=document.querySelector("iframe.giscus-frame");a&&a.contentWindow.postMessage({giscus:{setConfig:{theme:localStorage.getItem("pref-theme")?localStorage.getItem("pref-theme")==="dark"?"light":"dark":document.body.className.includes("dark")?"light":"dark"}}},"https://giscus.app")})</script>
</div>
<div class=post-right-toc><div class=toc>
<div open>
<p class=toc-title><i class="fa-solid fa-bookmark"></i><a href=#post-title>&nbsp; 目录</a></p>
<div class=inner><ul>
<li>
<a href=#%e9%95%9c%e5%83%8f%e5%9b%9e%e6%94%b6 aria-label=镜像回收>镜像回收</a><ul>
<li>
<a href=#%e4%b8%bb%e4%bd%93%e6%b5%81%e7%a8%8b aria-label=主体流程>主体流程</a></li>
<li>
<a href=#%e7%94%a8%e6%88%b7%e9%85%8d%e7%bd%ae aria-label=用户配置>用户配置</a></li>
<li>
<a href=#%e5%ae%9e%e9%aa%8c%e7%8e%af%e8%8a%82 aria-label=实验环节>实验环节</a></li></ul>
</li>
<li>
<a href=#%e5%ae%b9%e5%99%a8%e5%9b%9e%e6%94%b6 aria-label=容器回收>容器回收</a><ul>
<li>
<a href=#%e4%b8%bb%e4%bd%93%e6%b5%81%e7%a8%8b-1 aria-label=主体流程>主体流程</a></li>
<li>
<a href=#%e7%94%a8%e6%88%b7%e9%85%8d%e7%bd%ae-1 aria-label=用户配置>用户配置</a></li>
<li>
<a href=#%e5%ae%9e%e9%aa%8c%e7%8e%af%e8%8a%82-1 aria-label=实验环节>实验环节</a></li></ul>
</li>
<li>
<a href=#%e5%b0%8f%e7%bb%93 aria-label=小结>小结</a>
</li>
</ul>
</div>
</div>
</div>
<script>let activeElement,elements;window.addEventListener('DOMContentLoaded',function(b){elements=document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]'),activeElement=elements[0];const a=encodeURI(activeElement.getAttribute('id')).toLowerCase();document.querySelector(`.inner ul li a[href="#${a}"]`).classList.add('active')},!1),window.addEventListener('scroll',()=>{activeElement=Array.from(elements).find(a=>{if(getOffsetTop(a)-window.pageYOffset>0&&getOffsetTop(a)-window.pageYOffset<window.innerHeight/2)return a})||activeElement,elements.forEach(a=>{const b=encodeURI(a.getAttribute('id')).toLowerCase();a===activeElement?document.querySelector(`.inner ul li a[href="#${b}"]`)?.classList.add('active'):document.querySelector(`.inner ul li a[href="#${b}"]`)?.classList.remove('active')})},!1);function getOffsetTop(a){if(!a.getClientRects().length)return 0;let b=a.getBoundingClientRect(),c=a.ownerDocument.defaultView;return b.top+c.pageYOffset}</script>
</div>
<div class=clear_float></div>
</article>
</main>
<script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script>
<footer class=footer>
<p>
🔥
<span>2016 - 2022
<a href=https://srcio.cn/>博客 · 丁鹏</a>
</span>
&nbsp;|&nbsp;
<span>
<a href=https://beian.miit.gov.cn/>湘ICP备2022019990号</a>
</span>
</p>
<p>
📈
<span>本站共计
<a id=busuanzi_value_site_pv>0</a> 次访问 & <a id=busuanzi_value_site_uv>0</a> 位访客
</span>
</p>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='<i class="fa-regular fa-copy"></i>';function d(){a.innerHTML='<i class="fa-solid fa-check"></i>',setTimeout(()=>{a.innerHTML='<i class="fa-regular fa-copy"></i>'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script></body>
</html>
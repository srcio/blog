[{"content":"可重用软件设计的五个原则，SOLID 原则：\n 单一职责原则（Single Responsibility Principle） 开放 / 封闭原则（Open / Closed Principle） 里氏替换原则（Liskov Substitution Principle） 接口隔离原则（Interface Segregation Principle） 依赖倒置原则（Dependency Inversion Principle）  单一职责原则 SOLID 的第一个原则，S，是单一责任原则。\n A class should have one, and only one, reason to change. – Robert C Martin\n 现在 Go 显然没有 classses - 相反，我们有更强大的组合概念 - 但是如果你能回顾一下 class 这个词的用法，我认为此时会有一定价值。\n为什么一段代码只有一个改变的原因很重要？嗯，就像你自己的代码可能会改变一样令人沮丧，发现您的代码所依赖的代码在您脚下发生变化更痛苦。当你的代码必须改变时，它应该响应直接刺激作出改变，而不应该成为附带损害的受害者。\n因此，具有单一责任的代码修改的原因最少。\n耦合和内聚 描述改变一个软件是多么容易或困难的两个词是：耦合和内聚。\n 耦合只是一个词，描述了两个一起变化的东西 —— 一个运动诱导另一个运动。 一个相关但独立的概念是内聚，一种相互吸引的力量。  在软件上下文中，内聚是描述代码片段之间自然相互吸引的特性。\n为了描述 Go 程序中耦合和内聚的单元，我们可能会将谈谈函数和方法，这在讨论 SRP 时很常见，但是我相信它始于 Go 的 package 模型。\n库名称的设计 在 Go 中，所有的代码都在某个 package 中，一个设计良好的 package 从其名称开始。包的名称既是其用途的描述，也是名称空间前缀。Go 标准库中的一些优秀 package 示例：\n net/http - 提供 http 客户端和服务端 os/exec - 执行外部命令 encoding/json - 实现 JSON 文档的编码和解码 当你在自己的内部使用另一个 pakcage 的 symbols 时，要使用 import 声明，它在两个 package 之间建立一个源代码级的耦合。 他们现在彼此知道对方的存在。  糟糕的库名称 这种对名字的关注可不是迂腐。命名不佳的 package 如果真的有用途，会失去罗列其用途的机会。\n server package 提供什么？ …, 嗯，希望是服务端，但是它使用哪种协议？ private package 提供什么？我不应该看到的东西？它应该有公共符号吗？ common package，和它的伴儿 utils package 一样，经常被发现和其他’伙伴’一起发现 我们看到所有像这样的包裹，就成了各种各样的垃圾场，因为它们有许多责任，所以经常毫无理由地改变。  Unix 设计理念 在我看来，如果不提及 Doug McIlroy 的 Unix 哲学，任何关于解耦设计的讨论都将是不完整的；小而锋利的工具结合起来，解决更大的任务，通常是原始作者无法想象的任务。\n我认为 Go package 体现了 Unix 哲学的精神。实际上，每个 Go package 本身就是一个小的 Go 程序，一个单一的变更单元，具有单一的责任。\n开放 / 封闭原则 第二个原则，即 O，是 Bertrand Meyer 的开放 / 封闭原则，他在 1988 年写道：\n Software entities should be open for extension, but closed for modification. – Bertrand Meyer, Object-Oriented Software Construction\n 该建议如何适用于 21 年后写的语言？\npackage main type A struct { year int } func (a A) Greet() { fmt.Println(\u0026#34;Hello GolangUK\u0026#34;, a.year) } type B struct { A } func (b B) Greet() { fmt.Println(\u0026#34;Welcome to GolangUK\u0026#34;, b.year) } func main() { var a A a.year = 2016 var b B b.year = 2016 a.Greet() // Hello GolangUK 2016  b.Greet() // Welcome to GolangUK 2016 } 我们有一个类型 A ，有一个字段 year 和一个方法 Greet。我们有第二种类型，B 它嵌入了一个 A，因为 A 嵌入，因此调用者看到 B 的方法覆盖了 A 的方法。因为 A 作为字段嵌入 B ，B 可以提供自己的 Greet 方法，掩盖了 A 的 Greet 方法。\n但嵌入不仅适用于方法，还可以访问嵌入类型的字段。如您所见，因为 A 和 B 都在同一个包中定义，所以 B 可以访问 A 的私有 year 字段，就像在 B 中声明一样。\n因此嵌入是一个强大的工具，允许 Go 的类型对扩展开放。\npackage main type Cat struct { Name string } func (c Cat) Legs() int { return 4 } func (c Cat) PrintLegs() { fmt.Printf(\u0026#34;I have %d legs.\u0026#34;, c.Legs()) } type OctoCat struct { Cat } func (o OctoCat) Legs() int { return 5 } func main() { var octo OctoCat fmt.Println(octo.Legs()) // 5  octo.PrintLegs() // I have 4 legs } 在这个例子中，我们有一个 Cat 类型，可以用它的 Legs 方法计算它的腿数。我们将 Cat 类型嵌入到一个新类型 OctoCat 中，并声明 Octocats 有五条腿。但是，虽然 OctoCat 定义了自己的 Legs 方法，该方法返回 5，但是当调用 PrintLegs 方法时，它返回 4。\n这是因为 PrintLegs 是在 Cat 类型上定义的。 它需要 Cat 作为它的接收器，因此它会发送到 Cat 的 Legs 方法。Cat 不知道它嵌入的类型，因此嵌入时不能改变其方法集。\n因此，我们可以说 Go 的类型虽然对扩展开放，但对修改是封闭的。\n事实上，Go 中的方法只不过是围绕在具有预先声明形式参数（即接收器）的函数的语法糖。\nfunc (c Cat) PrintLegs() { fmt.Printf(\u0026#34;I have %d legs.\u0026#34;, c.Legs()) } func PrintLegs(c Cat) { fmt.Printf(\u0026#34;I have %d legs.\u0026#34;, c.Legs()) } 接收器正是你传入它的函数，函数的第一个参数，并且因为 Go 不支持函数重载，OctoCat 不能替代普通的 Cat 。 这让我想到了下一个原则。\n里氏替换原则 由 Barbara Liskov 提出的里氏替换原则粗略地指出，如果两种类型表现出的行为使得调用者无法区分，则这两种类型是可替代的。\n在基于类的语言中，里氏替换原则通常被解释为，具有各种具体子类型的抽象基类的规范。 但是 Go 没有类或继承，因此无法根据抽象类层次结构实现替换。\nInterfaces 相反，替换是 Go 接口的范围。在 Go 中，类型不需要指定它们实现特定接口，而是任何类型实现接口，只要它具有签名与接口声明匹配的方法。\n我们说在 Go 中，接口是隐式地而不是显式地满足的，这对它们在语言中的使用方式产生了深远的影响。\n设计良好的接口更可能是小型接口；流行的做法是一个接口只包含一个方法。从逻辑上讲，小接口使实现变得简单，反之则很难。因此形成了由普通行为的简单实现组成的 package。\nio.Reader type Reader interface { // Read reads up to len(buf) bytes into buf.  Read(buf []byte) (n int, err error) } 这令我很容易想到了我最喜欢的 Go 接口 io.Reader。\nio.Reader 接口非常简单； Read 将数据读入提供的缓冲区，并将读取的字节数和读取期间遇到的任何错误返回给调用者。看起来很简单，但非常强大。\n因为 io.Reader 可以处理任何表示为字节流的东西，所以我们几乎可以在任何东西上创建 Reader; 常量字符串，字节数组，标准输入，网络流，gzip 的 tar 文件，通过 ssh 远程执行的命令的标准输出。\n并且所有这些实现都可以互相替代，因为它们实现了相同的简单契约。\n因此，适用于 Go 的里氏替换原则，可以通过已故 Jim Weirich 的格言来概括。\n Require no more, promise no less. – Jim Weirich\n 接口隔离原则 第四个原则是接口隔离原则，其内容如下：\n Clients should not be forced to depend on methods they do not use. –Robert C. Martin\n 在 Go 中，接口隔离原则的应用可以指的是，隔离功能完成其工作所需的行为的过程。举一个具体的例子，假设我已经完成了‘编写一个将 Document 结构保存到磁盘的函数’的任务。\n// Save writes the contents of doc to the file f. func Save(f *os.File, doc *Document) error 我可以定义此函数，让我们称之为 Save，它将给定的 Document 写入到 *os.File。 但是这样做会有一些问题。\nSave 的签名排除了将数据写入网络位置的选项。假设网络存储可能以后成为需求，此功能的签名必须改变，并影响其所有调用者。\n由于 Save 直接操作磁盘上的文件，因此测试起来很不方便。要验证其操作，测试必须在写入后读取文件的内容。 此外，测试必须确保将 f 写入临时位置并随后将其删除。\n*os.File 还定义了许多与 Save 无关的方法，比如读取目录并检查路径是否是文件链接。 如果 Save 函数的签名能只描述 *os.File 相关的部分，将会很实用。\n我们如何处理这些问题呢？\n// Save writes the contents of doc to the supplied ReadWriterCloser. func Save(rwc io.ReadWriteCloser, doc *Document) error 使用 io.ReadWriteCloser 我们可以应用接口隔离原则，使用更通用的文件类型的接口来重新定义 Save。\n通过此更改，任何实现了 io.ReadWriteCloser 接口的类型都可以代替之前的 *os.File。使得 Save 应用程序更广泛，并向 Save 调用者阐明，*os.File 类型的哪些方法与操作相关。\n做为 Save 的编写者，我不再可以选择调用 *os.File 的那些不相关的方法，因为它隐藏在 io.ReadWriteCloser 接口背后。我们可以进一步采用接口隔离原理。\n首先，如果 Save 遵循单一责任原则，它将不可能读取它刚刚编写的文件来验证其内容 - 这应该是另一段代码的责任。因此，我们可以将我们传递给 Save 的接口的规范缩小，仅写入和关闭。\n// Save writes the contents of doc to the supplied WriteCloser. func Save(wc io.WriteCloser, doc *Document) error 其次，通过向 Save 提供一个关闭其流的机制，我们继续这种机制以使其看起来像文件类型的东西，这就产生一个问题，wc 会在什么情况下关闭。Save 可能会无条件地调用 Close，抑或在成功的情况下调用 Close。\n这给 Save 的调用者带来了问题，因为它可能希望在写入文档之后将其他数据写入流。\ntype NopCloser struct { io.Writer } // Close has no effect on the underlying writer. func (c *NopCloser) Close() error { return nil } 一个粗略的解决方案是定义一个新类型，它嵌入一个 io.Writer 并覆盖 Close 方法，以阻止 Save 方法关闭底层数据流。\n但这样可能会违反里氏替换原则，因为 NopCloser 实际上并没有关闭任何东西。\n// Save writes the contents of doc to the supplied Writer. func Save(w io.Writer, doc *Document) error 一个更好的解决方案是重新定义 Save 只接收 io.Writer，完全剥离它除了将数据写入流之外做任何事情的责任。\n通过应用接口隔离原则，我们的 Save 功能，同时得到了一个在需求方面最具体的函数 - 它只需要一个可写的参数 - 并且具有最通用的功能，现在我们可以使用 Save 保存我们的数据到任何一个实现 io.Writer 的地方。\n A great rule of thumb for Go is accept interfaces, return structs. – Jack Lindamood\n 退一步说，这句话是一个有趣的模因，在过去的几年里，它渗透入 Go 思潮。\n这个推特大小的版本缺乏细节，这不是 Jack 的错，但我认为它代表了第一个正当有理的 Go 设计传统\n依赖倒置原则 最后一个 SOLID 原则是依赖倒置原则，该原则指出：\n High-level modules should not depend on low-level modules. Both should depend on abstractions. Abstractions should not depend on details. Details should depend on abstractions. – Robert C. Martin\n 但是，对于 Go 程序员来说，依赖倒置在实践中意味着什么呢？\n如果您已经应用了我们之前谈到的所有原则，那么您的代码应该已经被分解为离散包，每个包都有一个明确定义的责任或目的。您的代码应该根据接口描述其依赖关系，并且应该考虑这些接口以仅描述这些函数所需的行为。 换句话说，除此之外没什么应该要做的。\n所以我认为，在 Go 的上下文中，Martin 所指的是 import graph 的结构。\n在 Go 中，import graph 必须是非循环的。 不遵守这种非循环要求将导致编译失败，但更为严重地是它代表设计中存在严重错误。\n在所有条件相同的情况下，精心设计的 Go 程序的 import graph 应该是宽的，相对平坦的，而不是高而窄的。 如果你有一个 package，其函数无法在不借助另一个 package 的情况下运行，那么这或许表明代码没有很好地沿 pakcage 边界分解。\n依赖倒置原则鼓励您将特定的责任，沿着 import graph 尽可能的推向更高层级，推给 main package 或顶级处理程序，留下较低级别的代码来处理抽象接口。\n总结 回顾一下，当应用于 Go 时，每个 SOLID 原则都是关于设计的强有力陈述，但综合起来它们具有中心主题。\n 单一职责原则，鼓励您将功能，类型、方法结构化为具有自然内聚的包；类型属于彼此，函数服务于单一目的。 开放 / 封闭原则，鼓励您使用嵌入将简单类型组合成更复杂的类型。 里氏替换原则，鼓励您根据接口而不是具体类型来表达包之间的依赖关系。通过定义小型接口，我们可以更加确信，实现将忠实地满足他们的契约。 接口隔离原则，进一步采用了这个想法，并鼓励您定义仅依赖于他们所需行为的函数和方法。如果您的函数仅需要具有单个接口类型的参数的方法，则该函数更可能只有一个责任。 依赖倒置原则，鼓励您按照从编译时间到运行时间的时序，转移 package 所依赖的知识。在 Go 中，我们可以通过特定 package 使用的 import 语句的数量减少看到了这一点。  如果要总结一下本次演讲，那可能就是这样：interfaces let you apply the SOLID principles to Go programs。\n因为接口让 Go 程序员描述他们的 package 提供了什么 - 而不是它怎么做的。换个说法就是 “解耦”，这确实是目标，因为越松散耦合的软件越容易修改。\n正如 Sandi Metz 所说：\n Design is the art of arranging code that needs to work today, and to be easy to change forever. – Sandi Metz\n 因为如果 Go 想要成为公司长期投资的语言，Go 程序的可维护性，更容易变更，将是他们决策的关键因素。\n","permalink":"https://srcio.cn/series/programming-go/golang-%E7%A8%8B%E5%BA%8F-solid-%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","summary":"可重用软件设计的五个原则，SOLID 原则： 单一职责原则（Single Responsibility Principle） 开放 / 封闭原则（Open / Closed Principle） 里","title":"Golang 程序 SOLID 设计原则"},{"content":"errors 包为你的 Go 程序提供一种对程序员调试、查看日志更友好的错误处理方式。\nGo 程序中传统的错误处理方法：\nif err != nil { return err } 递归的向上传递错误，这种方式有一个缺陷：最终处理错误的位置无法获取错误的调用上下文信息。\nerrors 包以不破坏错误的原始值的方式向错误中的添加调用上下文信息。\n获取包 go get github.com/pkg/errors 错误添加上下文 The errors.Wrap function returns a new error that adds context to the original error. For example\n_, err := ioutil.ReadAll(r) if err != nil { return errors.Wrap(err, \u0026#34;read failed\u0026#34;) } Retrieving the cause of an error Using errors.Wrap constructs a stack of errors, adding context to the preceding error. Depending on the nature of the error it may be necessary to reverse the operation of errors.Wrap to retrieve the original error for inspection. Any error value which implements this interface can be inspected by errors.Cause.\ntype causer interface { Cause() error } errors.Cause will recursively retrieve the topmost error which does not implement causer, which is assumed to be the original cause. For example:\nswitch err := errors.Cause(err).(type) { case *MyError: // handle specifically default: // unknown error } Read the package documentation for more information.\nRoadmap With the upcoming Go2 error proposals this package is moving into maintenance mode. The roadmap for a 1.0 release is as follows:\n 0.9. Remove pre Go 1.9 and Go 1.10 support, address outstanding pull requests (if possible) 1.0. Final release.  Contributing Because of the Go2 errors changes, this package is not accepting proposals for new functionality. With that said, we welcome pull requests, bug fixes and issue reports.\nBefore sending a PR, please discuss your change by raising an issue.\nLicense BSD-2-Clause\n","permalink":"https://srcio.cn/series/programming-go/gopkg-errors/","summary":"errors 包为你的 Go 程序提供一种对程序员调试、查看日志更友好的错误处理方式。 Go 程序中传统的错误处理方法： if err != nil { return err } 递归的向上传递错误，这种方式","title":"Go 包 - errors"},{"content":"场景介绍 从数据库获取到了菜单列表数据，这些菜单数据通过字段 ParentID 表示父子层级关系，现在需要将菜单列表数据转成树状的实例对象。\n数据库取出的初始数据：\nraw := []Menu{ {Name: \u0026#34;一级菜单 1\u0026#34;, ID: 1, PID: 0}, {Name: \u0026#34;一级菜单 2\u0026#34;, ID: 2, PID: 0}, {Name: \u0026#34;一级菜单 3\u0026#34;, ID: 3, PID: 0}, {Name: \u0026#34;二级菜单 1-1\u0026#34;, ID: 11, PID: 1}, {Name: \u0026#34;二级菜单 1-2\u0026#34;, ID: 12, PID: 1}, {Name: \u0026#34;二级菜单 1-3\u0026#34;, ID: 13, PID: 1}, {Name: \u0026#34;二级菜单 2-1\u0026#34;, ID: 21, PID: 2}, {Name: \u0026#34;二级菜单 2-2\u0026#34;, ID: 22, PID: 2}, {Name: \u0026#34;二级菜单 2-3\u0026#34;, ID: 23, PID: 2}, } 需要得到的目标数据：\n{ \u0026#34;name\u0026#34;:\u0026#34;根菜单\u0026#34;, \u0026#34;id\u0026#34;:0, \u0026#34;pid\u0026#34;:0, \u0026#34;SubMenus\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;一级菜单 1\u0026#34;, \u0026#34;id\u0026#34;:1, \u0026#34;pid\u0026#34;:0, \u0026#34;SubMenus\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 1-1\u0026#34;, \u0026#34;id\u0026#34;:11, \u0026#34;pid\u0026#34;:1 }, { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 1-2\u0026#34;, \u0026#34;id\u0026#34;:12, \u0026#34;pid\u0026#34;:1 }, { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 1-3\u0026#34;, \u0026#34;id\u0026#34;:13, \u0026#34;pid\u0026#34;:1 } ] }, { \u0026#34;name\u0026#34;:\u0026#34;一级菜单 2\u0026#34;, \u0026#34;id\u0026#34;:2, \u0026#34;pid\u0026#34;:0, \u0026#34;SubMenus\u0026#34;:[ { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 2-1\u0026#34;, \u0026#34;id\u0026#34;:21, \u0026#34;pid\u0026#34;:2 }, { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 2-2\u0026#34;, \u0026#34;id\u0026#34;:22, \u0026#34;pid\u0026#34;:2 }, { \u0026#34;name\u0026#34;:\u0026#34;二级菜单 2-3\u0026#34;, \u0026#34;id\u0026#34;:23, \u0026#34;pid\u0026#34;:2 } ] }, { \u0026#34;name\u0026#34;:\u0026#34;一级菜单 3\u0026#34;, \u0026#34;id\u0026#34;:3, \u0026#34;pid\u0026#34;:0 } ] } 代码实现 package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; ) func main() { // 数据库里存储的菜单 \trawMenus := []Menu{ {Name: \u0026#34;一级菜单 1\u0026#34;, ID: 1, PID: 0}, {Name: \u0026#34;一级菜单 2\u0026#34;, ID: 2, PID: 0}, {Name: \u0026#34;一级菜单 3\u0026#34;, ID: 3, PID: 0}, {Name: \u0026#34;二级菜单 1-1\u0026#34;, ID: 11, PID: 1}, {Name: \u0026#34;二级菜单 1-2\u0026#34;, ID: 12, PID: 1}, {Name: \u0026#34;二级菜单 1-3\u0026#34;, ID: 13, PID: 1}, {Name: \u0026#34;二级菜单 2-1\u0026#34;, ID: 21, PID: 2}, {Name: \u0026#34;二级菜单 2-2\u0026#34;, ID: 22, PID: 2}, {Name: \u0026#34;二级菜单 2-3\u0026#34;, ID: 23, PID: 2}, } menu := \u0026amp;Menu{ Name: \u0026#34;根菜单\u0026#34;, PID: 0, } for _, rm := range rawMenus { menu.setSubMenus(rm) } // 打印结果 \tb, _ := json.Marshal(menu) fmt.Println(string(b)) } type Menu struct { Name string `json:\u0026#34;name\u0026#34;` ID int `json:\u0026#34;id\u0026#34;` PID int `json:\u0026#34;pid\u0026#34;` SubMenus []Menu `json:\u0026#34;,omitempty\u0026#34;` } func (m *Menu) setSubMenus(menu Menu) bool { if menu.PID == m.ID { m.SubMenus = append(m.SubMenus, menu) return true } for i := range m.SubMenus { if m.SubMenus[i].setSubMenus(menu) { return true } } return false } 注意事项 Golang for 遍历使用 for _, item := range slice 时，item 是一份遍历元素的复制，而使用 for i := range slice 时，slice[i] 则是遍历元素本身，使用时需要注意切片扩容带来的地址变化问题。\n可以参考这篇来理解：👉🏻Golang 切片扩容\n","permalink":"https://srcio.cn/series/programming-go/%E6%95%B0%E7%BB%84%E9%80%92%E5%BD%92%E6%9E%84%E9%80%A0%E5%85%B7%E6%9C%89%E7%88%B6%E5%AD%90%E5%B1%82%E7%BA%A7%E5%85%B3%E7%B3%BB%E7%9A%84%E5%AF%B9%E8%B1%A1/","summary":"场景介绍 从数据库获取到了菜单列表数据，这些菜单数据通过字段 ParentID 表示父子层级关系，现在需要将菜单列表数据转成树状的实例对象。 数据库取出的初始数据","title":"数组递归构造具有父子层级关系的对象"},{"content":" 文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clusters-choosing-a-worker-node-size\n 在部署 Kubernetes 集群时，您首先会想到的问题之一恐怕就是：“我应该选择何种资源配额的计算节点以及应该配置多少个这样的节点才能满足计算需求？”。到底是使用少量的高级服务器还是使用大量的低端服务器更划算，更能满足需求呢？本文将从多个维度阐述不同的资源配置方式各自的优缺点，并从实践角度出发给出进行集群规划的一般方法。\n集群容量 首先，我们需要了解下本文关于集群容量的定义。一般来说，我们可以把 Kubernetes 集群看作是将一组单个节点抽象为了一个大的“超级节点”。这个超级节点的总计算容量（就 CPU 和内存而言）是所有组成节点资源容量的总和，也就是集群容量。显然，您可以采用多种不同的资源配置方式实现既定的目标集群容量。\n例如，假如您需要一个总容量为 8 个 CPU 和 32GB 内存的集群。\n 例如，因为要在集群上运行的应用程序需要此数量的资源。\n 以下是实现集群的两种可能方法：\n 通过这两种方式构建的集群拥有相同的资源容量，但是一种是使用 4 个较小的节点，而另一种是使用 2 个较大的节点。\n究竟哪种配置方式更好呢？\n为了解决这个问题，让我们对比下这两个相反的方向（即更少的高配节点与更多的低配节点）各自的优缺点。\n 请注意，本文中的“节点”始终代指工作节点。集群主节点数量和大小的选择是完全不同的主题。\n 更少的高配节点 这方面最极端的一个例子就是由单个工作节点提供整个集群的计算容量。\n在上面的示例中，这将是一个具有 16 个 CPU 和 16GB 内存的单个工作节点。\n优势 让我们来看看这种方法可能具有的优势。\n更少的管理开销 简单来说，管理少量机器相比管理大量机器会更省力。对节点进行升级和打补丁的操作能很迅速地完成，节点间的同步保持也更容易。此外，对于很少的机器而言，预期故障的绝对数量也会小于使用大量机器的场景。\n但请注意，这主要适用于裸机服务器而不适用于云实例。\n如果您使用云实例（作为托管 Kubernetes 服务的一部分或在云基础架构上安装的 Kubernetes），则实际上是将底层机器的管理外包给了云提供商。因此，管理云中的 10 个节点可能并不比管理云中的单个节点耗费更多管理成本。\n更低的单节点成本 虽然高端机器比低端机器更昂贵，但价格上涨不一定是线性的。换句话说，具有 10 个 CPU 和 10GB 内存的单台机器可能比具有 1 个 CPU 和 1GB 内存的 10 台机器便宜。\n但请注意，如果您使用云实例，这个原则可能并不适用。\n在主要的云提供商 Amazon Web Services、Google Cloud Platform 和 Microsoft Azure 的当前定价方案中，实例价格会随容量线性增加。例如，在 Google Cloud Platform 上，64 个 n1-standard-1 实例的成本与单个 n1-standard-64 实例完全相同——这两种方式都为您提供 64 个 CPU 和 240GB 内存。因此，在云上，您通常无法通过使用更大的机器来节省资金投入。\n可运行饥饿型应用 具备大型节点可能只是您要在集群中运行的应用程序类型的需求。\n例如，如果您有一个需要 8GB 内存的机器学习应用程序，则无法在仅具有 1GB 内存的节点的集群上运行它。但是，您可以在具有 10GB 内存节点的集群上运行它。\n劣势 看完了优势，让我们再来看看劣势。\n单节点运行大量 Pod 在较少的节点上运行相同的工作负载自然意味着在每个节点上运行更多的 Pod。\n这可能会成为一个问题。\n原因是每个 Pod 都会为在节点上运行的 Kubernetes 代理程序引入一些开销——例如容器运行时（如 Docker）、kubelet 和 cAdvisor。\n例如，kubelet 对节点上的每个容器执行周期性的 liveness 和 readiness 探测——更多容器意味着在每轮迭代中 kubelet 将执行更多工作。cAdvisor 收集节点上所有容器的资源使用统计信息，并且 kubelet 定期查询此信息并在其 API 上公开它——再次，这意味着每轮迭代中 cAdvisor 和 kubelet 的工作量都会增加。\n随着 Pod 数量的增长，这些问题的聚积可能会开始减慢系统速度，甚至使集群系统变得不可靠。\n 有报告称，节点被报告为未就绪，是因为周期性的的 kubelet 运行状况检查花费了太长时间来迭代节点上的所有容器。\n出于这些原因，Kubernetes 官方建议每个节点最多 110 个 Pod。对于这个数字，Kubernetes 已经进行过相关测试，可以在一般类型的节点上可靠地工作。\n当然，如果节点的性能足够好，您可能也能够成功地让每个节点运行更多的 Pod ——但很难预测这是否能够顺利进行，也许会遇到一些问题。\n大多数托管 Kubernetes 服务甚至对每个节点的 Pod 数量施加了严格的限制：\n 在 Amazon Elastic Kubernetes Service（EKS）上，每个节点的最大 Pod 数取决于节点类型，范围从 4 到 737。 在 Google Kubernetes Engine（GKE）上，无论节点类型如何，每个节点的限制为 100 个 Pod。 在 Azure Kubernetes Service（AKS）上，默认限制是每个节点 30 个 Pod，但最多可以增加到 250 个。  因此，如果您计划为每个节点运行大量 Pod，则应该事先进行测试，看能否按预期那样工作。\n有限的副本数量 较少的节点可能会限制应用程序的副本数量。\n例如，如果您有一个由 5 个副本组成的高可用应用程序，但您只有 2 个节点，那么应用程序的有效副本数量将减少到 2。这是因为 5 个副本只能分布在 2 个节点上，如果其中一个节点失败，它可能会同时挂掉该节点上的多个副本。相反，如果您有至少 5 个节点，则每个副本可以在单独的节点上运行，并且单个节点的故障最多只会挂掉其中一个副本。\n因此，如果您有高可用要求，则可能需要集群节点数大于某个下限值。\n更高的爆炸半径 如果您只有几个工作节点，那么节点失败造成的影响比使用大量节点时的影响要大。\n例如，如果您只有两个节点，那其中一个节点出现故障，就意味着一半的节点会消失。Kubernetes 可以将失败节点的工作负载重新安排到其他节点。但是，如果您只有几个节点，那风险也会增加，因为剩余节点上可能没有足够的备用资源容量来容纳故障节点的所有工作负载。结果是，部分应用程序将永久停机，直到再次启动故障节点。\n因此，如果您想减少硬件故障的影响，则应该选择更多的节点。\n更大的资源伸缩增量 Kubernetes 为云基础架构提供了 Cluster Autoscaler，允许根据当前需求自动添加或删除节点。如果使用大型节点，则会有较大的资源伸缩增量，这会使资源扩缩容更加笨重。\n例如，如果您只有 2 个节点，则添加节点意味着将群集容量增加 50%。这可能比您实际需要的资源多得多，就意味着您需要为未使用的资源付费。\n因此，如果您计划使用集群的自动弹性伸缩功能，则较小的节点允许您进行更轻量且经济高效的资源扩缩容。\n更多的低配节点 在讨论了更少高配节点的优缺点之后，让我们转向更多低配节点的场景。\n这种方法通过许多低配节点构建集群，而不使用更少的高配节点。\n*那它的优缺点又是什么呢？*\n优势 使用更多低配节点的优点正对应于使用更少高配节点的缺点。\n减少爆炸半径 如果您有更多节点，则每个节点上的 Pod 自然会更少。\n例如，如果您有 100 个 Pod 和 10 个节点，则每个节点平均只包含 10 个 Pod。这样，即便其中一个节点发生故障，它的影响也仅限于总工作负载的较小的一部分。有可能只有部分应用程序受到影响，并且可能只有少量副本挂掉，因此整个应用程序会仍然保持运行状态。\n此外，剩余节点上的备用资源很可能足以容纳故障节点的工作负载，因此Kubernetes 可以重新安排所有 Pod，并且您的应用程序可以相对快速地恢复到完全正常的运行状态。\n允许更多副本 如果您有一个多副本高可用应用程序以及足够的可用节点，Kubernetes 调度程序可以将每个副本分配给不同的节点。\n 您可以通过节点亲和、Pod 亲和/反亲和以及污点和容忍来影响调度程序对 Pod 的调度。\n 这意味着如果某个节点出现故障，则最多只有一个副本受影响，且您的应用程序仍然可用。\n劣势 看了使用更多低配节点的优点，那它有什么缺点呢？\n较大的节点数量 如果使用较小的节点，则自然需要更多节点来实现给定的集群容量。\n但是大量节点对 Kubernetes 控制平面来说可能是一个挑战。\n例如，每个节点都需要能够与其他节点通信，这使得可能的通信路径数量会按照节点数量的平方增长——所有节点都必须由控制平面管理。\nKubernetes controller manager 中的节点控制器定期遍历集群中的所有节点以运行状况检查——更多节点意味着节点控制器的负载更多。\n更多节点同时也意味着 etcd 数据库上的负载也更多——每个 kubelet 和 kube-proxy 都会成为一个 etcd 的 watcher 客户端（通过 APIServer），etcd 必须广播对象变化到这些客户端。\n通常，每个工作节点都会对主节点上的系统组件施加一些开销。\n 据官方统计，Kubernetes 声称支持最多 5000 个节点的集群。然而，在实践中，500 个节点可能已经形成了巨大的挑战。\n通过使用性能更高的主节点，往往可以减轻大量工作节点带来的影响。这也正是目前在实践中所应用的——这里是 kube-up 在云基础架构上使用的主节点大小：\n Google Cloud Platform  5 个工作节点 → n1-standard-1 主节点 500 个工作节点 → n1-standard-32 主节点   Amazon Web Services  5 个工作节点 → m3.medium 主节点 500 个工作节点 → c4.8xlarge 主节点    如您所见，对于 500 个工作节点，使用的主节点分别具有 32 和 36 个 CPU 以及 120GB 和 60GB 内存。\n这些都是相当大的机器！\n因此，如果您打算使用大量低配节点，则需要记住两件事：\n 您拥有的工作节点越多，主节点需要的性能就越高 如果您计划使用超过 500 个节点，则可能会遇到一些需要付出一些努力才能解决的性能瓶颈   像 Virtual Kubelet 这样的新开发产品允许您绕过这些限制，以构建具有大量工作节点的集群。\n 更多的系统开销 Kubernetes 在每个工作节点上运行一组系统守护进程——包括容器运行时（如 Docker）、kube-proxy 和包含 cAdvisor 的 kubelet。\n cAdvisor 包含在 kubelet 二进制文件中。\n 所有这些守护进程一起消耗固定数量的资源。\n如果使用许多低配节点，则这些系统组件消耗的资源占比会增大。\n例如，假设单个节点的所有系统守护程序一起使用 0.1 个 CPU 和 0.1GB 内存。如果您拥有 10 个 CPU 和 10GB 内存的单个节点，那么守护程序将占用集群容量的 1%。而如果您有 1 个 CPU 和 1GB 内存的 10 个节点，则后台程序将占用集群容量的 10%。在第二种情况下，10% 的资源消耗用于运行系统，而在第一种情况下，它只占 1%。\n因此，如果您希望最大化基础架构支出的回报，那么您可能会喜欢更少的节点。\n更低的资源利用率 如果您使用较小的节点，那么可能会产生大量资源碎片因资源太少而无法分配给任何工作负载，最终保持未使用状态。\n例如，假设您的所有 Pod 都需要 0.75GB 的内存。如果您有 10 个 1GB 内存的节点，那么最多可以运行 10 个这样的 Pod——您最终会在每个节点上有一块 0.25GB 的内存不能使用。这意味着，集群总内存的 25% 被浪费了。相反，如果您使用具有 10GB 内存的单个节点，那么您可以运行 13 个 Pod ——您最终会在这个节点上有一块 0.25GB 的内存不能使用。在这种情况下，您只会浪费 2.5% 的内存。\n因此，如果您想最大限度地减少资源浪费，使用更大的节点可能会带来更好的结果。\nPod 运行数量限制 在某些云基础架构上，低配节点上允许的最大 Pod 数量比您预期的要限制得更多。\nAmazon Elastic Kubernetes Service（EKS）就是这种情况，其中每个节点的最大 Pod 数取决于实例类型。\n例如，对于 t2.medium 实例，最大 Pod 数为 17，t2.small 为 11，而 t2.micro为 4。\n这些都是非常小的数字！\n任何超出这些限制的 Pod 都无法由 Kubernetes 调度，并被无限期地保持在 Pending 状态。\n如果您不了解这些限制，则可能导致难以发现的错误。\n因此，如果您计划在 Amazon EKS 上使用低配节点，请检查相应的每节点 Pod 数量限制，并计算节点是否可以容纳所有 Pod。\n结论 那么，您应该在集群中使用更少的高配节点还是更多的低配节点呢？\n这没有明确的答案。\n您要部署到集群的应用程序类型可能会影响您的决策。\n例如，如果您的应用程序需要 10GB 内存，则可能不应使用低配节点——集群中的节点应至少具有 10GB 内存。或者，如果您的应用程序需要 10 副本以实现高可用，那么您可能不应该只使用 2 个节点——您的集群应该至少有 10 个节点。\n对于中间的所有场景，它取决于您的具体需求。\n以上哪项优缺点与您相关？哪项与您不相关？\n话虽如此，但没有规则说所有节点必须具有相同的大小。没有什么能阻止您使用不同大小的节点来混合构建集群。Kubernetes 集群的工作节点可以是完全异构的。这可能会让您权衡两种方法的优缺点。\n最后，实践是检验真理的唯一标准——最好的方法是反复试验并找到最适合您的资源配置组合！\n","permalink":"https://srcio.cn/posts/k8s-node-resource-config/","summary":"文章转载自：https://sataqiu.github.io/2019/09/09/architecting-kubernetes-clu","title":"K8s 集群规划之节点资源配置"},{"content":" 文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html\n Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。\n镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。\n主体流程  如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。\n需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。\n用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：\n image-gc-high-threshold：磁盘使用率上限，有效范围 [0-100]，默认 85 image-gc-low-threshold：磁盘使用率下限，有效范围 [0-100]，默认 80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟  实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。\n实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。\n# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ... ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --image-gc-high-threshold=2 --image-gc-low-threshold=1 ... 我们在 Kubelet 启动参数的最后追加了 --image-gc-high-threshold=2 --image-gc-low-threshold=1，这么低的配置，Kubelet 应该会一直忙于进行镜像回收了，生产环境可不能这么配置！\n执行以下命令使得配置生效：\n# systemctl daemon-reload # systemctl restart kubelet 首先，看下本地都有哪些镜像：\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 接下来，我们运行一个 nginx 程序，让 Kubelet 自动拉取镜像。\nroot@shida-machine:~# kubectl run nginx --image=nginx deployment.apps/nginx created root@shida-machine:~# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 62s root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，nginx 镜像已经被自动 pull 到本地了，ID 为 f68d6e55e065。\n然后，删除 nginx Deployment：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \u0026quot;nginx\u0026quot; deleted 过大概 5 分钟后，再次检查本地镜像列表，发现 nginx 镜像已被清理！\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 通过以下命令查看镜像垃圾回收日志：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:03:20.883489 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72470076620 bytes down to the low threshold (1%). I0714 18:03:20.899370 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes 可以看到，日志中记录的删除镜像 ID 与 nginx 镜像的 ID 是一致的（均为 f68d6e55e065）。\n继续验证用户手动拉取的镜像是否会被清理，手动运行 nginx 程序：\nroot@shida-machine:~# docker run --name nginx -d nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx fc7181108d40: Pull complete d2e987ca2267: Pull complete 0b760b431b11: Pull complete Digest: sha256:48cbeee0cb0a3b5e885e36222f969e0a2f41819a68e07aeb6631ca7cb356fed1 Status: Downloaded newer image for nginx:latest 2fc8a836ba3c7cbd488c7fd4f2ffa7287b709abf1b7701685291c3b1e5df3472 通过查看镜像 GC 日志，会发现 GC 会尝试清理用户自己手动拉取的 nginx 镜像，但因为该镜像被使用中，所以这次删除操作不会成功：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:28:23.015586 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72501525708 bytes down to the low threshold (1%). I0714 18:28:23.306696 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 将该容器停止，继续观察回收动作：\nroot@shida-machine:~# docker stop nginx nginx root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:53:23.579629 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72549280972 bytes down to the low threshold (1%). I0714 18:53:23.629492 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，对于已经停止的容器，Kubelet 也是会尝试删除，但删除操作依然不会成功（存在死亡容器对该镜像的引用）。\n彻底删除 nginx 容器，此时就没有任何容器继续使用该镜像，经过 1 次 GC 后，nginx 镜像就会被清理。\nroot@shida-machine:~# docker rm nginx nginx root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 容器回收 了解了镜像回收的基本原理，我们再来看看容器回收。容器在停止运行（比如出错退出或者正常结束）后会残留一系列的垃圾文件，一方面会占据磁盘空间，另一方面也会影响系统运行速度。此时，就需要 Kubelet 容器回收了。要特别注意的是，Kubelet 回收的容器是指那些由其管理的的容器（也就是 Pod 容器），用户手动运行的容器不会被 Kubelet 进行垃圾回收。\n与容器垃圾回收相关的控制参数主要有 3 个：\n  MinAge：容器可以被执行垃圾回收的最小年龄\n  MaxPerPodContainer：每个 pod 内允许存在的死亡容器的最大数量\n  MaxContainers：节点上全部死亡容器的最大数量\n   注意：当 MaxPerPodContainer 与 MaxContainers 发生冲突时，Kubelet 会自动调整 MaxPerPodContainer 的取值以满足 MaxContainers 要求。\n 主体流程  容器回收主要针对三个目标资源：普通容器、sandbox 容器以及容器日志目录。\n对于普通容器，主要根据 MaxPerPodContainer 与 MaxContainers 的设置，按照 LRU 策略，从 Pod 的死亡容器列表删除一定数量的容器，直到满足配置需求；对于 sandbox 容器，按照每个 Pod 保留一个的原则清理多余的死亡 sandbox；对于日志目录，只要没有 Pod 与之关联了就将其删除。\nKubelet 的容器垃圾回收只针对 Pod 容器，非 Kubelet Pod 容器（比如通过 docker run 启动的容器）不会被主动清理。\n用户配置 影响容器垃圾回收的关键参数有：\nminimum-container-ttl-duration：容器可被回收的最小生存年龄，默认是 0 分钟，这意味着每个死亡容器都会被立即执行垃圾回收\nmaximum-dead-containers-per-container`：每个 Pod 要保留的死亡容器的最大数量，默认值为 `1 maximum-dead-containers：节点可保留的死亡容器的最大数量，默认值是 -1，这意味着节点没有限制死亡容器数量\n实验环节 还是以 nginx 为例，创建一个 nginx 服务：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 16 seconds ago Up 14 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到，Kubelet 启动了一个 sandbox 以及一个 nginx 实例。\n手动杀死 nginx 实例，模拟容器异常退出：\nroot@shida-machine:~# docker kill 7bef0308d9ea 7bef0308d9ea root@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 3 seconds ago Up 2 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 2 minutes ago Exited (137) 15 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 5 minutes ago Up 5 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到 Kubelet 重新拉起了一个新的 nginx 实例。\n等待几分钟，发现 Kubelet 并未清理异常退出的 nginx 容器（因为此时仅有一个 dead container）。\nroot@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 3 minutes ago Up 3 minutes k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 5 minutes ago Exited (137) 3 minutes ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 8 minutes ago Up 8 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 继续杀死当前 nginx 实例：\nroot@shida-machine:~# docker kill 408b23b2b72a 408b23b2b72a root@shida-machine:~# docker ps -a | grep nginx e064e376819f nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 9 seconds ago Up 7 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_2 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 5 minutes ago Exited (137) 40 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 10 minutes ago Up 10 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 这下看到效果了，仍然只有一个退出的容器被保留，而且被清理掉的是最老的死亡容器，这与之前的分析是一致的！\n删除这个 nginx Deployment，会发现所有的 nginx 容器都会被清理：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \u0026quot;nginx\u0026quot; deleted root@shida-machine:~# docker ps -a | grep nginx root@shida-machine:~# 进一步，我们修改 Kubelet 参数，设置 maximum-dead-containers 为 0，这就告诉 Kubelet 清理所有死亡容器。\n重复前边的实验步骤：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 8de9ae8e2c9b nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 33 seconds ago Up 32 seconds k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 41 seconds ago Up 38 seconds k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 root@shida-machine:~# docker kill 8de9ae8e2c9b 8de9ae8e2c9b root@shida-machine:~# docker ps -a | grep nginx 95ee5bd2cab2 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Up About a minute k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_1 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 2 minutes ago Up About a minute k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 结果显示，nginx Pod 的所有死亡容器都会被清理，因为我们已经强制要求节点不保留任何死亡容器，与预期一致！\n那对于手动运行的容器呢？我们通过 docker run 运行 nginx：\nroot@shida-machine:~# docker run --name nginx -d nginx 46ebb365f6be060a6950f44728e4f11e4666bf2fb007cad557ffc65ecf8aded8 root@shida-machine:~# docker ps | grep nginx 46ebb365f6be nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 9 seconds ago Up 6 seconds 80/tcp nginx 杀死该容器：\nroot@shida-machine:~# docker kill 46ebb365f6be 46ebb365f6be root@shida-machine:~# docker ps -a | grep nginx 46ebb365f6be nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Exited (137) 18 seconds ago nginx 经过几分钟，我们发现该死亡容器还是会存在的，Kubelet 不会清理这类容器！\n小结 Kubelet 每 5 分钟进行一次镜像清理。当磁盘使用率超过上限阈值，Kubelet 会按照 LRU 策略逐一清理没有被任何容器所使用的镜像，直到磁盘使用率降到下限阈值或没有空闲镜像可以清理。Kubelet 认为镜像可被清理的标准是未被任何 Pod 容器（包括那些死亡了的容器）所引用，那些非 Pod 容器（如用户通过 docker run 启动的容器）是不会被用来计算镜像引用关系的。也就是说，即便用户运行的容器使用了 A 镜像，只要没有任何 Pod 容器使用到 A，那 A 镜像对于 Kubelet 而言就是可被回收的。但是我们无需担心手动运行容器使用的镜像会被意外回收，因为 Kubelet 的镜像删除是非 force 类型的，底层容器运行时会使存在容器关联的镜像删除操作失败（因为 Docker 会认为仍有容器使用着 A 镜像）。\nKubelet 每 1 分钟执行一次容器清理。根据启动配置参数，Kubelet 会按照 LRU 策略依次清理每个 Pod 内的死亡容器，直到达到死亡容器限制数要求，对于 sandbox 容器，Kubelet 仅会保留最新的（这不受 GC 策略的控制）。对于日志目录，只要已经没有 Pod 继续占用，就将其清理。对于非 Pod 容器（如用户通过 docker run 启动的容器）不会被 Kubelet 垃圾回收。\n","permalink":"https://srcio.cn/posts/kubelet-recycle-policy/","summary":"文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html Kubelet 垃","title":"Kubelet 垃圾回收原理剖析"},{"content":"CRD 字段校验配置\napiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:scalings.control.srcio.iospec:group:control.srcio.ioversions:- name:v1served:truestorage:truescope:Namespacednames:plural:scalingssingular:scalingkind:Scalingvalidation:openAPIV3Schema:properties:spec:required:- targetDeployment- minReplicas- maxReplicas- metricType- step- scaleUp- scaleDownproperties:targetDeployment:type:stringminReplicas:type:integerminimum:0maxReplicas:type:integerminimum:0metricType:type:stringenum:- CPU- MEMORY- REQUESTSstep:type:integerminimum:1scaleUp:type:integerscaleDown:type:integerminimum:0  是否必须 参数类型 枚举范围 数值最大最小   ","permalink":"https://srcio.cn/series/programming-kubernetes/crd/","summary":"CRD 字段校验配置 apiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:scalings.control.srcio.iospec:group:control.srcio.ioversions:- name:v1served:truestorage:truescope:Namespacednames:plural:scalingssingular:scalingkind:Scalingvalidation:openAPIV3Schema:properties:spec:required:- targetDeployment- minReplicas- maxReplicas- metricType- step- scaleUp- scaleDownproperties:targetDeployment:type:stringminReplicas:type:integerminimum:0maxReplicas:type:integerminimum:0metricType:type:stringenum:- CPU- MEMORY- REQUESTSstep:type:integerminimum:1scaleUp:type:integerscaleDown:type:integerminimum:0 是否必须 参数类型 枚举范围 数值最大最小","title":"CRD 简介"},{"content":"怎么理解切片 s = append(s, item) 需要使用 s 重新接收呢？\n 在 golang 语言中所有的参数传递的方式都是值传递的，即便是指针，也是复制了一份指针传递； 切片发生扩容后，底层的数组发生了变化，不再是原来的数组结构。   ","permalink":"https://srcio.cn/series/programming-go/golang-%E5%88%87%E7%89%87%E6%89%A9%E5%AE%B9/","summary":"怎么理解切片 s = append(s, item) 需要使用 s 重新接收呢？ 在 golang 语言中所有的参数传递的方式都是值传递的，即便是指针，也是复制了一份指针传递； 切片发生扩容后，底","title":"Golang 切片扩容"},{"content":"安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。\n使用一下命令安装或升级 Docker Compose（linux）：\n Ubuntu，Debian：  sudo apt update sudo apt install docker-compose-plugin  基于 RPM 发行版:  sudo yum update sudo yum install docker-compose-plugin 验证安装版本：\ndocker-compose version 常用命令 运行\ndocker-compose up 查看运行\ndocker-compose ps 停止\ndocker-compose stop 启动\u0026amp;重启\ndocker-compose start docker-compose restart 退出\ndocker-compose down 使用 docker-compose -h 查看更多命令及参数。\n实践 使用 Docker Compose 运行一个简单的 golang web 程序。\n 程序初始化  mkdir docker-compose-go-demo cd docker-compose-go-demo go mod init docker-compose-go-demo 创建 main.go 文件，并写入程序代码  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func greet(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello Docker Compose! %s\u0026#34;, time.Now()) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, greet) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } 创建 Dockerfile 文件，并编写内容  FROMgolang:alpineWORKDIR/appCOPY . .EXPOSE8080ENTRYPOINT [ \u0026#34;go\u0026#34;,\u0026#34;run\u0026#34;,\u0026#34;main.go\u0026#34; ]创建 docker-comppose.yml 文件，并编写内容  version:\u0026#34;3.9\u0026#34;services:web:build:.# image: docker-compose-go-demo_web:v1# image: docker-compose-go-demo_web:v2ports:- \u0026#34;8080:8080\u0026#34;启动服务  docker-compose up -d 场景：\n web 服务业务代码修改了，希望不停机更新服务：  docker-compose up -d --build  包含多个服务，例如中间件，但只想重新编译其中业务服务，如 web：  docker-compose up -d --no-deps --build web  如果 docker-compose.yml 直接使用的镜像，那么直接更新，再次 docker-compose up -d 即可。\n ","permalink":"https://srcio.cn/posts/docker-compose/","summary":"安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。 使用一下命令安装或升级 Docker Compose（linu","title":"Docker Compose 实践"},{"content":"代码实现 package certutil import ( \u0026#34;bytes\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;time\u0026#34; ) // CA ca type CA struct { caInfo *x509.Certificate caPrivKey *rsa.PrivateKey caPem, caKeyPem []byte } // GetCAPem get ca pem bytes func (c *CA) GetCAPem() ([]byte, error) { if c.caPem == nil { // create the CA \tcaBytes, err := x509.CreateCertificate(rand.Reader, c.caInfo, c.caInfo, \u0026amp;c.caPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, err } // pem encode \tcaPEM := new(bytes.Buffer) _ = pem.Encode(caPEM, \u0026amp;pem.Block{ Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: caBytes, }) c.caPem = caPEM.Bytes() } return c.caPem, nil } // GetCAKeyPem get ca key pem func (c *CA) GetCAKeyPem() ([]byte, error) { if c.caKeyPem == nil { caPrivKeyPEM := new(bytes.Buffer) _ = pem.Encode(caPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026#34;RSA PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(c.caPrivKey), }) c.caKeyPem = caPrivKeyPEM.Bytes() } return c.caKeyPem, nil } // CreateCert make Certificate func (c *CA) CreateCert(ips []string, domains ...string) (certPem, certKey []byte, err error) { var ipAddresses []net.IP for _, ip := range ips { if i := net.ParseIP(ip); i != nil { ipAddresses = append(ipAddresses, i) } } // set up our server certificate \tcert := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(2019), Subject: pkix.Name{ Organization: []string{\u0026#34;srcio.cn\u0026#34;}, Country: []string{\u0026#34;CN\u0026#34;}, Province: []string{\u0026#34;Beijing\u0026#34;}, Locality: []string{\u0026#34;Beijing\u0026#34;}, StreetAddress: []string{\u0026#34;Beijing\u0026#34;}, PostalCode: []string{\u0026#34;000000\u0026#34;}, }, DNSNames: domains, IPAddresses: ipAddresses, NotBefore: time.Now(), NotAfter: time.Now().AddDate(99, 0, 0), SubjectKeyId: []byte{1, 2, 3, 4, 6}, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature, } certPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return nil, nil, err } certBytes, err := x509.CreateCertificate(rand.Reader, cert, c.caInfo, \u0026amp;certPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, nil, err } certPEM := new(bytes.Buffer) _ = pem.Encode(certPEM, \u0026amp;pem.Block{ Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: certBytes, }) certPrivKeyPEM := new(bytes.Buffer) _ = pem.Encode(certPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026#34;RSA PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(certPrivKey), }) return certPEM.Bytes(), certPrivKeyPEM.Bytes(), nil } // CreateCA create ca info func CreateCA() (*CA, error) { // set up our CA certificate \tca := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(2019), Subject: pkix.Name{ Organization: []string{\u0026#34;srcio.cn\u0026#34;}, Country: []string{\u0026#34;CN\u0026#34;}, Province: []string{\u0026#34;Beijing\u0026#34;}, Locality: []string{\u0026#34;Beijing\u0026#34;}, StreetAddress: []string{\u0026#34;Beijing\u0026#34;}, PostalCode: []string{\u0026#34;000000\u0026#34;}, }, NotBefore: time.Now(), NotAfter: time.Now().AddDate(99, 0, 0), IsCA: true, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, } // create our private and public key \tcaPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return nil, err } return \u0026amp;CA{ caInfo: ca, caPrivKey: caPrivKey, }, nil } // ParseCA parse caPem func ParseCA(caPem, caKeyPem []byte) (*CA, error) { p := \u0026amp;pem.Block{} p, caPem = pem.Decode(caPem) ca, err := x509.ParseCertificate(p.Bytes) if err != nil { return nil, err } p2 := \u0026amp;pem.Block{} p2, caKeyPem = pem.Decode(caKeyPem) caKey, err := x509.ParsePKCS1PrivateKey(p2.Bytes) if err != nil { return nil, err } return \u0026amp;CA{ caInfo: ca, caPrivKey: caKey, caPem: caPem, caKeyPem: caKeyPem, }, nil } // DomainSign create cert func DomainSign(ips []string, domains ...string) ([]byte, []byte, []byte, error) { ca, err := CreateCA() if err != nil { return nil, nil, nil, err } caPem, err := ca.GetCAPem() if err != nil { return nil, nil, nil, err } certPem, certKey, err := ca.CreateCert(ips, domains...) if err != nil { return nil, nil, nil, err } return caPem, certPem, certKey, nil } ","permalink":"https://srcio.cn/series/programming-go/golang-%E7%94%9F%E6%88%90%E8%AF%81%E4%B9%A6/","summary":"代码实现 package certutil import ( \u0026#34;bytes\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;time\u0026#34; ) // CA ca type CA struct { caInfo *x509.Certificate caPrivKey *rsa.PrivateKey caPem, caKeyPem []byte } // GetCAPem get ca pem bytes func (c *CA) GetCAPem() ([]byte, error) { if c.caPem == nil { // create the CA caBytes, err := x509.CreateCertificate(rand.Reader, c.caInfo, c.caInfo, \u0026amp;c.caPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil,","title":"Golang 生成证书"},{"content":"TLS 传输层安全协议（TLS），在互联网上，通常是由服务器单向的向客户端提供证书，以证明其身份。\nmTLS 双向 TLS 认证，是指在客户端和服务器之间使用双行加密通道，mTLS 是云原生应用中常用的通信安全协议。\n使用双向TLS连接的主要目的是当服务器应该只接受来自有限的允许的客户端的 TLS 连接时。例如，一个组织希望将服务器的 TLS 连接限制为只来自该组织的合法合作伙伴或客户。显然，为客户端添加IP白名单不是一个好的安全实践，因为IP可能被欺骗。\n为了简化 mTLS 握手的过程，我们这样简单梳理：\n  客户端发送访问服务器上受保护信息的请求；\n  服务器向客户端提供公钥证书；\n  客户端通过使用 CA 的公钥来验证服务器公钥证书的数字签名，以验证服务器的证书；\n  如果步骤 3 成功，客户机将其客户端公钥证书发送到服务器；\n  服务器使用步骤 3 中相同的方法验证客户机的证书；\n  如果成功，服务器将对受保护信息的访问权授予客户机。\n  代码实现 需要实现客户端验证服务端的公钥证书，服务端验证客户端的公钥证书。\n生成证书 echo \u0026#39;清理并生成目录\u0026#39; OUT=./certs DAYS=365 RSALEN=2048 CN=srcio rm -rf ${OUT}/* mkdir ${OUT} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 cd ${OUT} echo \u0026#39;生成CA的私钥\u0026#39; openssl genrsa -out ca.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成CA的签名证书\u0026#39; openssl req -new \\ -x509 \\ -key ca.key \\ -subj \u0026#34;/CN=${CN}\u0026#34; \\ -out ca.crt echo \u0026#39;\u0026#39; echo \u0026#39;生成server端私钥\u0026#39; openssl genrsa -out server.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成server端自签名\u0026#39; openssl req -new \\ -key server.key \\ -subj \u0026#34;/CN=${CN}\u0026#34; \\ -out server.csr echo \u0026#39;签发server端证书\u0026#39; openssl x509 -req -sha256 \\ -in server.csr \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -out server.crt -text \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;删除server端自签名证书\u0026#39; rm server.csr echo \u0026#39;\u0026#39; echo \u0026#39;生成client私钥\u0026#39; openssl genrsa -out client.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成client自签名\u0026#39; openssl req -new \\  -subj \u0026#34;/CN=${CN}\u0026#34; \\  -key client.key \\  -out client.csr echo \u0026#39;签发client证书\u0026#39; openssl x509 -req -sha256\\  -CA ca.crt -CAkey ca.key -CAcreateserial\\  -days ${DAYS}\\  -in client.csr\\  -out client.crt\\  -text \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;删除client端自签名\u0026#39; rm client.csr echo \u0026#39;\u0026#39; echo \u0026#39;删除临时文件\u0026#39; rm ca.srl echo \u0026#39;\u0026#39; echo \u0026#39;完成\u0026#39;% 服务端 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) var ( caCert = \u0026#34;../../certs/ca.crt\u0026#34; serverCert = \u0026#34;../../certs/server.crt\u0026#34; serverKey = \u0026#34;../../certs/server.key\u0026#34; ) type mtlsHandler struct { } func (m *mtlsHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello World! \u0026#34;, time.Now()) } func main() { pool := x509.NewCertPool() caCertBytes, err := os.ReadFile(caCert) if err != nil { panic(err) } pool.AppendCertsFromPEM(caCertBytes) server := \u0026amp;http.Server{ Addr: \u0026#34;:8443\u0026#34;, Handler: \u0026amp;mtlsHandler{}, TLSConfig: \u0026amp;tls.Config{ ClientCAs: pool, ClientAuth: tls.RequireAndVerifyClientCert, // 需要客户端证书 \t}, } log.Println(\u0026#34;server started...\u0026#34;) log.Fatalln(server.ListenAndServeTLS(serverCert, serverKey)) } 客户端 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) var ( caCert = \u0026#34;../../certs/ca.crt\u0026#34; clientCert = \u0026#34;../../certs/client.crt\u0026#34; clientKey = \u0026#34;../../certs/client.key\u0026#34; ) func main() { pool := x509.NewCertPool() caCertBytes, err := os.ReadFile(caCert) if err != nil { panic(err) } pool.AppendCertsFromPEM(caCertBytes) clientCertBytes, err := tls.LoadX509KeyPair(clientCert, clientKey) if err != nil { panic(err) } tr := \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ RootCAs: pool, Certificates: []tls.Certificate{clientCertBytes}, InsecureSkipVerify: true, }, } client := http.Client{ Transport: tr, } r, err := client.Get(\u0026#34;https://127.0.0.1:8443\u0026#34;) // server \tif err != nil { panic(err) } defer r.Body.Close() b, err := io.ReadAll(r.Body) if err != nil { panic(err) } fmt.Println(string(b)) } ","permalink":"https://srcio.cn/series/programming-go/golang-%E5%AE%9E%E7%8E%B0%E5%8F%8C%E5%90%91%E8%AE%A4%E8%AF%81/","summary":"TLS 传输层安全协议（TLS），在互联网上，通常是由服务器单向的向客户端提供证书，以证明其身份。 mTLS 双向 TLS 认证，是指在客户端和服务器之间使用双行加","title":"Golang 实现双向认证"},{"content":"本节介绍几种构造 rest.Config 实例的场景或者方法。\nrest.Config 可以帮助我们构建各种类型的 Kubernetes 客户端实例，从而访问 Kubernetes APIServer。\n通过 kubeconfig 文件构造 程序通过读取 kubeconfig 文件来构造一个 rest.Config 对象。\npackage main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func KubeConfig() *rest.Config { config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, clientcmd.RecommendedHomeFile) if err != nil { panic(err) } return config } 通过 Secret 资源构造 通过将程序部署在 Kubernetes 集群中，使用 Pod 所配置的 ServiceAccount（默认：default）账号构造 rest.Config 对象。\n 运行的 Pod 内都会存储一个\n每个 ServiceAccount 都有一个对应的 Secret，这个 Secret 包含了对集群的操作权限。\n package main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; ) func KubeConfig() *rest.Config { config, err := rest.InClusterConfig() if err != nil { panic(err) } return config } 通过 controller-runtime 快速获取 使用 sigs.k8s.io/controller-runtime 包快速获取 rest.Config 对象。\n优先级：\n 从 --kubeconfig 指定的文件获取 从 KUBECONFIG 环境变量配置的文件获取 运行在集群中，以 In-cluster 方式获取 从 $HOME/.kube/config 文件获取   优点：灵活，对配置友好，推荐使用该种方式来获取 rest.Config 对象。\n package main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; ) func KubeConfig() *rest.Config { config, err := ctrl.GetConfig() if err != nil { panic(err) } return config } 通过文本构造 从 kubeconfig 文本获取 rest.Config 对象。 例如可以用于多集群管理平台，都过租户上传的 kubeconfig 来实例化 rest.Config 对象。\npackage main import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) var fakeConfig = `--- apiVersion: v1 clusters: - cluster: certificate-authority-data: cert-xxx server: https://127.0.0.1:6443 name: cluster-t2ktl contexts: - context: cluster: cluster-t2ktl namespace: default user: user-7k89b name: context-ctc98 current-context: context-ctc98 kind: Config preferences: {} users: - name: user-7k89b user: token: token-xxx ` func KubeConfig() *rest.Config { realConfig, _ := ioutil.ReadFile(clientcmd.RecommendedHomeFile) fakeConfig = string(realConfig) client, err := clientcmd.RESTConfigFromKubeConfig([]byte(fakeConfig)) if err != nil { panic(err) } return client } ","permalink":"https://srcio.cn/series/programming-kubernetes/rest-config/","summary":"本节介绍几种构造 rest.Config 实例的场景或者方法。 rest.Config 可以帮助我们构建各种类型的 Kubernetes 客户端实例，从而访问 Kubernetes APIServer。 通过 kubeconfig 文件构造 程序通过读取 kubeconfig 文","title":"构造 rest.Config 实例"},{"content":"安装的 Go1.18 或更新版本，它为你提供了工作区模式（Workspace mode），帮助你更好做 go 模块之间依赖的管理。\n例如，你开发一个新项目，分了两个 go module，分别为 service-a 和 service-b，service-a 依赖了service-b ，现在项目还处于开发阶段，我们都是这么处理的。\n创建项目目录：\nmkdir service cd service 创建 service-b 模块:\nmkdir service-b cd service-b go mod init github.com/srcio/service-b 编写 service-b 模块代码：\nmkdir greeting vim greeting/hello.go package greeting import \u0026#34;fmt\u0026#34; func Hello(name string){ fmt.Println(\u0026#34;Hello, \u0026#34; + name) } 继续创建 service-a 模块：\ncd .. mkdir service-a cd service-a go mod init github.com/srcio/service-a 因为 service-a 需要依赖本地开发的 service-b 类库，所以我们需要在 go.mod 中引入 service-a ：\nvim go.mod module github.com/srcio/service-a go 1.18 require( github.com/srcio/service-b v1.0.0 ) replace( github.com/srcio/service-a v1.0.0 =\u0026gt; ../service-b ) 然后，编写主函数代码：\nvim main.go package main import( \u0026#34;github.com/srcio/service-b/greeting\u0026#34; ) func main(){ greeting.Hello() } 在上述的整个过程中，你会发现，我们引用了本地的代码类库 github.com/srcio/service-b v1.0.0 =\u0026gt; ../service-b。而且，如果此时别的开发者和你一起协作开发 service-a module的时候，他是无法引用到这个类库的（除非他本地也同步了../service-b）。\n现在你就可以通过 go work 来解决这种烦恼了。\n首先，你要做的是回到module的外面，然后执行 go work init 命令：\ncd .. go work init service-a go work init service-b 此时，你会发现，目录下多了个go.work文件，查看该文件内容：\n$ cat go.work go 1.18 use( ./service-a ./service-b ) module 依赖的类库目录就在 use 块中，此时，你可以删除 service-a目录下 go.mod 中的 replace 块，然后运行 main 函数了：\ncd service-a \u0026amp;\u0026amp; go run main.go # 或者你可以直接在工作区目录运行 go run service-a/main.go 如果你的主项目依赖多个本地类库，那么你可以使用如下命令添加\ngo work use service-c go work use service-d 最后，你可以通过 go help work，了解更多。\n","permalink":"https://srcio.cn/series/programming-go/go1.18-%E5%B7%A5%E4%BD%9C%E5%8C%BA%E6%A8%A1%E5%BC%8F/","summary":"安装的 Go1.18 或更新版本，它为你提供了工作区模式（Workspace mode），帮助你更好做 go 模块之间依赖的管理。 例如，你开发一个新项目，分了两个","title":"Go1.18 - 工作区模式"},{"content":"Giscus  开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载  必要条件  你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus.app，安装的时候，分配你的博客所用的 GitHub 仓库即可。   当然，如果你的博客没有托管在 Github 上，你也可以单独创建一个 Github 仓库作为开通 giscus 评论。\n 使用姿势  在 giscus.app 做自定义配置，填入你的仓库名称，选择主题等，Giscus 会自动帮你生成 javascript 脚本； Hugo 博客目录下，创建 layouts/partials/comments.html 文件，写入获取的脚本：  \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;[在此输入仓库]\u0026#34; data-repo-id=\u0026#34;[在此输入仓库 ID]\u0026#34; data-category=\u0026#34;[在此输入分类名]\u0026#34; data-category-id=\u0026#34;[在此输入分类 ID]\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;light\u0026#34; data-lang=\u0026#34;zh-CN\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt;  ⚠️注意：为了使下面的 javascript 脚本生效，data-theme 选择 light；或者你可以根据你选择的主题修改下面的 javascript 脚本。\n 自动主题  使用一个 div 作为评论区域的容器  \u0026lt;div class=\u0026#34;giscus_comments\u0026#34;\u0026gt; {{- partial \u0026#34;comments.html\u0026#34; . }} \u0026lt;/div\u0026gt; 在该容器下方写入主题自动切换的语句  \u0026lt;script\u0026gt; document.querySelector(\u0026#34;div.giscus_comments \u0026gt; script\u0026#34;) .setAttribute( \u0026#34;data-theme\u0026#34;, localStorage.getItem(\u0026#34;pref-theme\u0026#34;) ? localStorage.getItem(\u0026#34;pref-theme\u0026#34;) : window.matchMedia(\u0026#34;(prefers-color-scheme: dark)\u0026#34;).matches ? \u0026#34;dark\u0026#34; : \u0026#34;light\u0026#34;), document.querySelector(\u0026#34;#theme-toggle\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { let e = document.querySelector(\u0026#34;iframe.giscus-frame\u0026#34;); e \u0026amp;\u0026amp; e.contentWindow.postMessage({ giscus: { setConfig: { theme: localStorage.getItem(\u0026#34;pref-theme\u0026#34;) ? localStorage.getItem(\u0026#34;pref-theme\u0026#34;) === \u0026#34;dark\u0026#34; ? \u0026#34;light\u0026#34; : \u0026#34;dark\u0026#34; : document.body.className.includes(\u0026#34;dark\u0026#34;) ? \u0026#34;light\u0026#34; : \u0026#34;dark\u0026#34; } } }, \u0026#34;https://giscus.app\u0026#34;) }) \u0026lt;/script\u0026gt; 🔗 链接  应用：https://github.com/apps/giscus 源码：https://github.com/giscus/giscus 使用：https://giscus.app  ","permalink":"https://srcio.cn/posts/use-giscus/","summary":"Giscus 开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载 必要条件 你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus","title":"使用 Giscus 作为博客评论系统"},{"content":"本节介绍 Golang 程序如何通过 rest.Config 实例获取各种类型的 Kubernetes 客户端实例。 通过客户端访问 Kubernetes 中的 API 资源实例。\nClientset  获取 *kubernetes.Clientset\n推荐使用该客户端实例去操作 K8s API 资源。  package main import ( \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func Clientset(config *rest.Config) *kubernetes.Clientset { client, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } return client } 获取 *rest.RESTClient\n可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。  package main import \u0026#34;k8s.io/client-go/rest\u0026#34; func RESTClient(config *rest.Config) *rest.RESTClient { client, err := rest.RESTClientFor(config) if err != nil { panic(err) } return client } DiscoveryClient DiscoveryClient 动态客户端，通过动态指定 GVR 来操作任意的 Kubernetes 资源（内置资源 + CR）\n 使用嵌套的 map[string]interface{} 结构存储资源数据； 可以利用反射机制序列化成为特定资源实体； 灵活性更高，但无法做强数据类型检查和验证。  使用 DiscoveryClient 获取到的资源对象为 runtime.Object，可以通过 [``]\n获取 DiscoveryClient 可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。\npackage main import ( \u0026#34;k8s.io/client-go/discovery\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func DiscoveryClient(config *rest.Config) *discovery.DiscoveryClient { client, err := discovery.NewDiscoveryClientForConfig(config) if err != nil { panic(err) } return client } 使用 DiscoveryClient DynamicClient 获取 dynamic.Interface\npackage client import ( \u0026#34;k8s.io/client-go/dynamic\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func DynamicClient(config *rest.Config) dynamic.Interface { client, err := dynamic.NewForConfig(config) if err != nil { panic(err) } return client } 获取 runtimecli.Client\n可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。 但是如果该客户端实例需要操作自定义 K8s API 资源，New 函数传入的参数 runtimecli.Options 中 Scheme 对象需要调整。  package main import ( \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; runtimecli \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; // 假设定义了一个 foo CRD，里面包含 register.go 中定义了 AddToScheme 实例 \tfoosv1 \u0026#34;pkg/apis/foos/v1\u0026#34; ) func RuntimeClient(config *rest.Config) runtimecli.Client { client, err := runtimecli.New(config, runtimecli.Options{ Scheme: scheme.Scheme, }) if err != nil { panic(err) } return client } func RuntimeClientForCRD(config *rest.Config) runtimecli.Client { crScheme := runtime.NewScheme() foosv1.AddToScheme(scheme.Scheme) client, err := runtimecli.New(config, runtimecli.Options{ Scheme: crScheme, }) if err != nil { panic(err) } return client } 获取 *http.Client\n最原生的客户端，使用该客户端实例操作 K8s API 资源就纯靠自己手工封装了，不太推荐。  package main import ( \u0026#34;net/http\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func HTTPClient(config *rest.Config) *http.Client { client, err := rest.HTTPClientFor(config) if err != nil { panic(err) } return client } 总结 http.Client =\u0026gt; rest.RESTClient =\u0026gt; discovery.DiscoveryClient =\u0026gt; kubernetes.Clientset\n客户端的定制化越高，使用越高效。但同时对自定义 API 的资源操作的支持就越低。\n","permalink":"https://srcio.cn/series/programming-kubernetes/kube-client/","summary":"本节介绍 Golang 程序如何通过 rest.Config 实例获取各种类型的 Kubernetes 客户端实例。 通过客户端访问 Kubernetes 中的 API 资源实例。 Clientset 获取 *kubernetes.Clientset 推荐使用该客户端实例去操作 K8s API 资源。 package main import (","title":"构造 Kubernetes 客户端实例"},{"content":"术语 Group\nAPI 资源置于某个分组下，组作为相关功能的集合。一个组包含一个或多个版本。\nVersion\nAPI 资源的版本，API 资源版本是会不断迭代的。\nKind\nAPI 资源的的类型，用于存储 API 资源的描述信息或状态等。同一个 Kind 的 API 资源可以有多个版本，随着版本的不断迭代，Kind 代表的资源的会有字段内容的更改。\nGVK\nGroup/Version/Kind，例如 Deployment：\napiVersion:apps/v1kind:Deploymentmetadata:- name:deploy-1... 上面的代码示例描述了一个 API 资源对象，这个资源对象：\n Group 是 apps Version 是 v1 Kind 是 Deployment。   Resource\n代表 API 资源，与 GVK 一对一的关系。\nGVR\n可以将 GVK 比作是一个类，GVR 就是这个 GVK 类的实例。\n当我们以 REST 的方式向发起 API 资源的请求是，请求 URL 格式一般类似这样：/api/apps/v1/deployments，里面就包含了三个上面提到的术语概念：\n /apps：请求资源所在的组（Group） /v1：请求资源的版本（Version） /deployments：请求的资源的名称（Resource）  ","permalink":"https://srcio.cn/series/programming-kubernetes/api-design/","summary":"术语 Group API 资源置于某个分组下，组作为相关功能的集合。一个组包含一个或多个版本。 Version API 资源的版本，API 资源版本是会不断迭代的。 Kind API 资源的的类型，","title":"Kubernetes API 设计"},{"content":"","permalink":"https://srcio.cn/series/programming-kubernetes/operator/","summary":"","title":"Kuberentes Operator"},{"content":"本页是这篇Kubernetes 文档中一些内容摘要。\n大致分为 3 个步骤：\n 为 API 类型结构做 tag 标签 在 API 类型例如 pkg/apis/${Group}/${Version}/types.go 中的 Pod 结构体上打标签，支持的标签：   // +genclient：生成客户端函数（包括 Create, Update, Delete, DeleteCollection, Get, List, Update, Patch, Watch，如果 API 类型结构中包括 .Status 字段，还会额外生成 UpdateStatus 函数）； // +genclient:nonNamespaced：指定 API 类型是集群级别而不是命名空间级别的，生成的客户端函数都没有命名空间； // +genclient:onlyVerbs=create,get：只生成 Create, Get 客户端函数； // +genclient:skipVerbs=watch：生成除了 Watch 之外的所有其他客户端函数； // +genclient:noStatus：即使 API 类型结构包含 .Status 字段，也不生成 UpdateStatus 客户端函数； 有些情况下，可能你想要额外生成非标准的客户端函数，例如子资源函数，那么你需要使用下列这些 tag 标签： // +genclient:method=Scale,verb=update,subresource=scale,input=k8s.io/api/extensions/v1beta1.Scale,result=k8s.io/api/extensions/v1beta1.Scale：该例中使用标签，将会自动生成 Scale(string, *v1beta.Scale) *v1beta.Scale 客户端函数，里面配置了子资源函数的输入和输出参数。 另外，以下的 tag 标签也影响着客户端代码的生成： // +groupName=policy.authorization.k8s.io：使用该 group 名称生成到客户端代码中（默认使用 package 名）； // +groupGoName=AuthorizationPolicy：驼峰 Golang 标识符避免带有非唯一前缀例如 policy.authorization.k8s.io 和 policy.k8s.io 的组名冲突。这将可能导致两个 Policy() 函数生成到 clientset 代码中。  如果你是参与开发 k8s.io/kubernetes 项目，你只需要执行 ./hack/update-group.sh 脚本即可生成或更新代码； 如果你开发自己的项目，你需要使用 client-gen 命令以及其命令参数：  client-gen --input=\u0026#34;pkg/apis/${group1}/${version1},pkg/apis/${group2}/${version2}\u0026#34; --clientset-name=\u0026#34;my_clientset\u0026#34;  使用 client-gen -h 查看更多命令使用姿势。\n 除了 client-gen 自动生成的客户端代码外，你可以手动添加额外的代码，参考这里。  ","permalink":"https://srcio.cn/series/programming-kubernetes/client-gen-usage/","summary":"本页是这篇Kubernetes 文档中一些内容摘要。 大致分为 3 个步骤： 为 API 类型结构做 tag 标签 在 API 类型例如 pkg/apis/${Group}/${Version}/types.go 中的 Pod 结构体上打标签，支持的标签： // +","title":"使用 client-gen 生成 clientset 代码"},{"content":" 转载自：https://baijiahao.baidu.com/s?id=1718543569967913534\n 不知道小伙伴们注意到没有，现在我们访问大多数网站的时候，他们的网站前面都会显示一个HTTPS的字样，这就代表该网站是使用的SSL证书加密的。而SSL证书是需要权威的专门机构颁发的，那么这个颁发过程又是怎样的呢？怎样颁发才能更加高效便捷、省心省力呢？今天，我们就来详细聊聊ACME协议。正是它，大大简化了我们的SSL证书管理。\n在本篇文章中，首先我们会介绍一下HTTPS的加密传输过程，以便于为理解后面的知识扫清障碍；其次会通过了解传统人工的SSL证书管理方式来理解为什么我们需要使用ACME协议；最后详细解读一下ACME协议是如何工作的。\nHTTPS的加密传输过程 当我们通过浏览器访问某一网站时，通常分为两种形式，一种是HTTP明文传输，一种是HTTPS加密传输。也就是说，用HTTP的时候我们与网站之间发送和接收的数据都是原模原样地在互联网上流通的；而用HTTPS的时候无论是我们向网站发送的数据还是网站向我们发送的数据，都是经过特殊的手段加密的，其中涉及非对称加密和对称加密。\n通俗地讲，在对称加密中，加密和解密都是用同一把钥匙；而在非对称加密中有公钥和私钥两把钥匙，并且用公钥加密的数据是不能用公钥自身解密的，只能用私钥进行解密，理解这两点很重要。当然，我们这里所说的钥匙都是以数据或文件的形式存在的。\n在一台已经配好HTTPS访问的网站服务器上会有两个至关重要的文件，一个是私钥文件，一个是SSL证书。而在这个SSL证书中就包含了由私钥文件生成的公钥。接下来，我们就来看一下这个整体过程。\n当用户通过HTTPS访问网站时，网站会向用户的浏览器发送一个SSL证书。在用户浏览器收到SSL证书后，会通过系统内置的权威颁发机构名单来核实证书的可靠、可信度。这份名单是操作系统内置的，真实性、可靠性毋庸置疑。安全！\n一旦确认可信，用户浏览器就会生成一串只有自己知道的明文A，并用SSL证书中包含的公钥通过非对称加密将其加密成密文B，并将密文B发送给网站服务器。注意，由于这里采取的是非对称加密，所以密文B在传输中即使被截获也无法解密，只能用网站服务器手里的私钥解密。安全！\n在网站服务器收到密文B后，就会用自己手上私钥文件将其解密还原成明文A。至此，用户浏览器和网站服务器手上都拿到了明文A，而且只有他们两者知道。安全！\n接下来，网站服务器和用户浏览器就会用这个统一的明文A作为密钥，对所有传输数据做对称加密和解密。安全！\n以上只是HTTPS加密传输的整体过程，其中还有很多细节，不是我们今天讨论的重点，有兴趣的小伙伴可以做进一步了解。\n传统人工的SSL证书管理方式 在了解了HTTPS加密传输以后，我们发现：这个过程的关键环节就是第一步，也就是证书的可靠性。因为如果证书都不可靠的话，以后还有什么安全可言呢。所以，网站管理者的首要工作就是向权威颁发机构申请一个SSL证书，并且在网站服务器上做好相应配置。我们来看看网站管理者需要做的工作。\n发起一个SSL证书颁发申请。在这一步，我们需要向证书颁发机构提交我们的域名，以表明我们想为哪个域名申请SSL证书。\n证明自己是这个域名的拥有者。在这一步，一旦我们向权威颁发机构证明了自己确实是这个域名的拥有者，他们就会为我们颁发一个SSL证书。\n获取证书并在网站服务器做好相应配置。首先，我们需要权威机构颁发SSL证书获取下来，然后在网站服务器上做好相应配置。其次，由于我们的SSL证书通常只在某一个时间段内有效，所以网站管理员要时刻记住在证书到期之前从新重复之前的整个证书获取过程，这是很耗时也很容易忘记的。\nACME协议是如何工作的 正是由于以上人工管理SSL证书的的诸多不变，催生了ACME协议的诞生，它使我们能够在没有任何人为干预的情况下，从证书颁发机构即时获取证书。ACME协议是由著名的Let\u0026rsquo;s Encrypt开发的，它是一个免费的证书颁发机构。ACME协议有两个关键部分，一个是ACME客户端，一个是ACME服务端。ACME服务端是一个证书颁发机构，能够自动颁发签名证书。比如，Let\u0026rsquo;s Encrypt就是一个ACME服务端。ACME客户端通常在web服务器上运行，它向ACME服务端证明web服务器确实控制了它想要获取证书的域名。一旦证明了这一点，ACME客户端就可以请求、续签和吊销证书。\n创建一个账户。ACME客户端使用ACME协议创建一个账户。首先，ACME客户端要生成一个密钥对，然后向ACME服务端发送一个帐户创建请求。这个帐户创建请求将包含生成的公钥，并将使用生成的私钥对其进行签名。因此，这个私钥实际上将用于对所有ACME客户端请求进行签名。也就是说，这是ACME服务端识别ACME客户端的方式。当然，这个帐户创建请求也可以包含联系人信息。例如，如果此帐户存在任何问题，可以使用电子邮件地址通知管理员。请注意，帐户的创建步骤只需执行一次。因此，一旦ACME客户端在ACME服务端创建了一个帐户，就可以开始执行证书管理操作了。\n验证所有权。首先，ACME客户端向服务端发送一个请求，该请求表明ACME客户端希望为其获取证书的域名。接下来，ACME服务端将向客户端提供一些验证来响应此请求，以证明web服务器确实控制着某个域名。ACME服务端可能会要求ACME客户端在特定位置提供具有特定内容的HTTP资源，或者要求它提供具有特定内容的DNS记录。ACME客户端只需要响应其中一个验证，就可以证明域名的所有权。如果确实如此，那么ACME服务端就知道web服务器确实是该域名的拥有者。\n颁发证书。一旦ACME客户端成功地证明了域名的所有权，它就会生成一个密钥对，也就是私钥和公钥，而公钥则会用于生成一个证书签名请求，并将证书签名请求发送到ACME服务端。同样，整个请求都是使用ACME客户端的帐户私钥签名的。因此，ACME服务端随后将验证签名。而且，如果一切正常，它将继续发出签名证书，然后ACME客户端可以下载该证书。\n最后，请记住签名证书仅在一定时间段内有效。当然，当需要续订证书时，我们刚才看到的请求证书的过程也可以用于续订证书。而且，整个证书管理过程实际上可以完全自动化！因此，ACME客户端可用于请求和续订证书，而整个过程都是自动的，不需要任何人为干预，这显然比传统的证书管理简单得多！\n","permalink":"https://srcio.cn/posts/acme-%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","summary":"转载自：https://baijiahao.baidu.com/s?id=1718543569967913534 不知道小伙伴们注意到没有，现","title":"ACME 工作原理"},{"content":"本页收集了一些 Golang 相关的资源，用作学习和参考，以及工作实践等。\nGo 路线图   👉🏻 GOLANG ROADMAP 站点\n  👉🏻 Go 开发者路线图\n  Awesome Go 你可以在里面找到各种 Go 框架，库以及软件等。\n  👉🏻 awesome-go\n  👉🏻 awesome-go-cn\n  ","permalink":"https://srcio.cn/series/programming-go/golang-%E8%B5%84%E6%BA%90/","summary":"本页收集了一些 Golang 相关的资源，用作学习和参考，以及工作实践等。 Go 路线图 👉🏻 GOLANG ROADMAP 站点 👉🏻 Go 开发者路线图 Awesome Go 你可以在里面找到各种 Go 框架，库以及软件","title":"Golang 资源"}]
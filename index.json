[{"content":"CRD 字段校验配置\napiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:scalings.control.srcio.iospec:group:control.srcio.ioversions:- name:v1served:truestorage:truescope:Namespacednames:plural:scalingssingular:scalingkind:Scalingvalidation:openAPIV3Schema:properties:spec:required:- targetDeployment- minReplicas- maxReplicas- metricType- step- scaleUp- scaleDownproperties:targetDeployment:type:stringminReplicas:type:integerminimum:0maxReplicas:type:integerminimum:0metricType:type:stringenum:- CPU- MEMORY- REQUESTSstep:type:integerminimum:1scaleUp:type:integerscaleDown:type:integerminimum:0  是否必须 参数类型 枚举范围 数值最大最小   ","permalink":"https://srcio.cn/series/programming-kubernetes/crd/","summary":"CRD 字段校验配置\napiVersion:apiextensions.k8s.io/v1beta1kind:CustomResourceDefinitionmetadata:name:scalings.control.srcio.iospec:group:control.srcio.ioversions:- name:v1served:truestorage:truescope:Namespacednames:plural:scalingssingular:scalingkind:Scalingvalidation:openAPIV3Schema:properties:spec:required:- targetDeployment- minReplicas- maxReplicas- metricType- step- scaleUp- scaleDownproperties:targetDeployment:type:stringminReplicas:type:integerminimum:0maxReplicas:type:integerminimum:0metricType:type:stringenum:- CPU- MEMORY- REQUESTSstep:type:integerminimum:1scaleUp:type:integerscaleDown:type:integerminimum:0  是否必须 参数类型 枚举范围 数值最大最小   ","title":"CRD 简介"},{"content":"怎么理解切片 s = append(s, item) 需要使用 s 重新接收呢？\n 在 golang 语言中所有的参数传递的方式都是值传递的，即便是指针，也是复制了一份指针传递； 切片发生扩容后，底层的数组发生了变化，不再是原来的数组结构。    ","permalink":"https://srcio.cn/series/programming-go/slice-append/","summary":"怎么理解切片 s = append(s, item) 需要使用 s 重新接收呢？\n 在 golang 语言中所有的参数传递的方式都是值传递的，即便是指针，也是复制了一份指针传递； 切片发生扩容后，底层的数组发生了变化，不再是原来的数组结构。    ","title":"Golang 切片扩容"},{"content":"安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。\n使用一下命令安装或升级 Docker Compose（linux）：\n Ubuntu，Debian：  sudo apt update sudo apt install docker-compose-plugin  基于 RPM 发行版:  sudo yum update sudo yum install docker-compose-plugin 验证安装版本：\ndocker-compose version 常用命令 运行\ndocker-compose up 查看运行\ndocker-compose ps 停止\ndocker-compose stop 启动\u0026amp;重启\ndocker-compose start docker-compose restart 退出\ndocker-compose down 使用 docker-compose -h 查看更多命令及参数。\n实践 使用 Docker Compose 运行一个简单的 golang web 程序。\n 程序初始化  mkdir docker-compose-go-demo cd docker-compose-go-demo go mod init docker-compose-go-demo 创建 main.go 文件，并写入程序代码  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func greet(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello Docker Compose! %s\u0026#34;, time.Now()) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, greet) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } 创建 Dockerfile 文件，并编写内容  FROMgolang:alpineWORKDIR/appCOPY . .EXPOSE8080ENTRYPOINT [ \u0026#34;go\u0026#34;,\u0026#34;run\u0026#34;,\u0026#34;main.go\u0026#34; ]创建 docker-comppose.yml 文件，并编写内容  version:\u0026#34;3.9\u0026#34;services:web:build:.# image: docker-compose-go-demo_web:v1# image: docker-compose-go-demo_web:v2ports:- \u0026#34;8080:8080\u0026#34;启动服务  docker-compose up -d 场景：\n web 服务业务代码修改了，希望不停机更新服务：  docker-compose up -d --build  包含多个服务，例如中间件，但只想重新编译其中业务服务，如 web：  docker-compose up -d --no-deps --build web  如果 docker-compose.yml 直接使用的镜像，那么直接更新，再次 docker-compose up -d 即可。\n ","permalink":"https://srcio.cn/posts/docker-compose/","summary":"安装 如果你安装了 Docker Desktop，那么它已经帮你自动安装了 Docker Compose 插件。否则，需要额外安装插件。\n使用一下命令安装或升级 Docker Compose（linux）：\n Ubuntu，Debian：  sudo apt update sudo apt install docker-compose-plugin  基于 RPM 发行版:  sudo yum update sudo yum install docker-compose-plugin 验证安装版本：\ndocker-compose version 常用命令 运行\ndocker-compose up 查看运行\ndocker-compose ps 停止\ndocker-compose stop 启动\u0026amp;重启\ndocker-compose start docker-compose restart 退出\ndocker-compose down 使用 docker-compose -h 查看更多命令及参数。\n实践 使用 Docker Compose 运行一个简单的 golang web 程序。\n 程序初始化  mkdir docker-compose-go-demo cd docker-compose-go-demo go mod init docker-compose-go-demo 创建 main.","title":"Docker Compose 实践"},{"content":" 文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html\n Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。\n镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。\n主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。\n需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。\n用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：\nimage-gc-high-threshold`：磁盘使用率上限，有效范围 [0-100]，默认 `85 image-gc-low-threshold`：磁盘使用率下限，有效范围 [0-100]，默认 `80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟\n实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。\n实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。\n# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ... ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --image-gc-high-threshold=2 --image-gc-low-threshold=1 ... 我们在 Kubelet 启动参数的最后追加了 --image-gc-high-threshold=2 --image-gc-low-threshold=1，这么低的配置，Kubelet 应该会一直忙于进行镜像回收了，生产环境可不能这么配置！\n执行以下命令使得配置生效：\n# systemctl daemon-reload # systemctl restart kubelet 首先，看下本地都有哪些镜像：\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 接下来，我们运行一个 nginx 程序，让 Kubelet 自动拉取镜像。\nroot@shida-machine:~# kubectl run nginx --image=nginx deployment.apps/nginx created root@shida-machine:~# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 62s root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，nginx 镜像已经被自动 pull 到本地了，ID 为 f68d6e55e065。\n然后，删除 nginx Deployment：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \u0026quot;nginx\u0026quot; deleted 过大概 5 分钟后，再次检查本地镜像列表，发现 nginx 镜像已被清理！\nroot@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 通过以下命令查看镜像垃圾回收日志：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:03:20.883489 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72470076620 bytes down to the low threshold (1%). I0714 18:03:20.899370 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes 可以看到，日志中记录的删除镜像 ID 与 nginx 镜像的 ID 是一致的（均为 f68d6e55e065）。\n继续验证用户手动拉取的镜像是否会被清理，手动运行 nginx 程序：\nroot@shida-machine:~# docker run --name nginx -d nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx fc7181108d40: Pull complete d2e987ca2267: Pull complete 0b760b431b11: Pull complete Digest: sha256:48cbeee0cb0a3b5e885e36222f969e0a2f41819a68e07aeb6631ca7cb356fed1 Status: Downloaded newer image for nginx:latest 2fc8a836ba3c7cbd488c7fd4f2ffa7287b709abf1b7701685291c3b1e5df3472 通过查看镜像 GC 日志，会发现 GC 会尝试清理用户自己手动拉取的 nginx 镜像，但因为该镜像被使用中，所以这次删除操作不会成功：\nroot@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:28:23.015586 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72501525708 bytes down to the low threshold (1%). I0714 18:28:23.306696 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 将该容器停止，继续观察回收动作：\nroot@shida-machine:~# docker stop nginx nginx root@shida-machine:~# journalctl -u kubelet -o cat | grep imageGCManager ... I0714 18:53:23.579629 51179 image_gc_manager.go:300] [imageGCManager]: Disk usage on image filesystem is at 24% which is over the high threshold (2%). Trying to free 72549280972 bytes down to the low threshold (1%). I0714 18:53:23.629492 51179 image_gc_manager.go:371] [imageGCManager]: Removing image \u0026quot;sha256:f68d6e55e06520f152403e6d96d0de5c9790a89b4cfc99f4626f68146fa1dbdc\u0026quot; to free 109357355 bytes root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB nginx latest f68d6e55e065 12 days ago 109MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 可以看到，对于已经停止的容器，Kubelet 也是会尝试删除，但删除操作依然不会成功（存在死亡容器对该镜像的引用）。\n彻底删除 nginx 容器，此时就没有任何容器继续使用该镜像，经过 1 次 GC 后，nginx 镜像就会被清理。\nroot@shida-machine:~# docker rm nginx nginx root@shida-machine:~# docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/kube-proxy v1.14.4 5f2081c22306 6 days ago 82.1MB k8s.gcr.io/kube-apiserver v1.14.4 f3171d49fa9b 6 days ago 210MB k8s.gcr.io/kube-controller-manager v1.14.4 35f0904dc8fa 6 days ago 158MB k8s.gcr.io/kube-scheduler v1.14.4 ee080c083e45 6 days ago 81.6MB calico/node v3.7.3 bf4ff15c9db0 4 weeks ago 156MB calico/cni v3.7.3 1a6ade52d471 4 weeks ago 135MB calico/kube-controllers v3.7.3 283860d96794 4 weeks ago 46.8MB k8s.gcr.io/coredns 1.3.1 eb516548c180 6 months ago 40.3MB k8s.gcr.io/etcd 3.3.10 2c4adeb21b4f 7 months ago 258MB k8s.gcr.io/pause 3.1 da86e6ba6ca1 19 months ago 742kB 容器回收 了解了镜像回收的基本原理，我们再来看看容器回收。容器在停止运行（比如出错退出或者正常结束）后会残留一系列的垃圾文件，一方面会占据磁盘空间，另一方面也会影响系统运行速度。此时，就需要 Kubelet 容器回收了。要特别注意的是，Kubelet 回收的容器是指那些由其管理的的容器（也就是 Pod 容器），用户手动运行的容器不会被 Kubelet 进行垃圾回收。\n与容器垃圾回收相关的控制参数主要有 3 个：\nMinAge：容器可以被执行垃圾回收的最小年龄\nMaxPerPodContainer：每个 pod 内允许存在的死亡容器的最大数量\nMaxContainers：节点上全部死亡容器的最大数量\n 注意：当 MaxPerPodContainer 与 MaxContainers 发生冲突时，Kubelet 会自动调整 MaxPerPodContainer 的取值以满足 MaxContainers 要求。\n 主体流程   容器回收主要针对三个目标资源：普通容器、sandbox 容器以及容器日志目录。\n对于普通容器，主要根据 MaxPerPodContainer 与 MaxContainers 的设置，按照 LRU 策略，从 Pod 的死亡容器列表删除一定数量的容器，直到满足配置需求；对于 sandbox 容器，按照每个 Pod 保留一个的原则清理多余的死亡 sandbox；对于日志目录，只要没有 Pod 与之关联了就将其删除。\nKubelet 的容器垃圾回收只针对 Pod 容器，非 Kubelet Pod 容器（比如通过 docker run 启动的容器）不会被主动清理。\n用户配置 影响容器垃圾回收的关键参数有：\nminimum-container-ttl-duration：容器可被回收的最小生存年龄，默认是 0 分钟，这意味着每个死亡容器都会被立即执行垃圾回收\nmaximum-dead-containers-per-container`：每个 Pod 要保留的死亡容器的最大数量，默认值为 `1 maximum-dead-containers：节点可保留的死亡容器的最大数量，默认值是 -1，这意味着节点没有限制死亡容器数量\n实验环节 还是以 nginx 为例，创建一个 nginx 服务：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 16 seconds ago Up 14 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 2 minutes ago Up 2 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到，Kubelet 启动了一个 sandbox 以及一个 nginx 实例。\n手动杀死 nginx 实例，模拟容器异常退出：\nroot@shida-machine:~# docker kill 7bef0308d9ea 7bef0308d9ea root@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 3 seconds ago Up 2 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 2 minutes ago Exited (137) 15 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 5 minutes ago Up 5 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 可以看到 Kubelet 重新拉起了一个新的 nginx 实例。\n等待几分钟，发现 Kubelet 并未清理异常退出的 nginx 容器（因为此时仅有一个 dead container）。\nroot@shida-machine:~# docker ps -a | grep nginx 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 3 minutes ago Up 3 minutes k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7bef0308d9ea nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 5 minutes ago Exited (137) 3 minutes ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 8 minutes ago Up 8 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 继续杀死当前 nginx 实例：\nroot@shida-machine:~# docker kill 408b23b2b72a 408b23b2b72a root@shida-machine:~# docker ps -a | grep nginx e064e376819f nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 9 seconds ago Up 7 seconds k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_2 408b23b2b72a nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 5 minutes ago Exited (137) 40 seconds ago k8s_nginx_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_1 7e65e0db52c2 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 10 minutes ago Up 10 minutes k8s_POD_nginx-7db9fccd9b-p2p2t_default_69c38c2b-a64e-11e9-94bd-000c29ce064a_0 这下看到效果了，仍然只有一个退出的容器被保留，而且被清理掉的是最老的死亡容器，这与之前的分析是一致的！\n删除这个 nginx Deployment，会发现所有的 nginx 容器都会被清理：\nroot@shida-machine:~# kubectl delete deployment nginx deployment.extensions \u0026quot;nginx\u0026quot; deleted root@shida-machine:~# docker ps -a | grep nginx root@shida-machine:~# 进一步，我们修改 Kubelet 参数，设置 maximum-dead-containers 为 0，这就告诉 Kubelet 清理所有死亡容器。\n重复前边的实验步骤：\nroot@shida-machine:~# kubectl run nginx --image nginx deployment.apps/nginx created root@shida-machine:~# docker ps -a | grep nginx 8de9ae8e2c9b nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 33 seconds ago Up 32 seconds k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 41 seconds ago Up 38 seconds k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 root@shida-machine:~# docker kill 8de9ae8e2c9b 8de9ae8e2c9b root@shida-machine:~# docker ps -a | grep nginx 95ee5bd2cab2 nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Up About a minute k8s_nginx_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_1 d2cdfafdbe50 k8s.gcr.io/pause:3.1 \u0026quot;/pause\u0026quot; 2 minutes ago Up About a minute k8s_POD_nginx-7db9fccd9b-jl2xn_default_0cd67a29-a6a2-11e9-94bd-000c29ce064a_0 结果显示，nginx Pod 的所有死亡容器都会被清理，因为我们已经强制要求节点不保留任何死亡容器，与预期一致！\n那对于手动运行的容器呢？我们通过 docker run 运行 nginx：\nroot@shida-machine:~# docker run --name nginx -d nginx 46ebb365f6be060a6950f44728e4f11e4666bf2fb007cad557ffc65ecf8aded8 root@shida-machine:~# docker ps | grep nginx 46ebb365f6be nginx \u0026quot;nginx -g 'daemon of…\u0026quot; 9 seconds ago Up 6 seconds 80/tcp nginx 杀死该容器：\nroot@shida-machine:~# docker kill 46ebb365f6be 46ebb365f6be root@shida-machine:~# docker ps -a | grep nginx 46ebb365f6be nginx \u0026quot;nginx -g 'daemon of…\u0026quot; About a minute ago Exited (137) 18 seconds ago nginx 经过几分钟，我们发现该死亡容器还是会存在的，Kubelet 不会清理这类容器！\n小结 Kubelet 每 5 分钟进行一次镜像清理。当磁盘使用率超过上限阈值，Kubelet 会按照 LRU 策略逐一清理没有被任何容器所使用的镜像，直到磁盘使用率降到下限阈值或没有空闲镜像可以清理。Kubelet 认为镜像可被清理的标准是未被任何 Pod 容器（包括那些死亡了的容器）所引用，那些非 Pod 容器（如用户通过 docker run 启动的容器）是不会被用来计算镜像引用关系的。也就是说，即便用户运行的容器使用了 A 镜像，只要没有任何 Pod 容器使用到 A，那 A 镜像对于 Kubelet 而言就是可被回收的。但是我们无需担心手动运行容器使用的镜像会被意外回收，因为 Kubelet 的镜像删除是非 force 类型的，底层容器运行时会使存在容器关联的镜像删除操作失败（因为 Docker 会认为仍有容器使用着 A 镜像）。\nKubelet 每 1 分钟执行一次容器清理。根据启动配置参数，Kubelet 会按照 LRU 策略依次清理每个 Pod 内的死亡容器，直到达到死亡容器限制数要求，对于 sandbox 容器，Kubelet 仅会保留最新的（这不受 GC 策略的控制）。对于日志目录，只要已经没有 Pod 继续占用，就将其清理。对于非 Pod 容器（如用户通过 docker run 启动的容器）不会被 Kubelet 垃圾回收。\n","permalink":"https://srcio.cn/posts/kubelet-recycle-policy/","summary":"文章转载自：https://sataqiu.github.io/2019/07/15/k8s-kubelet-gc/index.html\n Kubelet 垃圾回收（Garbage Collection）是一个非常有用的功能，它负责自动清理节点上的无用镜像和容器。Kubelet 每隔 1 分钟进行一次容器清理，每隔 5 分钟进行一次镜像清理（截止到 v1.15 版本，垃圾回收间隔时间还都是在源码中固化的，不可自定义配置）。如果节点上已经运行了 Kubelet，不建议再额外运行其它的垃圾回收工具，因为这些工具可能错误地清理掉 Kubelet 认为本应保留的镜像或容器，从而可能造成不可预知的问题。\n镜像回收 Kubernetes 对节点上的所有镜像提供生命周期管理服务，这里的『所有镜像』是真正意义上的所有镜像，而不仅仅是通过 Kubelet 拉取的镜像。当磁盘使用率超过设定上限（HighThresholdPercent）时，Kubelet 就会按照 LRU 清除策略逐个清理掉那些没有被任何 Pod 容器（包括那些已经死亡的容器）所使用的镜像，直到磁盘使用率降到设定下限（LowThresholdPercent）或没有空闲镜像可以清理。此外，在进行镜像清理时，会考虑镜像的生存年龄，对于年龄没有达到最短生存年龄（MinAge）要求的镜像，暂不予以清理。\n主体流程   如上图所示，Kubelet 对于节点上镜像的回收流程还是比较简单的，在磁盘使用率超出设定上限后：首先，通过 CRI 容器运行时接口读取节点上的所有镜像以及 Pod 容器；然后，根据现有容器列表过滤出那些已经不被任何容器所使用的镜像；接着，按照镜像最近被使用时间排序，越久被用到的镜像越会被排在前面，优先清理；最后，就按照排好的顺序逐个清理镜像，直到磁盘使用率降到设定下限（或者已经没有空闲镜像可以清理）。\n需要注意的是，Kubelet 读取到的镜像列表是节点镜像列表，而读取到的容器列表却仅包括由其管理的容器（即 Pod 容器，包括 Pod 内的死亡容器）。因此，那些用户手动 run 起来的容器，对于 Kubelet 垃圾回收来说就是不可见的，也就不能阻止对相关镜像的垃圾回收。当然，Kubelet 的镜像回收不是 force 类型的回收，虽然会对用户手动下载的镜像进行回收动作，但如果确实有运行的（或者停止的任何）容器与该镜像关联的话，删除操作就会失败（被底层容器运行时阻止删除）。\n用户配置 通过上面的分析，我们知道影响镜像垃圾回收的关键参数有：\nimage-gc-high-threshold`：磁盘使用率上限，有效范围 [0-100]，默认 `85 image-gc-low-threshold`：磁盘使用率下限，有效范围 [0-100]，默认 `80 minimum-image-ttl-duration：镜像最短应该生存的年龄，默认 2 分钟\n实验环节 本节我们通过实验来验证镜像垃圾回收（基于 Kubelet 1.15 版本）。\n实验前，需要配置 Kubelet 启动参数，降低磁盘使用率上限，以便能够直接触发镜像回收。\n# vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf .","title":"Kubelet 垃圾回收原理剖析"},{"content":"代码实现 package certutil import ( \u0026#34;bytes\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;time\u0026#34; ) // CA ca type CA struct { caInfo *x509.Certificate caPrivKey *rsa.PrivateKey caPem, caKeyPem []byte } // GetCAPem get ca pem bytes func (c *CA) GetCAPem() ([]byte, error) { if c.caPem == nil { // create the CA \tcaBytes, err := x509.CreateCertificate(rand.Reader, c.caInfo, c.caInfo, \u0026amp;c.caPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, err } // pem encode \tcaPEM := new(bytes.Buffer) _ = pem.Encode(caPEM, \u0026amp;pem.Block{ Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: caBytes, }) c.caPem = caPEM.Bytes() } return c.caPem, nil } // GetCAKeyPem get ca key pem func (c *CA) GetCAKeyPem() ([]byte, error) { if c.caKeyPem == nil { caPrivKeyPEM := new(bytes.Buffer) _ = pem.Encode(caPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026#34;RSA PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(c.caPrivKey), }) c.caKeyPem = caPrivKeyPEM.Bytes() } return c.caKeyPem, nil } // CreateCert make Certificate func (c *CA) CreateCert(ips []string, domains ...string) (certPem, certKey []byte, err error) { var ipAddresses []net.IP for _, ip := range ips { if i := net.ParseIP(ip); i != nil { ipAddresses = append(ipAddresses, i) } } // set up our server certificate \tcert := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(2019), Subject: pkix.Name{ Organization: []string{\u0026#34;srcio.cn\u0026#34;}, Country: []string{\u0026#34;CN\u0026#34;}, Province: []string{\u0026#34;Beijing\u0026#34;}, Locality: []string{\u0026#34;Beijing\u0026#34;}, StreetAddress: []string{\u0026#34;Beijing\u0026#34;}, PostalCode: []string{\u0026#34;000000\u0026#34;}, }, DNSNames: domains, IPAddresses: ipAddresses, NotBefore: time.Now(), NotAfter: time.Now().AddDate(99, 0, 0), SubjectKeyId: []byte{1, 2, 3, 4, 6}, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature, } certPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return nil, nil, err } certBytes, err := x509.CreateCertificate(rand.Reader, cert, c.caInfo, \u0026amp;certPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, nil, err } certPEM := new(bytes.Buffer) _ = pem.Encode(certPEM, \u0026amp;pem.Block{ Type: \u0026#34;CERTIFICATE\u0026#34;, Bytes: certBytes, }) certPrivKeyPEM := new(bytes.Buffer) _ = pem.Encode(certPrivKeyPEM, \u0026amp;pem.Block{ Type: \u0026#34;RSA PRIVATE KEY\u0026#34;, Bytes: x509.MarshalPKCS1PrivateKey(certPrivKey), }) return certPEM.Bytes(), certPrivKeyPEM.Bytes(), nil } // CreateCA create ca info func CreateCA() (*CA, error) { // set up our CA certificate \tca := \u0026amp;x509.Certificate{ SerialNumber: big.NewInt(2019), Subject: pkix.Name{ Organization: []string{\u0026#34;srcio.cn\u0026#34;}, Country: []string{\u0026#34;CN\u0026#34;}, Province: []string{\u0026#34;Beijing\u0026#34;}, Locality: []string{\u0026#34;Beijing\u0026#34;}, StreetAddress: []string{\u0026#34;Beijing\u0026#34;}, PostalCode: []string{\u0026#34;000000\u0026#34;}, }, NotBefore: time.Now(), NotAfter: time.Now().AddDate(99, 0, 0), IsCA: true, ExtKeyUsage: []x509.ExtKeyUsage{x509.ExtKeyUsageClientAuth, x509.ExtKeyUsageServerAuth}, KeyUsage: x509.KeyUsageDigitalSignature | x509.KeyUsageCertSign, BasicConstraintsValid: true, } // create our private and public key \tcaPrivKey, err := rsa.GenerateKey(rand.Reader, 4096) if err != nil { return nil, err } return \u0026amp;CA{ caInfo: ca, caPrivKey: caPrivKey, }, nil } // ParseCA parse caPem func ParseCA(caPem, caKeyPem []byte) (*CA, error) { p := \u0026amp;pem.Block{} p, caPem = pem.Decode(caPem) ca, err := x509.ParseCertificate(p.Bytes) if err != nil { return nil, err } p2 := \u0026amp;pem.Block{} p2, caKeyPem = pem.Decode(caKeyPem) caKey, err := x509.ParsePKCS1PrivateKey(p2.Bytes) if err != nil { return nil, err } return \u0026amp;CA{ caInfo: ca, caPrivKey: caKey, caPem: caPem, caKeyPem: caKeyPem, }, nil } // DomainSign create cert func DomainSign(ips []string, domains ...string) ([]byte, []byte, []byte, error) { ca, err := CreateCA() if err != nil { return nil, nil, nil, err } caPem, err := ca.GetCAPem() if err != nil { return nil, nil, nil, err } certPem, certKey, err := ca.CreateCert(ips, domains...) if err != nil { return nil, nil, nil, err } return caPem, certPem, certKey, nil } ","permalink":"https://srcio.cn/series/programming-go/gen-cert/","summary":"代码实现 package certutil import ( \u0026#34;bytes\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;crypto/rsa\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;crypto/x509/pkix\u0026#34; \u0026#34;encoding/pem\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;net\u0026#34; \u0026#34;time\u0026#34; ) // CA ca type CA struct { caInfo *x509.Certificate caPrivKey *rsa.PrivateKey caPem, caKeyPem []byte } // GetCAPem get ca pem bytes func (c *CA) GetCAPem() ([]byte, error) { if c.caPem == nil { // create the CA \tcaBytes, err := x509.CreateCertificate(rand.Reader, c.caInfo, c.caInfo, \u0026amp;c.caPrivKey.PublicKey, c.caPrivKey) if err != nil { return nil, err } // pem encode \tcaPEM := new(bytes.","title":"Golang 生成证书"},{"content":"TLS 传输层安全协议（TLS），在互联网上，通常是由服务器单向的向客户端提供证书，以证明其身份。\nmTLS 双向 TLS 认证，是指在客户端和服务器之间使用双行加密通道，mTLS 是云原生应用中常用的通信安全协议。\n使用双向TLS连接的主要目的是当服务器应该只接受来自有限的允许的客户端的 TLS 连接时。例如，一个组织希望将服务器的 TLS 连接限制为只来自该组织的合法合作伙伴或客户。显然，为客户端添加IP白名单不是一个好的安全实践，因为IP可能被欺骗。\n为了简化 mTLS 握手的过程，我们这样简单梳理：\n  客户端发送访问服务器上受保护信息的请求；\n  服务器向客户端提供公钥证书；\n  客户端通过使用 CA 的公钥来验证服务器公钥证书的数字签名，以验证服务器的证书；\n  如果步骤 3 成功，客户机将其客户端公钥证书发送到服务器；\n  服务器使用步骤 3 中相同的方法验证客户机的证书；\n  如果成功，服务器将对受保护信息的访问权授予客户机。\n  代码实现 需要实现客户端验证服务端的公钥证书，服务端验证客户端的公钥证书。\n生成证书 echo \u0026#39;清理并生成目录\u0026#39; OUT=./certs DAYS=365 RSALEN=2048 CN=srcio rm -rf ${OUT}/* mkdir ${OUT} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 cd ${OUT} echo \u0026#39;生成CA的私钥\u0026#39; openssl genrsa -out ca.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成CA的签名证书\u0026#39; openssl req -new \\ -x509 \\ -key ca.key \\ -subj \u0026#34;/CN=${CN}\u0026#34; \\ -out ca.crt echo \u0026#39;\u0026#39; echo \u0026#39;生成server端私钥\u0026#39; openssl genrsa -out server.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成server端自签名\u0026#39; openssl req -new \\ -key server.key \\ -subj \u0026#34;/CN=${CN}\u0026#34; \\ -out server.csr echo \u0026#39;签发server端证书\u0026#39; openssl x509 -req -sha256 \\ -in server.csr \\ -CA ca.crt -CAkey ca.key -CAcreateserial \\ -out server.crt -text \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;删除server端自签名证书\u0026#39; rm server.csr echo \u0026#39;\u0026#39; echo \u0026#39;生成client私钥\u0026#39; openssl genrsa -out client.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成client自签名\u0026#39; openssl req -new \\  -subj \u0026#34;/CN=${CN}\u0026#34; \\  -key client.key \\  -out client.csr echo \u0026#39;签发client证书\u0026#39; openssl x509 -req -sha256\\  -CA ca.crt -CAkey ca.key -CAcreateserial\\  -days ${DAYS}\\  -in client.csr\\  -out client.crt\\  -text \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;删除client端自签名\u0026#39; rm client.csr echo \u0026#39;\u0026#39; echo \u0026#39;删除临时文件\u0026#39; rm ca.srl echo \u0026#39;\u0026#39; echo \u0026#39;完成\u0026#39;% 服务端 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) var ( caCert = \u0026#34;../../certs/ca.crt\u0026#34; serverCert = \u0026#34;../../certs/server.crt\u0026#34; serverKey = \u0026#34;../../certs/server.key\u0026#34; ) type mtlsHandler struct { } func (m *mtlsHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026#34;Hello World! \u0026#34;, time.Now()) } func main() { pool := x509.NewCertPool() caCertBytes, err := os.ReadFile(caCert) if err != nil { panic(err) } pool.AppendCertsFromPEM(caCertBytes) server := \u0026amp;http.Server{ Addr: \u0026#34;:8443\u0026#34;, Handler: \u0026amp;mtlsHandler{}, TLSConfig: \u0026amp;tls.Config{ ClientCAs: pool, ClientAuth: tls.RequireAndVerifyClientCert, // 需要客户端证书 \t}, } log.Println(\u0026#34;server started...\u0026#34;) log.Fatalln(server.ListenAndServeTLS(serverCert, serverKey)) } 客户端 package main import ( \u0026#34;crypto/tls\u0026#34; \u0026#34;crypto/x509\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) var ( caCert = \u0026#34;../../certs/ca.crt\u0026#34; clientCert = \u0026#34;../../certs/client.crt\u0026#34; clientKey = \u0026#34;../../certs/client.key\u0026#34; ) func main() { pool := x509.NewCertPool() caCertBytes, err := os.ReadFile(caCert) if err != nil { panic(err) } pool.AppendCertsFromPEM(caCertBytes) clientCertBytes, err := tls.LoadX509KeyPair(clientCert, clientKey) if err != nil { panic(err) } tr := \u0026amp;http.Transport{ TLSClientConfig: \u0026amp;tls.Config{ RootCAs: pool, Certificates: []tls.Certificate{clientCertBytes}, InsecureSkipVerify: true, }, } client := http.Client{ Transport: tr, } r, err := client.Get(\u0026#34;https://127.0.0.1:8443\u0026#34;) // server \tif err != nil { panic(err) } defer r.Body.Close() b, err := io.ReadAll(r.Body) if err != nil { panic(err) } fmt.Println(string(b)) } ","permalink":"https://srcio.cn/series/programming-go/mtls/","summary":"TLS 传输层安全协议（TLS），在互联网上，通常是由服务器单向的向客户端提供证书，以证明其身份。\nmTLS 双向 TLS 认证，是指在客户端和服务器之间使用双行加密通道，mTLS 是云原生应用中常用的通信安全协议。\n使用双向TLS连接的主要目的是当服务器应该只接受来自有限的允许的客户端的 TLS 连接时。例如，一个组织希望将服务器的 TLS 连接限制为只来自该组织的合法合作伙伴或客户。显然，为客户端添加IP白名单不是一个好的安全实践，因为IP可能被欺骗。\n为了简化 mTLS 握手的过程，我们这样简单梳理：\n  客户端发送访问服务器上受保护信息的请求；\n  服务器向客户端提供公钥证书；\n  客户端通过使用 CA 的公钥来验证服务器公钥证书的数字签名，以验证服务器的证书；\n  如果步骤 3 成功，客户机将其客户端公钥证书发送到服务器；\n  服务器使用步骤 3 中相同的方法验证客户机的证书；\n  如果成功，服务器将对受保护信息的访问权授予客户机。\n  代码实现 需要实现客户端验证服务端的公钥证书，服务端验证客户端的公钥证书。\n生成证书 echo \u0026#39;清理并生成目录\u0026#39; OUT=./certs DAYS=365 RSALEN=2048 CN=srcio rm -rf ${OUT}/* mkdir ${OUT} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 cd ${OUT} echo \u0026#39;生成CA的私钥\u0026#39; openssl genrsa -out ca.key ${RSALEN} \u0026gt;\u0026gt; /dev/null 2\u0026gt;\u0026amp;1 echo \u0026#39;生成CA的签名证书\u0026#39; openssl req -new \\ -x509 \\ -key ca.","title":"Golang 实现双向认证"},{"content":"本节介绍几种构造 rest.Config 实例的场景或者方法。\nrest.Config 可以帮助我们构建各种类型的 Kubernetes 客户端实例，从而访问 Kubernetes APIServer。\n通过 kubeconfig 文件构造 程序通过读取 kubeconfig 文件来构造一个 rest.Config 对象。\npackage main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func KubeConfig() *rest.Config { config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, clientcmd.RecommendedHomeFile) if err != nil { panic(err) } return config } 通过 Secret 资源构造 通过将程序部署在 Kubernetes 集群中，使用 Pod 所配置的 ServiceAccount（默认：default）账号构造 rest.Config 对象。\n 运行的 Pod 内都会存储一个\n每个 ServiceAccount 都有一个对应的 Secret，这个 Secret 包含了对集群的操作权限。\n package main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; ) func KubeConfig() *rest.Config { config, err := rest.InClusterConfig() if err != nil { panic(err) } return config } 通过 controller-runtime 快速获取 使用 sigs.k8s.io/controller-runtime 包快速获取 rest.Config 对象。\n优先级：\n 从 --kubeconfig 指定的文件获取 从 KUBECONFIG 环境变量配置的文件获取 运行在集群中，以 In-cluster 方式获取 从 $HOME/.kube/config 文件获取   优点：灵活，对配置友好，推荐使用该种方式来获取 rest.Config 对象。\n package main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; ctrl \u0026#34;sigs.k8s.io/controller-runtime\u0026#34; ) func KubeConfig() *rest.Config { config, err := ctrl.GetConfig() if err != nil { panic(err) } return config } 通过文本构造 从 kubeconfig 文本获取 rest.Config 对象。 例如可以用于多集群管理平台，都过租户上传的 kubeconfig 来实例化 rest.Config 对象。\npackage main import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) var fakeConfig = `--- apiVersion: v1 clusters: - cluster: certificate-authority-data: cert-xxx server: https://127.0.0.1:6443 name: cluster-t2ktl contexts: - context: cluster: cluster-t2ktl namespace: default user: user-7k89b name: context-ctc98 current-context: context-ctc98 kind: Config preferences: {} users: - name: user-7k89b user: token: token-xxx ` func KubeConfig() *rest.Config { realConfig, _ := ioutil.ReadFile(clientcmd.RecommendedHomeFile) fakeConfig = string(realConfig) client, err := clientcmd.RESTConfigFromKubeConfig([]byte(fakeConfig)) if err != nil { panic(err) } return client } ","permalink":"https://srcio.cn/series/programming-kubernetes/rest-config/","summary":"本节介绍几种构造 rest.Config 实例的场景或者方法。\nrest.Config 可以帮助我们构建各种类型的 Kubernetes 客户端实例，从而访问 Kubernetes APIServer。\n通过 kubeconfig 文件构造 程序通过读取 kubeconfig 文件来构造一个 rest.Config 对象。\npackage main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) func KubeConfig() *rest.Config { config, err := clientcmd.BuildConfigFromFlags(\u0026#34;\u0026#34;, clientcmd.RecommendedHomeFile) if err != nil { panic(err) } return config } 通过 Secret 资源构造 通过将程序部署在 Kubernetes 集群中，使用 Pod 所配置的 ServiceAccount（默认：default）账号构造 rest.Config 对象。\n 运行的 Pod 内都会存储一个\n每个 ServiceAccount 都有一个对应的 Secret，这个 Secret 包含了对集群的操作权限。\n package main import ( \u0026#34;k8s.io/client-go/rest\u0026#34; ) func KubeConfig() *rest.","title":"构造 rest.Config 实例"},{"content":"安装的 Go1.18 或更新版本，它为你提供了工作区模式（Workspace mode），帮助你更好做 go 模块之间依赖的管理。\n例如，你开发一个新项目，分了两个 go module，分别为 service-a 和 service-b，service-a 依赖了service-b ，现在项目还处于开发阶段，我们都是这么处理的。\n创建项目目录：\nmkdir service cd service 创建 service-b 模块:\nmkdir service-b cd service-b go mod init github.com/srcio/service-b 编写 service-b 模块代码：\nmkdir greeting vim greeting/hello.go package greeting import \u0026#34;fmt\u0026#34; func Hello(name string){ fmt.Println(\u0026#34;Hello, \u0026#34; + name) } 继续创建 service-a 模块：\ncd .. mkdir service-a cd service-a go mod init github.com/srcio/service-a 因为 service-a 需要依赖本地开发的 service-b 类库，所以我们需要在 go.mod 中引入 service-a ：\nvim go.mod module github.com/srcio/service-a go 1.18 require( github.com/srcio/service-b v1.0.0 ) replace( github.com/srcio/service-a v1.0.0 =\u0026gt; ../service-b ) 然后，编写主函数代码：\nvim main.go package main import( \u0026#34;github.com/srcio/service-b/greeting\u0026#34; ) func main(){ greeting.Hello() } 在上述的整个过程中，你会发现，我们引用了本地的代码类库 github.com/srcio/service-b v1.0.0 =\u0026gt; ../service-b。而且，如果此时别的开发者和你一起协作开发 service-a module的时候，他是无法引用到这个类库的（除非他本地也同步了../service-b）。\n现在你就可以通过 go work 来解决这种烦恼了。\n首先，你要做的是回到module的外面，然后执行 go work init 命令：\ncd .. go work init service-a go work init service-b 此时，你会发现，目录下多了个go.work文件，查看该文件内容：\n$ cat go.work go 1.18 use( ./service-a ./service-b ) module 依赖的类库目录就在 use 块中，此时，你可以删除 service-a目录下 go.mod 中的 replace 块，然后运行 main 函数了：\ncd service-a \u0026amp;\u0026amp; go run main.go # 或者你可以直接在工作区目录运行 go run service-a/main.go 如果你的主项目依赖多个本地类库，那么你可以使用如下命令添加\ngo work use service-c go work use service-d 最后，你可以通过 go help work，了解更多。\n","permalink":"https://srcio.cn/series/programming-go/go-work/","summary":"安装的 Go1.18 或更新版本，它为你提供了工作区模式（Workspace mode），帮助你更好做 go 模块之间依赖的管理。\n例如，你开发一个新项目，分了两个 go module，分别为 service-a 和 service-b，service-a 依赖了service-b ，现在项目还处于开发阶段，我们都是这么处理的。\n创建项目目录：\nmkdir service cd service 创建 service-b 模块:\nmkdir service-b cd service-b go mod init github.com/srcio/service-b 编写 service-b 模块代码：\nmkdir greeting vim greeting/hello.go package greeting import \u0026#34;fmt\u0026#34; func Hello(name string){ fmt.Println(\u0026#34;Hello, \u0026#34; + name) } 继续创建 service-a 模块：\ncd .. mkdir service-a cd service-a go mod init github.com/srcio/service-a 因为 service-a 需要依赖本地开发的 service-b 类库，所以我们需要在 go.mod 中引入 service-a ：","title":"Go1.18 - 工作区模式"},{"content":"本节介绍 Golang 程序如何通过 rest.Config 实例获取各种类型的 Kubernetes 客户端实例。 通过客户端访问 Kubernetes 中的 API 资源实例。\nClientset  获取 *kubernetes.Clientset\n推荐使用该客户端实例去操作 K8s API 资源。  package main import ( \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func Clientset(config *rest.Config) *kubernetes.Clientset { client, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } return client } 获取 *rest.RESTClient\n可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。  package main import \u0026#34;k8s.io/client-go/rest\u0026#34; func RESTClient(config *rest.Config) *rest.RESTClient { client, err := rest.RESTClientFor(config) if err != nil { panic(err) } return client } DiscoveryClient DiscoveryClient 动态客户端，通过动态指定 GVR 来操作任意的 Kubernetes 资源（内置资源 + CR）\n 使用嵌套的 map[string]interface{} 结构存储资源数据； 可以利用反射机制序列化成为特定资源实体； 灵活性更高，但无法做强数据类型检查和验证。  使用 DiscoveryClient 获取到的资源对象为 runtime.Object，可以通过 [``]\n获取 DiscoveryClient 可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。\npackage main import ( \u0026#34;k8s.io/client-go/discovery\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func DiscoveryClient(config *rest.Config) *discovery.DiscoveryClient { client, err := discovery.NewDiscoveryClientForConfig(config) if err != nil { panic(err) } return client } 使用 DiscoveryClient DynamicClient 获取 dynamic.Interface\npackage client import ( \u0026#34;k8s.io/client-go/dynamic\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func DynamicClient(config *rest.Config) dynamic.Interface { client, err := dynamic.NewForConfig(config) if err != nil { panic(err) } return client } 获取 runtimecli.Client\n可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。 但是如果该客户端实例需要操作自定义 K8s API 资源，New 函数传入的参数 runtimecli.Options 中 Scheme 对象需要调整。  package main import ( \u0026#34;k8s.io/apimachinery/pkg/runtime\u0026#34; \u0026#34;k8s.io/client-go/kubernetes/scheme\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; runtimecli \u0026#34;sigs.k8s.io/controller-runtime/pkg/client\u0026#34; // 假设定义了一个 foo CRD，里面包含 register.go 中定义了 AddToScheme 实例 \tfoosv1 \u0026#34;pkg/apis/foos/v1\u0026#34; ) func RuntimeClient(config *rest.Config) runtimecli.Client { client, err := runtimecli.New(config, runtimecli.Options{ Scheme: scheme.Scheme, }) if err != nil { panic(err) } return client } func RuntimeClientForCRD(config *rest.Config) runtimecli.Client { crScheme := runtime.NewScheme() foosv1.AddToScheme(scheme.Scheme) client, err := runtimecli.New(config, runtimecli.Options{ Scheme: crScheme, }) if err != nil { panic(err) } return client } 获取 *http.Client\n最原生的客户端，使用该客户端实例操作 K8s API 资源就纯靠自己手工封装了，不太推荐。  package main import ( \u0026#34;net/http\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func HTTPClient(config *rest.Config) *http.Client { client, err := rest.HTTPClientFor(config) if err != nil { panic(err) } return client } 总结 http.Client =\u0026gt; rest.RESTClient =\u0026gt; discovery.DiscoveryClient =\u0026gt; kubernetes.Clientset\n客户端的定制化越高，使用越高效。但同时对自定义 API 的资源操作的支持就越低。\n","permalink":"https://srcio.cn/series/programming-kubernetes/kube-client/","summary":"本节介绍 Golang 程序如何通过 rest.Config 实例获取各种类型的 Kubernetes 客户端实例。 通过客户端访问 Kubernetes 中的 API 资源实例。\nClientset  获取 *kubernetes.Clientset\n推荐使用该客户端实例去操作 K8s API 资源。  package main import ( \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) func Clientset(config *rest.Config) *kubernetes.Clientset { client, err := kubernetes.NewForConfig(config) if err != nil { panic(err) } return client } 获取 *rest.RESTClient\n可以通过该客户端实例获取内置的以及自定义的 K8s API 资源。  package main import \u0026#34;k8s.io/client-go/rest\u0026#34; func RESTClient(config *rest.Config) *rest.RESTClient { client, err := rest.RESTClientFor(config) if err != nil { panic(err) } return client } DiscoveryClient DiscoveryClient 动态客户端，通过动态指定 GVR 来操作任意的 Kubernetes 资源（内置资源 + CR）","title":"构造 Kubernetes 客户端实例"},{"content":"术语 Group\nAPI 资源置于某个分组下，组作为相关功能的集合。一个组包含一个或多个版本。\nVersion\nAPI 资源的版本，API 资源版本是会不断迭代的。\nKind\nAPI 资源的的类型，用于存储 API 资源的描述信息或状态等。同一个 Kind 的 API 资源可以有多个版本，随着版本的不断迭代，Kind 代表的资源的会有字段内容的更改。\nGVK\nGroup/Version/Kind，例如 Deployment：\napiVersion:apps/v1kind:Deploymentmetadata:- name:deploy-1... 上面的代码示例描述了一个 API 资源对象，这个资源对象：\n Group 是 apps Version 是 v1 Kind 是 Deployment。   Resource\n代表 API 资源，与 GVK 一对一的关系。\nGVR\n可以将 GVK 比作是一个类，GVR 就是这个 GVK 类的实例。\n当我们以 REST 的方式向发起 API 资源的请求是，请求 URL 格式一般类似这样：/api/apps/v1/deployments，里面就包含了三个上面提到的术语概念：\n /apps：请求资源所在的组（Group） /v1：请求资源的版本（Version） /deployments：请求的资源的名称（Resource）  ","permalink":"https://srcio.cn/series/programming-kubernetes/api-design/","summary":"术语 Group\nAPI 资源置于某个分组下，组作为相关功能的集合。一个组包含一个或多个版本。\nVersion\nAPI 资源的版本，API 资源版本是会不断迭代的。\nKind\nAPI 资源的的类型，用于存储 API 资源的描述信息或状态等。同一个 Kind 的 API 资源可以有多个版本，随着版本的不断迭代，Kind 代表的资源的会有字段内容的更改。\nGVK\nGroup/Version/Kind，例如 Deployment：\napiVersion:apps/v1kind:Deploymentmetadata:- name:deploy-1... 上面的代码示例描述了一个 API 资源对象，这个资源对象：\n Group 是 apps Version 是 v1 Kind 是 Deployment。   Resource\n代表 API 资源，与 GVK 一对一的关系。\nGVR\n可以将 GVK 比作是一个类，GVR 就是这个 GVK 类的实例。\n当我们以 REST 的方式向发起 API 资源的请求是，请求 URL 格式一般类似这样：/api/apps/v1/deployments，里面就包含了三个上面提到的术语概念：\n /apps：请求资源所在的组（Group） /v1：请求资源的版本（Version） /deployments：请求的资源的名称（Resource）  ","title":"Kubernetes API 设计"},{"content":"","permalink":"https://srcio.cn/series/programming-kubernetes/operator/","summary":"","title":"Kuberentes Operator"},{"content":"本页是这篇Kubernetes 文档中一些内容摘要。\n大致分为 3 个步骤：\n 为 API 类型结构做 tag 标签 在 API 类型例如 pkg/apis/${Group}/${Version}/types.go 中的 Pod 结构体上打标签，支持的标签：   // +genclient：生成客户端函数（包括 Create, Update, Delete, DeleteCollection, Get, List, Update, Patch, Watch，如果 API 类型结构中包括 .Status 字段，还会额外生成 UpdateStatus 函数）； // +genclient:nonNamespaced：指定 API 类型是集群级别而不是命名空间级别的，生成的客户端函数都没有命名空间； // +genclient:onlyVerbs=create,get：只生成 Create, Get 客户端函数； // +genclient:skipVerbs=watch：生成除了 Watch 之外的所有其他客户端函数； // +genclient:noStatus：即使 API 类型结构包含 .Status 字段，也不生成 UpdateStatus 客户端函数； 有些情况下，可能你想要额外生成非标准的客户端函数，例如子资源函数，那么你需要使用下列这些 tag 标签： // +genclient:method=Scale,verb=update,subresource=scale,input=k8s.io/api/extensions/v1beta1.Scale,result=k8s.io/api/extensions/v1beta1.Scale：该例中使用标签，将会自动生成 Scale(string, *v1beta.Scale) *v1beta.Scale 客户端函数，里面配置了子资源函数的输入和输出参数。 另外，以下的 tag 标签也影响着客户端代码的生成： // +groupName=policy.authorization.k8s.io：使用该 group 名称生成到客户端代码中（默认使用 package 名）； // +groupGoName=AuthorizationPolicy：驼峰 Golang 标识符避免带有非唯一前缀例如 policy.authorization.k8s.io 和 policy.k8s.io 的组名冲突。这将可能导致两个 Policy() 函数生成到 clientset 代码中。  如果你是参与开发 k8s.io/kubernetes 项目，你只需要执行 ./hack/update-group.sh 脚本即可生成或更新代码； 如果你开发自己的项目，你需要使用 client-gen 命令以及其命令参数：  client-gen --input=\u0026#34;pkg/apis/${group1}/${version1},pkg/apis/${group2}/${version2}\u0026#34; --clientset-name=\u0026#34;my_clientset\u0026#34;  使用 client-gen -h 查看更多命令使用姿势。\n 除了 client-gen 自动生成的客户端代码外，你可以手动添加额外的代码，参考这里。  ","permalink":"https://srcio.cn/series/programming-kubernetes/client-gen-usage/","summary":"本页是这篇Kubernetes 文档中一些内容摘要。\n大致分为 3 个步骤：\n 为 API 类型结构做 tag 标签 在 API 类型例如 pkg/apis/${Group}/${Version}/types.go 中的 Pod 结构体上打标签，支持的标签：   // +genclient：生成客户端函数（包括 Create, Update, Delete, DeleteCollection, Get, List, Update, Patch, Watch，如果 API 类型结构中包括 .Status 字段，还会额外生成 UpdateStatus 函数）； // +genclient:nonNamespaced：指定 API 类型是集群级别而不是命名空间级别的，生成的客户端函数都没有命名空间； // +genclient:onlyVerbs=create,get：只生成 Create, Get 客户端函数； // +genclient:skipVerbs=watch：生成除了 Watch 之外的所有其他客户端函数； // +genclient:noStatus：即使 API 类型结构包含 .Status 字段，也不生成 UpdateStatus 客户端函数； 有些情况下，可能你想要额外生成非标准的客户端函数，例如子资源函数，那么你需要使用下列这些 tag 标签： // +genclient:method=Scale,verb=update,subresource=scale,input=k8s.io/api/extensions/v1beta1.Scale,result=k8s.io/api/extensions/v1beta1.Scale：该例中使用标签，将会自动生成 Scale(string, *v1beta.Scale) *v1beta.Scale 客户端函数，里面配置了子资源函数的输入和输出参数。 另外，以下的 tag 标签也影响着客户端代码的生成： // +groupName=policy.","title":"使用 client-gen 生成 clientset 代码"},{"content":"Giscus  开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载  必要条件  你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus.app，安装的时候，分配你的博客所用的 GitHub 仓库即可。   当然，如果你的博客没有托管在 Github 上，你也可以单独创建一个 Github 仓库作为开通 giscus 评论。\n 使用姿势  在 giscus.app 做自定义配置，填入你的仓库名称，选择主题等，Giscus 会自动帮你生成 javascript 脚本； Hugo 博客目录下，创建 layouts/partials/comments.html 文件，写入获取的脚本：  \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;[在此输入仓库]\u0026#34; data-repo-id=\u0026#34;[在此输入仓库 ID]\u0026#34; data-category=\u0026#34;[在此输入分类名]\u0026#34; data-category-id=\u0026#34;[在此输入分类 ID]\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;light\u0026#34; data-lang=\u0026#34;zh-CN\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt;  ⚠️注意：为了使下面的 javascript 脚本生效，data-theme 选择 light；或者你可以根据你选择的主题修改下面的 javascript 脚本。\n 自动主题  使用一个 div 作为评论区域的容器  \u0026lt;div class=\u0026#34;giscus_comments\u0026#34;\u0026gt; {{- partial \u0026#34;comments.html\u0026#34; . }} \u0026lt;/div\u0026gt; 在该容器下方写入主题自动切换的语句  \u0026lt;script\u0026gt; document.querySelector(\u0026#34;div.giscus_comments \u0026gt; script\u0026#34;) .setAttribute( \u0026#34;data-theme\u0026#34;, localStorage.getItem(\u0026#34;pref-theme\u0026#34;) ? localStorage.getItem(\u0026#34;pref-theme\u0026#34;) : window.matchMedia(\u0026#34;(prefers-color-scheme: dark)\u0026#34;).matches ? \u0026#34;dark\u0026#34; : \u0026#34;light\u0026#34;), document.querySelector(\u0026#34;#theme-toggle\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { let e = document.querySelector(\u0026#34;iframe.giscus-frame\u0026#34;); e \u0026amp;\u0026amp; e.contentWindow.postMessage({ giscus: { setConfig: { theme: localStorage.getItem(\u0026#34;pref-theme\u0026#34;) ? localStorage.getItem(\u0026#34;pref-theme\u0026#34;) === \u0026#34;dark\u0026#34; ? \u0026#34;light\u0026#34; : \u0026#34;dark\u0026#34; : document.body.className.includes(\u0026#34;dark\u0026#34;) ? \u0026#34;light\u0026#34; : \u0026#34;dark\u0026#34; } } }, \u0026#34;https://giscus.app\u0026#34;) }) \u0026lt;/script\u0026gt; 🔗 链接  应用：https://github.com/apps/giscus 源码：https://github.com/giscus/giscus 使用：https://giscus.app  ","permalink":"https://srcio.cn/posts/use-giscus/","summary":"Giscus  开源、无广告、永久免费 支持多语言 支持表情反馈 支持懒加载  必要条件  你的博客所用的 GitHub 的仓库必须是 Public，并且开通了 Dicussion 功能； 安装 giscus.app，安装的时候，分配你的博客所用的 GitHub 仓库即可。   当然，如果你的博客没有托管在 Github 上，你也可以单独创建一个 Github 仓库作为开通 giscus 评论。\n 使用姿势  在 giscus.app 做自定义配置，填入你的仓库名称，选择主题等，Giscus 会自动帮你生成 javascript 脚本； Hugo 博客目录下，创建 layouts/partials/comments.html 文件，写入获取的脚本：  \u0026lt;script src=\u0026#34;https://giscus.app/client.js\u0026#34; data-repo=\u0026#34;[在此输入仓库]\u0026#34; data-repo-id=\u0026#34;[在此输入仓库 ID]\u0026#34; data-category=\u0026#34;[在此输入分类名]\u0026#34; data-category-id=\u0026#34;[在此输入分类 ID]\u0026#34; data-mapping=\u0026#34;pathname\u0026#34; data-strict=\u0026#34;0\u0026#34; data-reactions-enabled=\u0026#34;1\u0026#34; data-emit-metadata=\u0026#34;0\u0026#34; data-input-position=\u0026#34;bottom\u0026#34; data-theme=\u0026#34;light\u0026#34; data-lang=\u0026#34;zh-CN\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async\u0026gt; \u0026lt;/script\u0026gt;  ⚠️注意：为了使下面的 javascript 脚本生效，data-theme 选择 light；或者你可以根据你选择的主题修改下面的 javascript 脚本。\n 自动主题  使用一个 div 作为评论区域的容器  \u0026lt;div class=\u0026#34;giscus_comments\u0026#34;\u0026gt; {{- partial \u0026#34;comments.","title":"使用 Giscus 作为博客评论系统"}]